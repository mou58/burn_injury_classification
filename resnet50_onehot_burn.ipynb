{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mou58/burn_injury_classification/blob/master/resnet50_onehot_burn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0Y_AC6u0a9bs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y_AC6u0a9bs",
        "outputId": "c285d289-5c9d-478e-8095-34aec4f32d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07589ece",
      "metadata": {
        "id": "07589ece"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization \n",
        "from tensorflow.keras.layers import Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import random\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef0e8fe",
      "metadata": {
        "id": "4ef0e8fe"
      },
      "source": [
        "## Implement the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25481f96",
      "metadata": {
        "id": "25481f96"
      },
      "outputs": [],
      "source": [
        "def conv_block(input, filters:int, kernel_size:tuple=(1,1), strides:tuple=(1,1), \n",
        "               padding:str='valid', bn_axis:int=3, activation:str=None):\n",
        "    \n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(input)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    \n",
        "    if activation is not None:\n",
        "        x = Activation(activation)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def resnet_block(input, filters:list, kernel_size:tuple=(3,3), strides:tuple=(1,1), convolutional:bool=False):\n",
        "    \n",
        "    x_shortcut = input\n",
        "    \n",
        "    x = conv_block(input, filters[0], (1,1), strides, 'valid', 3, 'relu')\n",
        "    x = conv_block(x, filters[1], kernel_size, (1,1), 'same', 3, 'relu')    \n",
        "    x = conv_block(x, filters[2], (1,1), (1,1), 'valid', 3, None) \n",
        "    \n",
        "    if convolutional: x_shortcut = conv_block(input, filters[2], (1,1), strides, 'valid', 3, None)\n",
        "        \n",
        "    x = Add()([x, x_shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def resnet50(input_shape, n_classes, aug=None):\n",
        "    \n",
        "    input = Input(input_shape)\n",
        "\n",
        "    # Stage 0: zero-padding\n",
        "    if aug is not None: # with augmentation\n",
        "      x = aug(input)\n",
        "      x = ZeroPadding2D((3, 3))(x) \n",
        "    else: x = ZeroPadding2D((3, 3))(input) # without augmentation\n",
        "        \n",
        "    # Stage 1\n",
        "    x = conv_block(x, 64, (7,7), (2,2), 'valid', 3, 'relu')    \n",
        "    x = MaxPooling2D((3,3), strides=(2,2))(x)\n",
        "    \n",
        "    # Stage 2\n",
        "    x = resnet_block(x, [64,64,256], (3,3), (1,1), True)\n",
        "    x = resnet_block(x, [64,64,256], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [64,64,256], (3,3), (1,1), False)\n",
        "    \n",
        "    # Stage 3\n",
        "    x = resnet_block(x, [128,128,512], (3,3), (2,2), True)\n",
        "    x = resnet_block(x, [128,128,512], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [128,128,512], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [128,128,512], (3,3), (1,1), False)\n",
        "    \n",
        "    # Stage 4\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (2,2), True)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    \n",
        "    # Stage 5\n",
        "    x = resnet_block(x, [512,512,2048], (3,3), (2,2), True)\n",
        "    x = resnet_block(x, [512,512,2048], (3,3), (1,1), False) \n",
        "    x = resnet_block(x, [512,512,2048], (3,3), (1,1), False) \n",
        "    \n",
        "    # Stage 6: Average pooling\n",
        "    x = AveragePooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    # Output layer\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(n_classes, activation='softmax', name='fc')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=input, outputs=x, name='ResNet50')\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e2a4c8",
      "metadata": {
        "id": "55e2a4c8"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d24e2cd",
      "metadata": {
        "id": "4d24e2cd"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "# This function creates image dataset after preprocessing images. Image can be grayscale or rgb\n",
        "def create_image_data(CATEGORIES, img_size, dimension, DataDir):\n",
        "    database = []\n",
        "    for category in CATEGORIES:\n",
        "        print(category)\n",
        "        path = os.path.join(DataDir, category)\n",
        "        class_num = CATEGORIES.index(category)\n",
        "        for img_name in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                if dimension == 1:\n",
        "                    img = cv2.imread(os.path.join(path, img_name), cv2.IMREAD_GRAYSCALE)\n",
        "                elif dimension == 3:\n",
        "                    img = cv2.imread(os.path.join(path, img_name))\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                else:\n",
        "                    print('Please select dimension either 1 or 3')\n",
        "                img = cv2.resize(img, (img_size, img_size))\n",
        "                database.append([img, class_num])\n",
        "            except Exception as e:\n",
        "                pass\n",
        "    return database"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "# This function preprocess single image\n",
        "def prepare(filepath, IMG_SIZE, dimension):\n",
        "    if dimension == 1:\n",
        "        img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "    elif dimension == 3:\n",
        "        img_array = cv2.imread(filepath)\n",
        "        img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "    else:\n",
        "        print(\"Please select dimension either 1 or 3\")\n",
        "    new_array = new_array.reshape(-1, IMG_SIZE, IMG_SIZE, dimension)\n",
        "    return new_array"
      ],
      "metadata": {
        "id": "KlMJQae7HX9L"
      },
      "id": "KlMJQae7HX9L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 200\n",
        "DIM = 3\n",
        "CATEGORIES = [\"superficial\", \"deep\", \"full\"]\n",
        "\n",
        "n_classes = len(CATEGORIES)\n",
        "\n",
        "TRAINDIR = '/content/drive/MyDrive/CSC 514/Burns_BIP_US_database/Training set'\n",
        "TESTDIR = '/content/drive/MyDrive/CSC 514/Burns_BIP_US_database/Test'"
      ],
      "metadata": {
        "id": "YFJcqSasHdRz"
      },
      "id": "YFJcqSasHdRz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668bcd26",
      "metadata": {
        "id": "668bcd26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e0b4b1e-6eea-4b4f-95f4-c26b28fe82b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "superficial\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:08<00:00,  1.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deep\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:05<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:07<00:00,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of the training data (20, 200, 200, 3)\n",
            "Training labels [1 2 1 0 0 0 0 2 2 0 2 0 1 2 1 0 0 1 0 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Create training dataset\n",
        "training_data = create_image_data(CATEGORIES, IMG_SIZE, DIM, TRAINDIR)\n",
        "\n",
        "# Randomize the dataset\n",
        "random.shuffle(training_data)\n",
        "\n",
        "XF = [] # XF will contain training image data\n",
        "yF = []# yF will contain corresponding image class\n",
        "\n",
        "for features,label in training_data:\n",
        "    XF.append(features)\n",
        "    yF.append(label)\n",
        "\n",
        "XF = np.array(XF).reshape(-1, IMG_SIZE, IMG_SIZE, DIM)\n",
        "print(\"\\nShape of the training data\", XF.shape)\n",
        "\n",
        "# Normalize data\n",
        "# XF2 = XF/255.0\n",
        "\n",
        "yF = np.array(yF)\n",
        "print('Training labels', yF)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create testing dataset\n",
        "testing_data = create_image_data(CATEGORIES, IMG_SIZE, DIM, TESTDIR)\n",
        "\n",
        "# Randomize the dataset\n",
        "random.shuffle(testing_data)\n",
        "\n",
        "Xt = []\n",
        "yt = []\n",
        "\n",
        "for features,label in testing_data:\n",
        "    Xt.append(features)\n",
        "    yt.append(label)\n",
        "\n",
        "Xt = np.array(Xt).reshape(-1, IMG_SIZE, IMG_SIZE, DIM)\n",
        "print(\"Shape Xt\", Xt.shape)\n",
        "\n",
        "yt = np.array(yt)\n",
        "print(yt)\n",
        "\n",
        "# Xt = Xt/255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPL5on4VmGHI",
        "outputId": "f78189e7-dd7b-4e51-f234-0bf3b4aebee8"
      },
      "id": "uPL5on4VmGHI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "superficial\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33/33 [00:21<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deep\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:16<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:08<00:00,  1.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape Xt (74, 200, 200, 3)\n",
            "[1 2 2 0 1 0 1 2 0 1 0 1 0 1 1 0 0 0 1 2 2 1 1 2 0 1 1 1 2 0 1 0 1 0 0 2 0\n",
            " 2 0 1 0 0 1 1 1 2 1 0 0 2 0 1 2 2 1 1 0 1 0 0 0 0 2 0 0 0 0 1 1 0 0 1 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize samples"
      ],
      "metadata": {
        "id": "FTF6b7UcoU_w"
      },
      "id": "FTF6b7UcoU_w"
    },
    {
      "cell_type": "code",
      "source": [
        "idx = int(np.random.randint(low=0, high=len(XF), size=1)) # randomly choose an integer\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(np.squeeze(XF[idx]))\n",
        "plt.title(CATEGORIES[yF[idx]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "xLGvM4MKoR4Y",
        "outputId": "be9e6f33-b313-4d5b-9dde-15878609debe"
      },
      "id": "xLGvM4MKoR4Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'superficial')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to onehot"
      ],
      "metadata": {
        "id": "R5lH2E7smZZg"
      },
      "id": "R5lH2E7smZZg"
    },
    {
      "cell_type": "code",
      "source": [
        "yF = tf.keras.utils.to_categorical(yF, num_classes=n_classes)\n",
        "yt_onehot = tf.keras.utils.to_categorical(yt, num_classes=n_classes)"
      ],
      "metadata": {
        "id": "VYSs_71mmY4Y"
      },
      "id": "VYSs_71mmY4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentation"
      ],
      "metadata": {
        "id": "u5LoGa5Cvp0P"
      },
      "id": "u5LoGa5Cvp0P"
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "      layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "      layers.RandomRotation(0.2),\n",
        "      # layers.RandomZoom(0.2),\n",
        "    ]\n",
        "  )"
      ],
      "metadata": {
        "id": "XRttsXUhvsJh"
      },
      "id": "XRttsXUhvsJh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XF = XF/255.0\n",
        "# Xt = Xt/255.0\n",
        "\n",
        "XF = tf.keras.applications.resnet50.preprocess_input(XF)\n",
        "Xt = tf.keras.applications.resnet50.preprocess_input(Xt)"
      ],
      "metadata": {
        "id": "rWGQ6kITXLa7"
      },
      "id": "rWGQ6kITXLa7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "86823353",
      "metadata": {
        "id": "86823353"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1379a537",
      "metadata": {
        "id": "1379a537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b9e3b8-ec40-4c67-ec22-b6ff5bdd38f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet50_2022-12-07_20-50-29\n"
          ]
        }
      ],
      "source": [
        "model_name = 'resnet50' + '_' + datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "print(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a2d4a9",
      "metadata": {
        "id": "39a2d4a9"
      },
      "outputs": [],
      "source": [
        "model = resnet50(input_shape=XF.shape[1:], n_classes=n_classes, aug=None)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c944dbd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c944dbd6",
        "outputId": "3e424da4-4ae6-459c-b7b2-4d2118f22cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ResNet50\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 200, 200, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " zero_padding2d (ZeroPadding2D)  (None, 206, 206, 3)  0          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 100, 100, 64  9472        ['zero_padding2d[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 100, 100, 64  256        ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 100, 100, 64  0           ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 49, 49, 64)   0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 49, 49, 64)   4160        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 49, 49, 64)   36928       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 49, 49, 256)  16640       ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 49, 49, 256)  16640       ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 49, 49, 256)  1024       ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 49, 49, 256)  1024       ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 49, 49, 256)  0           ['batch_normalization_3[0][0]',  \n",
            "                                                                  'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 49, 49, 256)  0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 49, 49, 64)   16448       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 49, 49, 64)   36928       ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 49, 49, 256)  16640       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 49, 49, 256)  1024       ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 49, 49, 256)  0           ['batch_normalization_7[0][0]',  \n",
            "                                                                  'activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 49, 49, 256)  0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 49, 49, 64)   16448       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 49, 49, 64)   36928       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 49, 49, 256)  16640       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 49, 49, 256)  1024       ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 49, 49, 256)  0           ['batch_normalization_10[0][0]', \n",
            "                                                                  'activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 49, 49, 256)  0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 25, 25, 128)  32896       ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 25, 25, 128)  512        ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 25, 25, 128)  147584      ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 25, 25, 128)  512        ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 25, 25, 512)  66048       ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 25, 25, 512)  131584      ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 25, 25, 512)  0           ['batch_normalization_13[0][0]', \n",
            "                                                                  'batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 25, 25, 512)  0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 25, 25, 128)  65664       ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 25, 25, 128)  512        ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 25, 25, 128)  147584      ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 25, 25, 128)  512        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 25, 25, 512)  66048       ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 25, 25, 512)  0           ['batch_normalization_17[0][0]', \n",
            "                                                                  'activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 25, 25, 512)  0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 25, 25, 128)  65664       ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 25, 25, 128)  512        ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 25, 25, 128)  147584      ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 25, 25, 128)  512        ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 25, 25, 512)  66048       ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 25, 25, 512)  0           ['batch_normalization_20[0][0]', \n",
            "                                                                  'activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 25, 25, 512)  0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 25, 25, 128)  65664       ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 25, 25, 128)  512        ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 25, 25, 128)  147584      ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 25, 25, 128)  512        ['conv2d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 25, 25, 512)  66048       ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 25, 25, 512)  0           ['batch_normalization_23[0][0]', \n",
            "                                                                  'activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 25, 25, 512)  0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 13, 13, 256)  131328      ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_25[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_23[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 13, 13, 1024  525312      ['activation_21[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_26[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_27[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 13, 13, 1024  0           ['batch_normalization_26[0][0]', \n",
            "                                )                                 'batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 13, 13, 1024  0           ['add_7[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_26[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_30[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 13, 13, 1024  0           ['batch_normalization_30[0][0]', \n",
            "                                )                                 'activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 13, 13, 1024  0           ['add_8[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_29[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_33[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 13, 13, 1024  0           ['batch_normalization_33[0][0]', \n",
            "                                )                                 'activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 13, 13, 1024  0           ['add_9[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_32[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_36[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 13, 13, 1024  0           ['batch_normalization_36[0][0]', \n",
            "                                )                                 'activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 13, 13, 1024  0           ['add_10[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_37[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_35[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_39[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 13, 13, 1024  0           ['batch_normalization_39[0][0]', \n",
            "                                )                                 'activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 13, 13, 1024  0           ['add_11[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_41[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_38[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_42[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 13, 13, 1024  0           ['batch_normalization_42[0][0]', \n",
            "                                )                                 'activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 13, 13, 1024  0           ['add_12[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 7, 7, 512)    524800      ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 7, 7, 512)    2359808     ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 7, 7, 2048)   1050624     ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 7, 7, 2048)   2099200     ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 7, 7, 2048)  8192        ['conv2d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 7, 7, 2048)  8192        ['conv2d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_45[0][0]', \n",
            "                                                                  'batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 7, 7, 2048)   0           ['add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 7, 7, 512)    1049088     ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 7, 7, 512)    2359808     ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 7, 7, 2048)   1050624     ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 7, 7, 2048)  8192        ['conv2d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_49[0][0]', \n",
            "                                                                  'activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 7, 7, 2048)   0           ['add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 7, 7, 512)    1049088     ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 7, 7, 512)    2359808     ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 7, 7, 2048)   1050624     ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 7, 7, 2048)  8192        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_52[0][0]', \n",
            "                                                                  'activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 7, 7, 2048)   0           ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 4, 4, 2048)  0           ['activation_48[0][0]']          \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 32768)        0           ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " fc (Dense)                     (None, 3)            98307       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,686,019\n",
            "Trainable params: 23,632,899\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbefa37",
      "metadata": {
        "id": "dfbefa37"
      },
      "source": [
        "## Compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8dadd1",
      "metadata": {
        "id": "9f8dadd1"
      },
      "outputs": [],
      "source": [
        "loss = 'categorical_crossentropy'\n",
        "\n",
        "metrics = ['accuracy']\n",
        "\n",
        "model.compile(loss=loss, optimizer=Adam(learning_rate = 1e-5), metrics=metrics)\n",
        "\n",
        "checkpoint_loc = '/content/drive/MyDrive/CSC 514/checkpoints/' + model_name\n",
        "log_path = \"/content/drive/MyDrive/CSC 514/logs/\"\n",
        "\n",
        "# Create checkpoint directory if does not exist\n",
        "if not os.path.exists(checkpoint_loc): os.makedirs(checkpoint_loc)\n",
        "if not os.path.exists(log_path): os.makedirs(log_path)\n",
        "\n",
        "checkpoint_path = os.path.join(checkpoint_loc, 'best_model.h5')\n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100, verbose=1, mode='auto',\n",
        "#         restore_best_weights=True)\n",
        "\n",
        "callbacks = [\n",
        "    # tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=1),\n",
        "    \n",
        "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1,\n",
        "                      monitor='val_loss',\n",
        "                      patience=10,\n",
        "                      min_lr=0.00001,\n",
        "                      verbose=1,\n",
        "                      mode='auto'),\n",
        "    \n",
        "    tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                      monitor = 'val_loss',\n",
        "                      verbose = 1,\n",
        "                      save_best_only=True,\n",
        "                      save_weights_only=True,\n",
        "                      ),\n",
        "    tf.keras.callbacks.CSVLogger(os.path.join(log_path, model_name + '.csv'), separator=',', append=True),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49748766",
      "metadata": {
        "id": "49748766"
      },
      "outputs": [],
      "source": [
        "# Uncomment to load the model\n",
        "# model = resnet50(input_shape=XF.shape[1:], n_classes=n_classes)  \n",
        "# model.load_weights(checkpoint_loc + '//' + 'best_model.h5')\n",
        "# model.compile(loss=loss, optimizer=Adam(learning_rate = 1e-5), metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa95d6d",
      "metadata": {
        "id": "8fa95d6d"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca9ddec7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca9ddec7",
        "outputId": "094d6883-1c13-4cc2-f7e9-7ebabe130498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.2005 - accuracy: 0.2778\n",
            "Epoch 1: val_loss improved from inf to 1.11273, saving model to /content/drive/MyDrive/CSC 514/checkpoints/resnet50_2022-12-07_20-50-29/best_model.h5\n",
            "5/5 [==============================] - 17s 655ms/step - loss: 2.2005 - accuracy: 0.2778 - val_loss: 1.1127 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 2/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.2335 - accuracy: 0.4375\n",
            "Epoch 2: val_loss improved from 1.11273 to 1.06157, saving model to /content/drive/MyDrive/CSC 514/checkpoints/resnet50_2022-12-07_20-50-29/best_model.h5\n",
            "5/5 [==============================] - 1s 211ms/step - loss: 1.3396 - accuracy: 0.3889 - val_loss: 1.0616 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 3/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.2155 - accuracy: 0.4375\n",
            "Epoch 3: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.2054 - accuracy: 0.4444 - val_loss: 1.0951 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 4/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7818 - accuracy: 0.5000\n",
            "Epoch 4: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.7340 - accuracy: 0.5556 - val_loss: 1.1406 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 5/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.0431 - accuracy: 0.4375\n",
            "Epoch 5: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 1.1616 - accuracy: 0.3889 - val_loss: 1.1472 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 6/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5914 - accuracy: 0.8125\n",
            "Epoch 6: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.5877 - accuracy: 0.7778 - val_loss: 1.1948 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 7/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.8165 - accuracy: 0.5625\n",
            "Epoch 7: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.8740 - accuracy: 0.5556 - val_loss: 1.2484 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 8/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7690 - accuracy: 0.6250\n",
            "Epoch 8: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.7533 - accuracy: 0.6667 - val_loss: 1.2641 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 9/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.8324 - accuracy: 0.6875\n",
            "Epoch 9: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.7675 - accuracy: 0.7222 - val_loss: 1.2884 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 10/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.8367 - accuracy: 0.5625\n",
            "Epoch 10: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.8232 - accuracy: 0.6111 - val_loss: 1.3199 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 11/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6390 - accuracy: 0.6875\n",
            "Epoch 11: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.5864 - accuracy: 0.7222 - val_loss: 1.3454 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 12/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5881 - accuracy: 0.8125\n",
            "Epoch 12: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.5313 - accuracy: 0.8333 - val_loss: 1.3569 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 13/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5147 - accuracy: 0.7500\n",
            "Epoch 13: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.6483 - accuracy: 0.6667 - val_loss: 1.3905 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 14/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5426 - accuracy: 0.8125\n",
            "Epoch 14: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.6226 - accuracy: 0.7778 - val_loss: 1.4405 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 15/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5312 - accuracy: 0.8125\n",
            "Epoch 15: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.4911 - accuracy: 0.8333 - val_loss: 1.4434 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 16/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.7573 - accuracy: 0.7500\n",
            "Epoch 16: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.7024 - accuracy: 0.7778 - val_loss: 1.4626 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 17/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3420 - accuracy: 0.8750\n",
            "Epoch 17: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.3196 - accuracy: 0.8889 - val_loss: 1.5086 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 18/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5592 - accuracy: 0.7500\n",
            "Epoch 18: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.5123 - accuracy: 0.7778 - val_loss: 1.5463 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 19/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.4026 - accuracy: 0.8750\n",
            "Epoch 19: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.3762 - accuracy: 0.8889 - val_loss: 1.6023 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 20/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.4484 - accuracy: 0.8125\n",
            "Epoch 20: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.4474 - accuracy: 0.8333 - val_loss: 1.6730 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 21/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.4344 - accuracy: 0.9375\n",
            "Epoch 21: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.4138 - accuracy: 0.9444 - val_loss: 1.6765 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 22/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3511 - accuracy: 0.9375\n",
            "Epoch 22: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.3510 - accuracy: 0.9444 - val_loss: 1.6569 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 23/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3202 - accuracy: 0.9375\n",
            "Epoch 23: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.3435 - accuracy: 0.9444 - val_loss: 1.6993 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 24/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.4053 - accuracy: 0.8750\n",
            "Epoch 24: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.3846 - accuracy: 0.8889 - val_loss: 1.7802 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 25/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3925 - accuracy: 0.8125\n",
            "Epoch 25: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.3954 - accuracy: 0.8333 - val_loss: 1.8044 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 26/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2346 - accuracy: 0.9375\n",
            "Epoch 26: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.2270 - accuracy: 0.9444 - val_loss: 1.7978 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 27/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3651 - accuracy: 0.8750\n",
            "Epoch 27: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.3333 - accuracy: 0.8889 - val_loss: 1.8336 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 28/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2587 - accuracy: 1.0000\n",
            "Epoch 28: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.2399 - accuracy: 1.0000 - val_loss: 1.8628 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 29/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1919 - accuracy: 0.9375\n",
            "Epoch 29: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.2169 - accuracy: 0.9444 - val_loss: 1.8728 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 30/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3069 - accuracy: 0.8750\n",
            "Epoch 30: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.2853 - accuracy: 0.8889 - val_loss: 1.9560 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 31/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2676 - accuracy: 0.8750\n",
            "Epoch 31: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.2577 - accuracy: 0.8889 - val_loss: 2.0054 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 32/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3408 - accuracy: 0.8750\n",
            "Epoch 32: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.3068 - accuracy: 0.8889 - val_loss: 2.0543 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 33/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.3099 - accuracy: 0.8889\n",
            "Epoch 33: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.3099 - accuracy: 0.8889 - val_loss: 2.0667 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 34/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2242 - accuracy: 1.0000\n",
            "Epoch 34: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.2016 - accuracy: 1.0000 - val_loss: 2.0534 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 35/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2222 - accuracy: 1.0000\n",
            "Epoch 35: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.2016 - accuracy: 1.0000 - val_loss: 2.1098 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 36/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2785 - accuracy: 0.9375\n",
            "Epoch 36: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.3045 - accuracy: 0.8889 - val_loss: 2.1561 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 37/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1427 - accuracy: 1.0000\n",
            "Epoch 37: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1389 - accuracy: 1.0000 - val_loss: 2.1985 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 38/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.4107 - accuracy: 0.7500\n",
            "Epoch 38: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.3770 - accuracy: 0.7778 - val_loss: 2.2639 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 39/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2125 - accuracy: 0.9375\n",
            "Epoch 39: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.1987 - accuracy: 0.9444 - val_loss: 2.2272 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 40/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2413 - accuracy: 0.9375\n",
            "Epoch 40: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.2700 - accuracy: 0.9444 - val_loss: 2.2247 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 41/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0908 - accuracy: 1.0000\n",
            "Epoch 41: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.1648 - accuracy: 0.9444 - val_loss: 2.3940 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 42/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2262 - accuracy: 0.9375\n",
            "Epoch 42: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.2499 - accuracy: 0.8889 - val_loss: 2.5144 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 43/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2664 - accuracy: 0.9375\n",
            "Epoch 43: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.2420 - accuracy: 0.9444 - val_loss: 2.3654 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 44/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1667 - accuracy: 1.0000\n",
            "Epoch 44: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.1655 - accuracy: 1.0000 - val_loss: 2.2360 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 45/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 1.0000\n",
            "Epoch 45: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.1318 - accuracy: 1.0000 - val_loss: 2.1891 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 46/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2739 - accuracy: 0.9375\n",
            "Epoch 46: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.2764 - accuracy: 0.9444 - val_loss: 2.3464 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 47/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1852 - accuracy: 1.0000\n",
            "Epoch 47: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.1678 - accuracy: 1.0000 - val_loss: 2.4054 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 48/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2713 - accuracy: 0.8750\n",
            "Epoch 48: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.2531 - accuracy: 0.8889 - val_loss: 2.3935 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 49/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1641 - accuracy: 1.0000\n",
            "Epoch 49: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.1467 - accuracy: 1.0000 - val_loss: 2.4865 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 50/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1669 - accuracy: 0.9375\n",
            "Epoch 50: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1645 - accuracy: 0.9444 - val_loss: 2.4637 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 51/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1248 - accuracy: 0.9375\n",
            "Epoch 51: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.1127 - accuracy: 0.9444 - val_loss: 2.4327 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 52/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0892 - accuracy: 1.0000\n",
            "Epoch 52: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.0933 - accuracy: 1.0000 - val_loss: 2.4726 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 53/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1386 - accuracy: 1.0000\n",
            "Epoch 53: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1483 - accuracy: 1.0000 - val_loss: 2.4788 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 54/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2180 - accuracy: 0.9375\n",
            "Epoch 54: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.2214 - accuracy: 0.9444 - val_loss: 2.4335 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 55/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0750 - accuracy: 1.0000\n",
            "Epoch 55: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0988 - accuracy: 1.0000 - val_loss: 2.4257 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 56/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1350 - accuracy: 1.0000\n",
            "Epoch 56: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.1254 - accuracy: 1.0000 - val_loss: 2.4893 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 57/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1462 - accuracy: 0.9375\n",
            "Epoch 57: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.1331 - accuracy: 0.9444 - val_loss: 2.4922 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 58/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1397 - accuracy: 1.0000\n",
            "Epoch 58: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.1301 - accuracy: 1.0000 - val_loss: 2.5126 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 59/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1085 - accuracy: 1.0000\n",
            "Epoch 59: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1030 - accuracy: 1.0000 - val_loss: 2.6172 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 60/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1293 - accuracy: 1.0000\n",
            "Epoch 60: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.1394 - accuracy: 1.0000 - val_loss: 2.7862 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 61/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1710 - accuracy: 0.9375\n",
            "Epoch 61: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.1631 - accuracy: 0.9444 - val_loss: 2.7132 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 62/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0965 - accuracy: 1.0000\n",
            "Epoch 62: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1927 - accuracy: 0.9444 - val_loss: 2.5317 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 63/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1105 - accuracy: 1.0000\n",
            "Epoch 63: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.1470 - accuracy: 0.9444 - val_loss: 2.3116 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 64/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1368 - accuracy: 1.0000\n",
            "Epoch 64: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1433 - accuracy: 1.0000 - val_loss: 2.3185 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 65/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0941 - accuracy: 1.0000\n",
            "Epoch 65: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0992 - accuracy: 1.0000 - val_loss: 2.4366 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 66/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0614 - accuracy: 1.0000\n",
            "Epoch 66: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0615 - accuracy: 1.0000 - val_loss: 2.6298 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 67/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1074 - accuracy: 0.9375\n",
            "Epoch 67: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0962 - accuracy: 0.9444 - val_loss: 2.8116 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 68/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0723 - accuracy: 1.0000\n",
            "Epoch 68: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.0736 - accuracy: 1.0000 - val_loss: 2.9361 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 69/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1193 - accuracy: 1.0000\n",
            "Epoch 69: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.1068 - accuracy: 1.0000 - val_loss: 2.9017 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 70/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1143 - accuracy: 1.0000\n",
            "Epoch 70: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.1658 - accuracy: 1.0000 - val_loss: 2.8647 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 71/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0846 - accuracy: 1.0000\n",
            "Epoch 71: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0819 - accuracy: 1.0000 - val_loss: 3.0869 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 72/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2161 - accuracy: 0.8750\n",
            "Epoch 72: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.2430 - accuracy: 0.8889 - val_loss: 2.9481 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 73/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1541 - accuracy: 0.8750\n",
            "Epoch 73: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1406 - accuracy: 0.8889 - val_loss: 2.5501 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 74/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0908 - accuracy: 1.0000\n",
            "Epoch 74: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0894 - accuracy: 1.0000 - val_loss: 2.3065 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 75/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0878 - accuracy: 1.0000\n",
            "Epoch 75: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0812 - accuracy: 1.0000 - val_loss: 2.3479 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 76/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1163 - accuracy: 1.0000\n",
            "Epoch 76: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1150 - accuracy: 1.0000 - val_loss: 2.3485 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 77/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1066 - accuracy: 1.0000\n",
            "Epoch 77: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.1209 - accuracy: 1.0000 - val_loss: 2.2707 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 78/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0942 - accuracy: 1.0000\n",
            "Epoch 78: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1342 - accuracy: 0.9444 - val_loss: 2.3247 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 79/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0600 - accuracy: 1.0000\n",
            "Epoch 79: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0536 - accuracy: 1.0000 - val_loss: 2.4482 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 80/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1643 - accuracy: 0.9375\n",
            "Epoch 80: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.1490 - accuracy: 0.9444 - val_loss: 2.4613 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 81/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1895 - accuracy: 0.9375\n",
            "Epoch 81: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.2460 - accuracy: 0.8889 - val_loss: 2.4866 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 82/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0867 - accuracy: 1.0000\n",
            "Epoch 82: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0828 - accuracy: 1.0000 - val_loss: 2.1759 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 83/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0677 - accuracy: 1.0000\n",
            "Epoch 83: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0774 - accuracy: 1.0000 - val_loss: 1.9064 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 84/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0481 - accuracy: 1.0000\n",
            "Epoch 84: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0469 - accuracy: 1.0000 - val_loss: 1.9937 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 85/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0496 - accuracy: 1.0000\n",
            "Epoch 85: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 2.0323 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 86/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0860 - accuracy: 1.0000\n",
            "Epoch 86: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0789 - accuracy: 1.0000 - val_loss: 2.0421 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 87/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0302 - accuracy: 1.0000\n",
            "Epoch 87: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 2.0017 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 88/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0433 - accuracy: 1.0000\n",
            "Epoch 88: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0426 - accuracy: 1.0000 - val_loss: 1.9192 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 89/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1142 - accuracy: 1.0000\n",
            "Epoch 89: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1325 - accuracy: 1.0000 - val_loss: 1.9035 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 90/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0682 - accuracy: 1.0000\n",
            "Epoch 90: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 2.0788 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 91/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0524 - accuracy: 1.0000\n",
            "Epoch 91: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.1077 - accuracy: 0.9444 - val_loss: 2.2355 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 92/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0389 - accuracy: 1.0000\n",
            "Epoch 92: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 2.1302 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 93/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1121 - accuracy: 1.0000\n",
            "Epoch 93: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1003 - accuracy: 1.0000 - val_loss: 2.0563 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 94/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0635 - accuracy: 1.0000\n",
            "Epoch 94: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 2.1051 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 95/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0456 - accuracy: 1.0000\n",
            "Epoch 95: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 2.1184 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 96/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0728 - accuracy: 1.0000\n",
            "Epoch 96: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0678 - accuracy: 1.0000 - val_loss: 1.9813 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 97/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 97: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 1.8918 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 98/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0438 - accuracy: 1.0000\n",
            "Epoch 98: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 1.9113 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 99/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0689 - accuracy: 1.0000\n",
            "Epoch 99: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0617 - accuracy: 1.0000 - val_loss: 1.9179 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 100/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0538 - accuracy: 1.0000\n",
            "Epoch 100: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0740 - accuracy: 1.0000 - val_loss: 1.7155 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 101/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0418 - accuracy: 1.0000\n",
            "Epoch 101: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 1.9320 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 102/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0567 - accuracy: 1.0000\n",
            "Epoch 102: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0587 - accuracy: 1.0000 - val_loss: 2.0174 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 103/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000\n",
            "Epoch 103: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 1.8914 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 104/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0240 - accuracy: 1.0000\n",
            "Epoch 104: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 1.7496 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 105/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0694 - accuracy: 1.0000\n",
            "Epoch 105: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0942 - accuracy: 1.0000 - val_loss: 1.7137 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 106/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0260 - accuracy: 1.0000\n",
            "Epoch 106: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 1.6014 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 107/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0473 - accuracy: 1.0000\n",
            "Epoch 107: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0575 - accuracy: 1.0000 - val_loss: 1.4648 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 108/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0382 - accuracy: 1.0000\n",
            "Epoch 108: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 1.6177 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 109/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0396 - accuracy: 1.0000\n",
            "Epoch 109: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 1.8742 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 110/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0505 - accuracy: 1.0000\n",
            "Epoch 110: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0751 - accuracy: 1.0000 - val_loss: 1.7717 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 111/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000\n",
            "Epoch 111: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 1.8548 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 112/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000\n",
            "Epoch 112: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 2.0161 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 113/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000\n",
            "Epoch 113: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0406 - accuracy: 1.0000 - val_loss: 1.8962 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 114/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0261 - accuracy: 1.0000\n",
            "Epoch 114: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 1.9554 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 115/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 115: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 1.8507 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 116/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 116: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 1.8616 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 117/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 1.0000\n",
            "Epoch 117: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0615 - accuracy: 1.0000 - val_loss: 1.9660 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 118/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0220 - accuracy: 1.0000\n",
            "Epoch 118: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 1.8918 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 119/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0508 - accuracy: 1.0000\n",
            "Epoch 119: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0468 - accuracy: 1.0000 - val_loss: 2.0126 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 120/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0435 - accuracy: 1.0000\n",
            "Epoch 120: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 1.9603 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 121/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0394 - accuracy: 1.0000\n",
            "Epoch 121: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 1.8958 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 122/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0425 - accuracy: 1.0000\n",
            "Epoch 122: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0392 - accuracy: 1.0000 - val_loss: 2.0856 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 123/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000\n",
            "Epoch 123: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 2.0154 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 124/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0476 - accuracy: 1.0000\n",
            "Epoch 124: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 2.0067 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 125/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0265 - accuracy: 1.0000\n",
            "Epoch 125: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 2.0301 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 126/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0354 - accuracy: 1.0000\n",
            "Epoch 126: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 2.0444 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 127/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0225 - accuracy: 1.0000\n",
            "Epoch 127: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 1.8682 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 128/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0529 - accuracy: 1.0000\n",
            "Epoch 128: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0737 - accuracy: 1.0000 - val_loss: 1.7008 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 129/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0333 - accuracy: 1.0000\n",
            "Epoch 129: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0325 - accuracy: 1.0000 - val_loss: 1.8259 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 130/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0435 - accuracy: 1.0000\n",
            "Epoch 130: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0412 - accuracy: 1.0000 - val_loss: 1.6699 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 131/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0313 - accuracy: 1.0000\n",
            "Epoch 131: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 1.7879 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 132/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0205 - accuracy: 1.0000\n",
            "Epoch 132: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 1.6055 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 133/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0573 - accuracy: 1.0000\n",
            "Epoch 133: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0518 - accuracy: 1.0000 - val_loss: 1.6804 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 134/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0611 - accuracy: 1.0000\n",
            "Epoch 134: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 1.7379 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 135/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 135: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 1.6140 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 136/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0776 - accuracy: 1.0000\n",
            "Epoch 136: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0694 - accuracy: 1.0000 - val_loss: 1.6631 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 137/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000\n",
            "Epoch 137: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 1.6862 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 138/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0270 - accuracy: 1.0000\n",
            "Epoch 138: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 1.5148 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 139/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
            "Epoch 139: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 1.5021 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 140/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0201 - accuracy: 1.0000\n",
            "Epoch 140: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 1.5546 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 141/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0596 - accuracy: 1.0000\n",
            "Epoch 141: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0551 - accuracy: 1.0000 - val_loss: 1.7962 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 142/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 142: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 2.0246 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 143/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0255 - accuracy: 1.0000\n",
            "Epoch 143: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 2.0282 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 144/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0254 - accuracy: 1.0000\n",
            "Epoch 144: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 2.0514 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 145/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000\n",
            "Epoch 145: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 2.1358 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 146/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0996 - accuracy: 0.9375\n",
            "Epoch 146: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0900 - accuracy: 0.9444 - val_loss: 2.1923 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 147/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000\n",
            "Epoch 147: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 2.2343 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 148/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
            "Epoch 148: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 2.2398 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 149/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 149: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 2.2445 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 150/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 150: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 2.2398 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 151/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0196 - accuracy: 1.0000\n",
            "Epoch 151: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 2.3037 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 152/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 152: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 2.3906 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 153/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000\n",
            "Epoch 153: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 2.3521 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 154/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 154: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 2.3778 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 155/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 155: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 2.3358 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 156/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0215 - accuracy: 1.0000\n",
            "Epoch 156: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 2.3365 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 157/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0245 - accuracy: 1.0000\n",
            "Epoch 157: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 2.3799 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 158/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 158: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 2.1793 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 159/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0466 - accuracy: 1.0000\n",
            "Epoch 159: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 1.9867 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 160/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 160: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 2.2076 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 161/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
            "Epoch 161: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 2.1487 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 162/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 1.0000\n",
            "Epoch 162: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 2.0980 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 163/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0219 - accuracy: 1.0000\n",
            "Epoch 163: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 2.2685 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 164/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 164: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 2.4106 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 165/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0188 - accuracy: 1.0000\n",
            "Epoch 165: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 2.3273 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 166/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 166: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 2.4442 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 167/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000\n",
            "Epoch 167: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 2.3199 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 168/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0245 - accuracy: 1.0000\n",
            "Epoch 168: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 2.2986 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 169/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0429 - accuracy: 1.0000\n",
            "Epoch 169: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 2.1303 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 170/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 170: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 2.2004 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 171/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 171: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 2.0223 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 172/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0181 - accuracy: 1.0000\n",
            "Epoch 172: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 2.0820 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 173/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 173: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 2.0905 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 174/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 174: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 2.1392 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 175/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0134 - accuracy: 1.0000\n",
            "Epoch 175: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 1.9653 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 176/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 176: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 1.7546 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 177/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0220 - accuracy: 1.0000\n",
            "Epoch 177: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 1.8952 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 178/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000\n",
            "Epoch 178: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 2.0424 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 179/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000\n",
            "Epoch 179: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 2.2083 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 180/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0201 - accuracy: 1.0000\n",
            "Epoch 180: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 2.3012 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 181/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 181: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 2.3402 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 182/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0217 - accuracy: 1.0000\n",
            "Epoch 182: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 2.3519 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 183/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 183: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 2.4475 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 184/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0247 - accuracy: 1.0000\n",
            "Epoch 184: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 2.3945 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 185/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 185: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 2.2831 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 186/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0145 - accuracy: 1.0000\n",
            "Epoch 186: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 2.2534 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 187/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0176 - accuracy: 1.0000\n",
            "Epoch 187: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 2.2439 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 188/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 188: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.2631 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 189/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 189: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 2.2287 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 190/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 1.0000\n",
            "Epoch 190: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 2.3023 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 191/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0201 - accuracy: 1.0000\n",
            "Epoch 191: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 2.3012 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 192/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 192: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 2.2551 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 193/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0297 - accuracy: 1.0000\n",
            "Epoch 193: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 2.1981 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 194/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 194: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.0638 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 195/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 195: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.9179 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 196/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
            "Epoch 196: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 2.0501 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 197/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0196 - accuracy: 1.0000\n",
            "Epoch 197: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 2.0899 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 198/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0162 - accuracy: 1.0000\n",
            "Epoch 198: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 2.3345 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 199/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0178 - accuracy: 1.0000\n",
            "Epoch 199: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 2.4729 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 200/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000\n",
            "Epoch 200: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.5479 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 201/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0146 - accuracy: 1.0000\n",
            "Epoch 201: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 2.6240 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 202/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 202: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 2.4998 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 203/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 203: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 2.3623 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 204/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 204: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 2.4968 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 205/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0203 - accuracy: 1.0000\n",
            "Epoch 205: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 2.3744 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 206/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 206: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 2.4177 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 207/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 207: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 2.4057 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 208/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 208: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.3283 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 209/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 209: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 2.3489 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 210/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0226 - accuracy: 1.0000\n",
            "Epoch 210: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 2.5795 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 211/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 211: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 2.5402 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 212/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 212: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.5139 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 213/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 213: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 2.4903 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 214/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 214: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 2.3272 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 215/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 215: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 2.5270 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 216/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 216: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 2.4587 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 217/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0228 - accuracy: 1.0000\n",
            "Epoch 217: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 2.3929 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 218/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 218: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 2.2427 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 219/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0130 - accuracy: 1.0000\n",
            "Epoch 219: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 1.9729 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 220/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0181 - accuracy: 1.0000\n",
            "Epoch 220: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 2.1318 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 221/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 221: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 2.2413 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 222/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 222: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.4061 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 223/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 223: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 2.3221 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 224/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 224: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.1697 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 225/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0211 - accuracy: 1.0000\n",
            "Epoch 225: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 2.2106 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 226/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 226: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.2688 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 227/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0367 - accuracy: 1.0000\n",
            "Epoch 227: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 2.4010 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 228/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0239 - accuracy: 1.0000\n",
            "Epoch 228: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 2.2498 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 229/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 229: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.4038 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 230/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0234 - accuracy: 1.0000\n",
            "Epoch 230: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 2.5770 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 231/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0163 - accuracy: 1.0000\n",
            "Epoch 231: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 2.5767 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 232/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 232: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 2.3314 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 233/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000\n",
            "Epoch 233: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 2.4870 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 234/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 234: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 2.7706 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 235/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 235: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 2.7389 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 236/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 236: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.7266 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 237/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0671 - accuracy: 1.0000\n",
            "Epoch 237: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0607 - accuracy: 1.0000 - val_loss: 2.6347 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 238/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 238: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.5905 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 239/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 239: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 2.3939 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 240/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 240: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 2.3223 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 241/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 241: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 2.1909 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 242/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 242: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 2.2152 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 243/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 243: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 2.3317 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 244/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 244: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 2.3988 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 245/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0130 - accuracy: 1.0000\n",
            "Epoch 245: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 2.6004 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 246/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 246: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.7860 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 247/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 247: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 2.8915 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 248/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 248: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 2.8883 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 249/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 249: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.1098 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 250/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 250: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 3.2060 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 251/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 251: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 3.2287 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 252/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 252: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 3.3696 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 253/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 253: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.2837 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 254/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 254: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 3.3093 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 255/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 255: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.2899 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 256/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 256: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 3.0977 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 257/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0172 - accuracy: 1.0000\n",
            "Epoch 257: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 3.0236 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 258/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 258: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 2.8084 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 259/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0195 - accuracy: 1.0000\n",
            "Epoch 259: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 2.8665 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 260/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 260: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 2.8755 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 261/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 261: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.8871 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 262/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 262: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.0093 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 263/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 263: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 2.9450 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 264/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 264: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.0920 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 265/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 265: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.7901 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 266/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 266: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 3.0327 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 267/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 267: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 3.0366 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 268/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 268: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.9853 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 269/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 269: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.0492 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 270/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 270: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 3.0066 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 271/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0173 - accuracy: 1.0000\n",
            "Epoch 271: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 3.0042 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 272/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 272: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 2.9281 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 273/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 273: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.8847 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 274/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 274: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.8555 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 275/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 275: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 2.9561 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 276/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 276: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.0398 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 277/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 277: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 2.8640 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 278/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 278: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.9653 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 279/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 279: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.9169 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 280/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 280: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 2.9389 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 281/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 281: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.0197 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 282/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 282: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.8973 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 283/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 283: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.8701 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 284/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 284: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.8870 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 285/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 285: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 2.7684 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 286/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 286: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.9642 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 287/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 1.0000\n",
            "Epoch 287: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 3.0964 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 288/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 288: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 2.9446 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 289/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 289: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 2.8956 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 290/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 290: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 3.0785 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 291/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 291: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.1142 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 292/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 292: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.1445 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 293/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 293: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.2709 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 294/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0154 - accuracy: 1.0000\n",
            "Epoch 294: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 3.2799 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 295/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 295: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.9666 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 296/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 296: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 2.9263 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 297/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 297: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 2.7714 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 298/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 298: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.8169 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 299/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 299: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 2.9141 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 300/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 300: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.9904 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 301/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 301: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.9937 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 302/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 302: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.0823 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 303/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 303: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.9809 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 304/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 304: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.8002 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 305/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 305: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.7640 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 306/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 306: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 2.6620 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 307/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 307: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.6001 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 308/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 308: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.5481 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 309/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 309: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 2.6611 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 310/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 310: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.7500 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 311/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 311: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.7349 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 312/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 312: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 2.7884 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 313/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 313: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.8219 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 314/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 314: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.9675 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 315/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 315: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 2.8585 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 316/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 316: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 2.7409 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 317/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 317: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.6818 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 318/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 318: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.7898 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 319/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 319: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.5695 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 320/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 320: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.5978 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 321/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 321: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.4675 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 322/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 322: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 2.5954 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 323/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 323: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.6998 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 324/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 324: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.6489 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 325/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 325: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.7063 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 326/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 326: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.6539 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 327/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 327: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.6503 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 328/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 328: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 2.5943 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 329/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 329: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 2.5760 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 330/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 330: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.5264 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 331/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 331: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 2.6214 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 332/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 332: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.4928 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 333/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 333: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.4776 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 334/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0198 - accuracy: 1.0000\n",
            "Epoch 334: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 2.5206 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 335/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 335: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.4799 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 336/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 336: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.5949 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 337/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 337: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.5860 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 338/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 338: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 2.5857 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 339/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 339: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.4712 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 340/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 340: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.5848 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 341/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 341: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 2.5332 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 342/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 342: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.5560 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 343/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 343: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.5582 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 344/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 1.0000\n",
            "Epoch 344: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 2.2510 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 345/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 345: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.3649 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 346/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 346: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 2.3637 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 347/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 347: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.3036 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 348/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 348: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.2369 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 349/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 349: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.1818 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 350/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 350: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.1614 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 351/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 351: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 2.1849 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 352/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 352: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.5280 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 353/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 353: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.4582 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 354/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 354: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.4480 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 355/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 355: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.4370 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 356/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 356: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 2.3865 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 357/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 357: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.3344 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 358/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 358: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.3504 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 359/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 359: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.2542 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 360/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 360: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.2601 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 361/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 361: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.3436 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 362/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 362: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.2762 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 363/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 363: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.3325 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 364/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 364: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.6140 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 365/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 365: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.8453 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 366/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 366: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.7797 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 367/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 367: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 79ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.8061 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 368/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 368: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.8697 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 369/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 369: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 2.6416 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 370/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 370: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.5112 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 371/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000\n",
            "Epoch 371: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 2.5624 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 372/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 372: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.4936 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 373/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 373: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.6094 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 374/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 374: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.6707 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 375/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 375: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.6544 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 376/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 376: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.5873 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 377/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 377: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.3592 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 378/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 378: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.4193 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 379/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 379: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.5096 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 380/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 380: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 2.8971 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 381/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 381: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.0938 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 382/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 382: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.2977 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 383/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 383: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.4560 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 384/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 384: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.4303 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 385/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 385: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.3980 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 386/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 386: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.4584 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 387/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9444\n",
            "Epoch 387: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0575 - accuracy: 0.9444 - val_loss: 3.4008 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 388/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 388: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.6854 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 389/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0204 - accuracy: 1.0000\n",
            "Epoch 389: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 2.6929 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 390/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 390: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.8332 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 391/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 391: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 2.7742 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 392/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 1.0000\n",
            "Epoch 392: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 2.6057 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 393/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 393: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 2.5357 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 394/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 394: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.6924 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 395/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 395: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.8753 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 396/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 396: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.9474 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 397/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 397: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.8732 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 398/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 398: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 2.9383 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 399/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 399: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.9034 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 400/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 400: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.1155 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 401/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 401: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.9211 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 402/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 402: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.9124 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 403/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 403: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.6991 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 404/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 404: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.6722 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 405/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 405: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.4527 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 406/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 406: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.3970 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 407/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 407: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.2144 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 408/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 408: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 2.2577 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 409/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 409: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.1975 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 410/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 410: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.9813 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 411/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 411: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.0556 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 412/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 412: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.1423 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 413/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 413: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.2307 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 414/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 414: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.2817 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 415/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 415: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 2.2583 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 416/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 416: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.3424 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 417/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 417: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.3442 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 418/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 418: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 2.2956 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 419/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 419: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.4742 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 420/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 420: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 2.1772 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 421/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 421: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.1455 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 422/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 422: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.3488 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 423/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 423: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.4509 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 424/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 424: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.3347 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 425/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 425: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.2885 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 426/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 426: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 2.4831 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 427/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 427: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.4013 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 428/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 428: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.5566 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 429/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 429: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.5696 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 430/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 430: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 2.5730 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 431/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 431: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.6708 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 432/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 432: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 2.6886 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 433/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 433: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.8054 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 434/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 434: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.7113 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 435/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 435: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.9659 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 436/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 436: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.9832 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 437/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 437: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.9190 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 438/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 438: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.8218 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 439/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 439: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.7185 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 440/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 440: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.5456 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 441/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 441: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.3451 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 442/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 442: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.2324 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 443/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 443: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.2206 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 444/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 444: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.2034 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 445/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 445: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.2674 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 446/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 446: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.1432 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 447/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 447: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.3659 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 448/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 448: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.4358 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 449/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 449: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.5787 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 450/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 450: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 2.4599 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 451/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 451: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.3428 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 452/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 452: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.2572 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 453/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 453: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.0638 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 454/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 454: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.9091 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 455/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 455: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.0638 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 456/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 456: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 2.0806 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 457/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 457: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.9792 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 458/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 458: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.9169 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 459/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 459: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.7995 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 460/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 460: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.9485 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 461/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000    \n",
            "Epoch 461: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.9430 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 462/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 462: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.0478 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 463/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 463: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.9557 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 464/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 464: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.9908 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 465/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 465: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 2.0006 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 466/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 466: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.1735 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 467/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 467: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 2.2625 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 468/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 468: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.3329 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 469/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 469: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.1545 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 470/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 470: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.2264 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 471/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 471: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.3517 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 472/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.5510e-04 - accuracy: 1.0000\n",
            "Epoch 472: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 6.8270e-04 - accuracy: 1.0000 - val_loss: 2.2945 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 473/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 473: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.1701 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 474/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 474: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.2294 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 475/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 475: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.2915 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 476/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 476: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.4692 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 477/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 477: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.3025 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 478/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 478: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.3949 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 479/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 479: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 2.5038 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 480/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 480: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.4295 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 481/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 481: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.2990 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 482/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000    \n",
            "Epoch 482: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.5124 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 483/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 483: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.5025 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 484/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 484: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.4431 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 485/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 485: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.5752 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 486/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 486: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.4164 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 487/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 487: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.4104 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 488/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.5254e-04 - accuracy: 1.0000\n",
            "Epoch 488: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 8.5254e-04 - accuracy: 1.0000 - val_loss: 2.4206 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 489/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 489: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.3713 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 490/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 490: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.4477 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 491/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 491: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.5805 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 492/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 492: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.5727 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 493/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 493: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.5340 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 494/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 494: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.6131 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 495/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 495: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.4774 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 496/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 496: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.5979 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 497/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 497: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.6557 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 498/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000    \n",
            "Epoch 498: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.4040 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 499/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 499: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.3444 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 500/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 500: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.2254 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 501/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 501: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.3218 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 502/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 502: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.2052 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 503/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 503: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.1489 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 504/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 504: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.1021 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 505/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 505: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.1125 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 506/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 506: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.1265 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 507/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 507: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.0864 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 508/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 508: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.1926 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 509/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.9133e-04 - accuracy: 1.0000\n",
            "Epoch 509: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 8.4043e-04 - accuracy: 1.0000 - val_loss: 2.2061 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 510/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000    \n",
            "Epoch 510: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.1503 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 511/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 511: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.1350 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 512/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000    \n",
            "Epoch 512: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.9251 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 513/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 513: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.7930 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 514/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 514: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 2.0120 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 515/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 515: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.2230 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 516/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 516: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.4135 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 517/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 517: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.3314 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 518/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 518: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.3273 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 519/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 519: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.4049 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 520/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 520: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.3218 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 521/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 521: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.3223 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 522/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 522: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.3445 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 523/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 523: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.4712 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 524/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 524: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 2.4272 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 525/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 525: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.4381 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 526/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 526: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.4285 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 527/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 527: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.5124 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 528/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 528: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.5863 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 529/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.4468e-04 - accuracy: 1.0000\n",
            "Epoch 529: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.4650 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 530/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 530: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.3380 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 531/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 531: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.3823 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 532/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 532: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.2503 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 533/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000    \n",
            "Epoch 533: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.2804 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 534/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 534: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.3411 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 535/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 535: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.2934 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 536/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 536: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.1942 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 537/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 537: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.3543 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 538/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 538: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.4479 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 539/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.4641e-04 - accuracy: 1.0000\n",
            "Epoch 539: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.2112e-04 - accuracy: 1.0000 - val_loss: 2.5944 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 540/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 540: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.5356 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 541/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.7988e-04 - accuracy: 1.0000\n",
            "Epoch 541: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 5.5565e-04 - accuracy: 1.0000 - val_loss: 2.4058 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 542/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 542: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.4508 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 543/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 543: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 2.5504 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 544/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 544: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.6283 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 545/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000    \n",
            "Epoch 545: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.5916 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 546/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 546: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.3169 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 547/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 547: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.3291 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 548/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 548: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.4040 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 549/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 549: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.2669 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 550/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 550: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.3538 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 551/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 551: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.2818 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 552/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 552: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.3425 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 553/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.6912e-04 - accuracy: 1.0000\n",
            "Epoch 553: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 9.6912e-04 - accuracy: 1.0000 - val_loss: 2.2566 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 554/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 554: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.3664 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 555/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 555: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.3590 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 556/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 556: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.3455 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 557/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.4061e-04 - accuracy: 1.0000\n",
            "Epoch 557: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 8.3113e-04 - accuracy: 1.0000 - val_loss: 2.3283 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 558/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 558: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.3610 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 559/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 559: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.2850 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 560/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 560: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.3281 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 561/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 561: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.4138 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 562/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 562: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.4765 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 563/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 563: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.6150 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 564/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 564: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.6086 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 565/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 565: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.7270 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 566/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 566: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.7246 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 567/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.7154e-04 - accuracy: 1.0000\n",
            "Epoch 567: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 9.9802e-04 - accuracy: 1.0000 - val_loss: 2.5745 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 568/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.7859e-04 - accuracy: 1.0000\n",
            "Epoch 568: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 9.2160e-04 - accuracy: 1.0000 - val_loss: 2.4183 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 569/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 569: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.3332 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 570/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 570: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.2125 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 571/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 571: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 2.4291 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 572/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000    \n",
            "Epoch 572: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.3496 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 573/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000    \n",
            "Epoch 573: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.2689 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 574/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 574: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.3522 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 575/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000    \n",
            "Epoch 575: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.4042 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 576/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 576: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.5057 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 577/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 577: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.5115 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 578/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 578: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.4666 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 579/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 579: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.5459 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 580/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 580: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.6248 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 581/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 581: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.5296 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 582/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 582: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.3890 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 583/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000    \n",
            "Epoch 583: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.5578 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 584/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.7186e-04 - accuracy: 1.0000\n",
            "Epoch 584: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 7.7186e-04 - accuracy: 1.0000 - val_loss: 2.6601 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 585/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 585: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.5642 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 586/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 586: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.4932 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 587/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 587: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.4654 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 588/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 588: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.3855 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 589/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 589: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.2295 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 590/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6376e-04 - accuracy: 1.0000\n",
            "Epoch 590: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 4.6293e-04 - accuracy: 1.0000 - val_loss: 2.3498 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 591/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 591: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.3793 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 592/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.0554e-04 - accuracy: 1.0000\n",
            "Epoch 592: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.3627 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 593/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 593: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.4059 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 594/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 594: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.4452 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 595/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 595: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.4248 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 596/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 596: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.3717 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 597/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 597: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.5281 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 598/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.9159e-04 - accuracy: 1.0000\n",
            "Epoch 598: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.1230e-04 - accuracy: 1.0000 - val_loss: 2.3992 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 599/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5468e-04 - accuracy: 1.0000\n",
            "Epoch 599: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.4684e-04 - accuracy: 1.0000 - val_loss: 2.4027 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 600/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 600: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 2.2720 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 601/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 601: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.2426 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 602/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.6329e-04 - accuracy: 1.0000\n",
            "Epoch 602: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 7.6329e-04 - accuracy: 1.0000 - val_loss: 2.2187 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 603/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 603: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.1729 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 604/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 604: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.0894 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 605/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 605: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.1638 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 606/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 606: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.1181 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 607/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 607: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.0090 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 608/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.1175e-04 - accuracy: 1.0000\n",
            "Epoch 608: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 7.1175e-04 - accuracy: 1.0000 - val_loss: 2.0002 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 609/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 609: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.2090 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 610/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.8216e-04 - accuracy: 1.0000\n",
            "Epoch 610: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 3.8216e-04 - accuracy: 1.0000 - val_loss: 2.2528 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 611/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.6289e-04 - accuracy: 1.0000\n",
            "Epoch 611: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 8.6289e-04 - accuracy: 1.0000 - val_loss: 2.2227 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 612/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 612: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.4000 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 613/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 613: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 9.7824e-04 - accuracy: 1.0000 - val_loss: 2.1922 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 614/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000    \n",
            "Epoch 614: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.3789 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 615/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 615: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.3034 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 616/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 616: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.2917 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 617/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 617: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.1575 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 618/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 618: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 9.7522e-04 - accuracy: 1.0000 - val_loss: 2.1413 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 619/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 619: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.2810 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 620/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 620: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.2092 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 621/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1568e-04 - accuracy: 1.0000\n",
            "Epoch 621: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.3768 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 622/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.2041e-04 - accuracy: 1.0000\n",
            "Epoch 622: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 5.1031e-04 - accuracy: 1.0000 - val_loss: 2.4028 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 623/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 623: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.3345 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 624/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3095e-04 - accuracy: 1.0000\n",
            "Epoch 624: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.9287e-04 - accuracy: 1.0000 - val_loss: 2.4134 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 625/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.3857e-04 - accuracy: 1.0000\n",
            "Epoch 625: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 8.3538e-04 - accuracy: 1.0000 - val_loss: 2.4211 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 626/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.3015e-04 - accuracy: 1.0000\n",
            "Epoch 626: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 7.5425e-04 - accuracy: 1.0000 - val_loss: 2.6488 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 627/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.8539e-04 - accuracy: 1.0000\n",
            "Epoch 627: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 6.2460e-04 - accuracy: 1.0000 - val_loss: 2.5961 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 628/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6014e-04 - accuracy: 1.0000\n",
            "Epoch 628: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 5.0907e-04 - accuracy: 1.0000 - val_loss: 2.6986 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 629/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4918e-04 - accuracy: 1.0000\n",
            "Epoch 629: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.6157e-04 - accuracy: 1.0000 - val_loss: 2.4932 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 630/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.3737e-04 - accuracy: 1.0000\n",
            "Epoch 630: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.5514 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 631/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.8871e-04 - accuracy: 1.0000\n",
            "Epoch 631: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 8.0639e-04 - accuracy: 1.0000 - val_loss: 2.3438 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 632/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 632: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.1842 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 633/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 633: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.1701 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 634/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.6368e-04 - accuracy: 1.0000\n",
            "Epoch 634: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 8.2489e-04 - accuracy: 1.0000 - val_loss: 2.4288 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 635/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.1293e-04 - accuracy: 1.0000\n",
            "Epoch 635: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 8.4683e-04 - accuracy: 1.0000 - val_loss: 2.4586 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 636/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.6300e-04 - accuracy: 1.0000\n",
            "Epoch 636: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.2420e-04 - accuracy: 1.0000 - val_loss: 2.5574 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 637/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 637: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.6903 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 638/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 638: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.5427 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 639/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 639: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.3808 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 640/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.7801e-04 - accuracy: 1.0000\n",
            "Epoch 640: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 9.4689e-04 - accuracy: 1.0000 - val_loss: 2.4506 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 641/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2328e-04 - accuracy: 1.0000\n",
            "Epoch 641: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.5794e-04 - accuracy: 1.0000 - val_loss: 2.2915 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 642/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.5108e-04 - accuracy: 1.0000\n",
            "Epoch 642: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.4685e-04 - accuracy: 1.0000 - val_loss: 2.3856 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 643/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.5826e-04 - accuracy: 1.0000\n",
            "Epoch 643: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.7892e-04 - accuracy: 1.0000 - val_loss: 2.3621 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 644/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.2585e-04 - accuracy: 1.0000\n",
            "Epoch 644: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 7.4747e-04 - accuracy: 1.0000 - val_loss: 2.3110 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 645/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 645: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.3099 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 646/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.5079e-04 - accuracy: 1.0000\n",
            "Epoch 646: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 8.1859e-04 - accuracy: 1.0000 - val_loss: 2.2417 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 647/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.0341e-04 - accuracy: 1.0000\n",
            "Epoch 647: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.8609e-04 - accuracy: 1.0000 - val_loss: 2.5074 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 648/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 648: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.3268 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 649/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 649: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 9.9155e-04 - accuracy: 1.0000 - val_loss: 2.5092 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 650/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.0958e-04 - accuracy: 1.0000\n",
            "Epoch 650: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 8.4313e-04 - accuracy: 1.0000 - val_loss: 2.4066 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 651/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.5882e-04 - accuracy: 1.0000\n",
            "Epoch 651: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 8.8971e-04 - accuracy: 1.0000 - val_loss: 2.4564 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 652/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 652: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.4820 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 653/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 653: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.7156 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 654/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.7319e-04 - accuracy: 1.0000\n",
            "Epoch 654: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 5.2291e-04 - accuracy: 1.0000 - val_loss: 2.7874 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 655/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 655: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.6949 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 656/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 656: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6936 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 657/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.2183e-04 - accuracy: 1.0000\n",
            "Epoch 657: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 5.9540e-04 - accuracy: 1.0000 - val_loss: 2.6560 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 658/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.6958e-04 - accuracy: 1.0000\n",
            "Epoch 658: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.2290e-04 - accuracy: 1.0000 - val_loss: 2.7060 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 659/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 659: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.8395 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 660/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.2081e-04 - accuracy: 1.0000\n",
            "Epoch 660: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.2261e-04 - accuracy: 1.0000 - val_loss: 2.7980 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 661/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 661: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6813 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 662/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.6044e-04 - accuracy: 1.0000\n",
            "Epoch 662: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 6.9942e-04 - accuracy: 1.0000 - val_loss: 2.6559 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 663/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.0831e-04 - accuracy: 1.0000\n",
            "Epoch 663: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 5.2312e-04 - accuracy: 1.0000 - val_loss: 2.6161 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 664/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.1992e-04 - accuracy: 1.0000\n",
            "Epoch 664: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 7.1353e-04 - accuracy: 1.0000 - val_loss: 2.5932 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 665/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.0266e-04 - accuracy: 1.0000\n",
            "Epoch 665: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 8.3183e-04 - accuracy: 1.0000 - val_loss: 2.7074 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 666/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3272e-04 - accuracy: 1.0000\n",
            "Epoch 666: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 3.8818e-04 - accuracy: 1.0000 - val_loss: 2.7003 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 667/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000    \n",
            "Epoch 667: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.5643 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 668/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000    \n",
            "Epoch 668: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.3323 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 669/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.2490e-04 - accuracy: 1.0000\n",
            "Epoch 669: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 8.6823e-04 - accuracy: 1.0000 - val_loss: 2.3934 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 670/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3420e-04 - accuracy: 1.0000\n",
            "Epoch 670: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.6066e-04 - accuracy: 1.0000 - val_loss: 2.2278 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 671/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 671: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.3765 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 672/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 672: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.3158 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 673/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.0231e-04 - accuracy: 1.0000\n",
            "Epoch 673: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.4532e-04 - accuracy: 1.0000 - val_loss: 2.2384 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 674/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.9481e-04 - accuracy: 1.0000\n",
            "Epoch 674: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.7068e-04 - accuracy: 1.0000 - val_loss: 2.0918 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 675/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.8069e-04 - accuracy: 1.0000\n",
            "Epoch 675: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 7.1021e-04 - accuracy: 1.0000 - val_loss: 2.0313 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 676/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.1163e-04 - accuracy: 1.0000\n",
            "Epoch 676: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.9818e-04 - accuracy: 1.0000 - val_loss: 2.2778 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 677/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.8993e-04 - accuracy: 1.0000\n",
            "Epoch 677: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 8.2161e-04 - accuracy: 1.0000 - val_loss: 2.5674 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 678/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 678: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.5871 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 679/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.4626e-04 - accuracy: 1.0000\n",
            "Epoch 679: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 8.4626e-04 - accuracy: 1.0000 - val_loss: 2.3914 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 680/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 680: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.5210 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 681/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.8638e-04 - accuracy: 1.0000\n",
            "Epoch 681: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 8.5315e-04 - accuracy: 1.0000 - val_loss: 2.5834 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 682/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000    \n",
            "Epoch 682: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.4626 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 683/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.2220e-04 - accuracy: 1.0000\n",
            "Epoch 683: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 6.2220e-04 - accuracy: 1.0000 - val_loss: 2.6489 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 684/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.0082e-04 - accuracy: 1.0000\n",
            "Epoch 684: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 8.0082e-04 - accuracy: 1.0000 - val_loss: 2.5872 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 685/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 685: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.7298 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 686/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.4505e-04 - accuracy: 1.0000\n",
            "Epoch 686: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 5.0566e-04 - accuracy: 1.0000 - val_loss: 2.7117 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 687/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.3986e-04 - accuracy: 1.0000\n",
            "Epoch 687: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 4.3986e-04 - accuracy: 1.0000 - val_loss: 2.7956 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 688/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 688: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.6842 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 689/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.1962e-04 - accuracy: 1.0000\n",
            "Epoch 689: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 5.7638e-04 - accuracy: 1.0000 - val_loss: 2.6836 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 690/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.4142e-04 - accuracy: 1.0000\n",
            "Epoch 690: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 4.2438e-04 - accuracy: 1.0000 - val_loss: 2.7074 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 691/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.6686e-04 - accuracy: 1.0000\n",
            "Epoch 691: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 6.6686e-04 - accuracy: 1.0000 - val_loss: 2.3892 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 692/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 692: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.6918 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 693/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3622e-04 - accuracy: 1.0000\n",
            "Epoch 693: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.6065e-04 - accuracy: 1.0000 - val_loss: 2.5935 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 694/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6324e-04 - accuracy: 1.0000\n",
            "Epoch 694: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 5.2777e-04 - accuracy: 1.0000 - val_loss: 2.6915 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 695/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.8377e-04 - accuracy: 1.0000\n",
            "Epoch 695: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 7.1685e-04 - accuracy: 1.0000 - val_loss: 3.0301 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 696/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3864e-04 - accuracy: 1.0000\n",
            "Epoch 696: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.0772e-04 - accuracy: 1.0000 - val_loss: 3.0019 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 697/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.5970e-04 - accuracy: 1.0000\n",
            "Epoch 697: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.9383e-04 - accuracy: 1.0000 - val_loss: 2.9259 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 698/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.0256e-04 - accuracy: 1.0000\n",
            "Epoch 698: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.0256e-04 - accuracy: 1.0000 - val_loss: 2.9776 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 699/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.4536e-04 - accuracy: 1.0000\n",
            "Epoch 699: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.5838e-04 - accuracy: 1.0000 - val_loss: 2.8284 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 700/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.3443e-04 - accuracy: 1.0000\n",
            "Epoch 700: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 8.3588e-04 - accuracy: 1.0000 - val_loss: 2.7534 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 701/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.8876e-04 - accuracy: 1.0000\n",
            "Epoch 701: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.4687e-04 - accuracy: 1.0000 - val_loss: 2.7416 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 702/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.6641e-04 - accuracy: 1.0000\n",
            "Epoch 702: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.9163 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 703/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.2977e-04 - accuracy: 1.0000\n",
            "Epoch 703: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.9215e-04 - accuracy: 1.0000 - val_loss: 3.0028 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 704/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.9178e-04 - accuracy: 1.0000\n",
            "Epoch 704: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 6.4555e-04 - accuracy: 1.0000 - val_loss: 2.8149 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 705/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.8786e-04 - accuracy: 1.0000\n",
            "Epoch 705: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 5.3087e-04 - accuracy: 1.0000 - val_loss: 2.8241 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 706/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.2489e-04 - accuracy: 1.0000\n",
            "Epoch 706: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 8.9058e-04 - accuracy: 1.0000 - val_loss: 2.9163 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 707/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.6328e-04 - accuracy: 1.0000\n",
            "Epoch 707: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 5.2491e-04 - accuracy: 1.0000 - val_loss: 2.9659 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 708/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.8028e-04 - accuracy: 1.0000\n",
            "Epoch 708: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.4722e-04 - accuracy: 1.0000 - val_loss: 2.8737 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 709/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 709: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.8331 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 710/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.9113e-04 - accuracy: 1.0000\n",
            "Epoch 710: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.5524e-04 - accuracy: 1.0000 - val_loss: 2.6993 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 711/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.9382e-04 - accuracy: 1.0000\n",
            "Epoch 711: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 5.6536e-04 - accuracy: 1.0000 - val_loss: 2.7387 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 712/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.4792e-04 - accuracy: 1.0000\n",
            "Epoch 712: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 7.8088e-04 - accuracy: 1.0000 - val_loss: 2.5659 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 713/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000    \n",
            "Epoch 713: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.7628 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 714/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.6882e-04 - accuracy: 1.0000\n",
            "Epoch 714: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 5.1406e-04 - accuracy: 1.0000 - val_loss: 2.8835 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 715/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.4007e-04 - accuracy: 1.0000\n",
            "Epoch 715: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 7.9147e-04 - accuracy: 1.0000 - val_loss: 2.9576 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 716/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.6772e-04 - accuracy: 1.0000\n",
            "Epoch 716: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 7.1266e-04 - accuracy: 1.0000 - val_loss: 2.9258 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 717/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.7011e-04 - accuracy: 1.0000\n",
            "Epoch 717: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.3341e-04 - accuracy: 1.0000 - val_loss: 2.7602 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 718/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 718: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.6853 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 719/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.0680e-04 - accuracy: 1.0000\n",
            "Epoch 719: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.4595e-04 - accuracy: 1.0000 - val_loss: 2.6388 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 720/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 720: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.6491 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 721/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000    \n",
            "Epoch 721: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.7013 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 722/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.9017e-04 - accuracy: 1.0000\n",
            "Epoch 722: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.6053e-04 - accuracy: 1.0000 - val_loss: 2.6156 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 723/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.7074e-04 - accuracy: 1.0000\n",
            "Epoch 723: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 5.8038e-04 - accuracy: 1.0000 - val_loss: 2.6746 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 724/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.0355e-04 - accuracy: 1.0000\n",
            "Epoch 724: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 7.0355e-04 - accuracy: 1.0000 - val_loss: 2.5370 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 725/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.0036e-04 - accuracy: 1.0000\n",
            "Epoch 725: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 2.7479 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 726/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.3744e-04 - accuracy: 1.0000\n",
            "Epoch 726: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 5.3744e-04 - accuracy: 1.0000 - val_loss: 2.6138 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 727/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 727: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7040 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 728/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 728: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.5854 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 729/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 729: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.6380 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 730/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.0416e-04 - accuracy: 1.0000\n",
            "Epoch 730: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 5.0416e-04 - accuracy: 1.0000 - val_loss: 2.7574 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 731/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 731: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.9071 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 732/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 732: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.8855 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 733/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.6271e-04 - accuracy: 1.0000\n",
            "Epoch 733: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.3097e-04 - accuracy: 1.0000 - val_loss: 2.8457 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 734/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 734: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 2.8945 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 735/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 735: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.7265 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 736/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 736: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.8004 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 737/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 737: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7046 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 738/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1988e-04 - accuracy: 1.0000\n",
            "Epoch 738: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.6736e-04 - accuracy: 1.0000 - val_loss: 2.7229 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 739/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.1712e-04 - accuracy: 1.0000\n",
            "Epoch 739: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 5.1874e-04 - accuracy: 1.0000 - val_loss: 2.8287 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 740/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3629e-04 - accuracy: 1.0000\n",
            "Epoch 740: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 5.1780e-04 - accuracy: 1.0000 - val_loss: 2.8080 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 741/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.9948e-04 - accuracy: 1.0000\n",
            "Epoch 741: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 5.4444e-04 - accuracy: 1.0000 - val_loss: 2.8500 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 742/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.5797e-04 - accuracy: 1.0000\n",
            "Epoch 742: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.0059e-04 - accuracy: 1.0000 - val_loss: 2.9014 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 743/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 743: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9744 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 744/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.5385e-04 - accuracy: 1.0000\n",
            "Epoch 744: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 9.6226e-04 - accuracy: 1.0000 - val_loss: 2.9778 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 745/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.4924e-04 - accuracy: 1.0000\n",
            "Epoch 745: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 5.1576e-04 - accuracy: 1.0000 - val_loss: 2.8134 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 746/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.7226e-04 - accuracy: 1.0000\n",
            "Epoch 746: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.1695e-04 - accuracy: 1.0000 - val_loss: 2.8660 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 747/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.4183e-04 - accuracy: 1.0000\n",
            "Epoch 747: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.1803e-04 - accuracy: 1.0000 - val_loss: 2.9002 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 748/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.2465e-04 - accuracy: 1.0000\n",
            "Epoch 748: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.6852e-04 - accuracy: 1.0000 - val_loss: 3.0422 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 749/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.1611e-04 - accuracy: 1.0000\n",
            "Epoch 749: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.8229e-04 - accuracy: 1.0000 - val_loss: 3.1540 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 750/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.7554e-04 - accuracy: 1.0000\n",
            "Epoch 750: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.7554e-04 - accuracy: 1.0000 - val_loss: 3.1594 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 751/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.4420e-04 - accuracy: 1.0000\n",
            "Epoch 751: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.5969e-04 - accuracy: 1.0000 - val_loss: 3.1600 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 752/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.3153e-04 - accuracy: 1.0000\n",
            "Epoch 752: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 4.8626e-04 - accuracy: 1.0000 - val_loss: 3.1441 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 753/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.2886e-04 - accuracy: 1.0000\n",
            "Epoch 753: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 8.9243e-04 - accuracy: 1.0000 - val_loss: 3.1679 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 754/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 754: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.3445 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 755/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.9828e-04 - accuracy: 1.0000\n",
            "Epoch 755: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.4615 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 756/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.0757e-04 - accuracy: 1.0000\n",
            "Epoch 756: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 7.6884e-04 - accuracy: 1.0000 - val_loss: 3.4757 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 757/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1589e-04 - accuracy: 1.0000\n",
            "Epoch 757: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.4999 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 758/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9654e-04 - accuracy: 1.0000\n",
            "Epoch 758: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.1098e-04 - accuracy: 1.0000 - val_loss: 3.1996 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 759/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.6236e-04 - accuracy: 1.0000\n",
            "Epoch 759: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 7.8279e-04 - accuracy: 1.0000 - val_loss: 3.1348 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 760/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.6154e-04 - accuracy: 1.0000\n",
            "Epoch 760: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 8.6154e-04 - accuracy: 1.0000 - val_loss: 3.1265 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 761/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 761: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.5162 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 762/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.4557e-04 - accuracy: 1.0000\n",
            "Epoch 762: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 5.8341e-04 - accuracy: 1.0000 - val_loss: 3.4360 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 763/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.7298e-04 - accuracy: 1.0000\n",
            "Epoch 763: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 9.1501e-04 - accuracy: 1.0000 - val_loss: 3.2739 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 764/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.8809e-04 - accuracy: 1.0000\n",
            "Epoch 764: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 5.4879e-04 - accuracy: 1.0000 - val_loss: 3.2692 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 765/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.1303e-04 - accuracy: 1.0000\n",
            "Epoch 765: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.5246e-04 - accuracy: 1.0000 - val_loss: 3.1917 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 766/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1150e-04 - accuracy: 1.0000\n",
            "Epoch 766: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.7047e-04 - accuracy: 1.0000 - val_loss: 3.1141 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 767/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.4054e-04 - accuracy: 1.0000\n",
            "Epoch 767: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.2964e-04 - accuracy: 1.0000 - val_loss: 2.8406 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 768/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.1309e-04 - accuracy: 1.0000\n",
            "Epoch 768: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.9795e-04 - accuracy: 1.0000 - val_loss: 2.7132 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 769/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.2688e-04 - accuracy: 1.0000\n",
            "Epoch 769: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 2.2688e-04 - accuracy: 1.0000 - val_loss: 2.8198 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 770/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.4262e-04 - accuracy: 1.0000\n",
            "Epoch 770: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.0521e-04 - accuracy: 1.0000 - val_loss: 2.6916 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 771/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 771: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.7045 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 772/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.5407e-04 - accuracy: 1.0000\n",
            "Epoch 772: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 4.5407e-04 - accuracy: 1.0000 - val_loss: 2.8139 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 773/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6856e-04 - accuracy: 1.0000\n",
            "Epoch 773: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.0698e-04 - accuracy: 1.0000 - val_loss: 2.6350 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 774/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1509e-04 - accuracy: 1.0000\n",
            "Epoch 774: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 3.1562e-04 - accuracy: 1.0000 - val_loss: 2.6875 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 775/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.8577e-04 - accuracy: 1.0000\n",
            "Epoch 775: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.9012e-04 - accuracy: 1.0000 - val_loss: 2.8131 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 776/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.0002e-04 - accuracy: 1.0000\n",
            "Epoch 776: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 5.3832e-04 - accuracy: 1.0000 - val_loss: 2.7226 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 777/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8809e-04 - accuracy: 1.0000\n",
            "Epoch 777: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.6007e-04 - accuracy: 1.0000 - val_loss: 2.8293 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 778/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.3523e-04 - accuracy: 1.0000\n",
            "Epoch 778: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 7.6765e-04 - accuracy: 1.0000 - val_loss: 3.0014 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 779/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7291e-04 - accuracy: 1.0000\n",
            "Epoch 779: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 3.7814e-04 - accuracy: 1.0000 - val_loss: 2.8944 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 780/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6501e-04 - accuracy: 1.0000\n",
            "Epoch 780: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.5249e-04 - accuracy: 1.0000 - val_loss: 2.7721 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 781/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0098e-04 - accuracy: 1.0000\n",
            "Epoch 781: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.1249e-04 - accuracy: 1.0000 - val_loss: 2.9151 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 782/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3250e-04 - accuracy: 1.0000\n",
            "Epoch 782: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.1663e-04 - accuracy: 1.0000 - val_loss: 3.0223 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 783/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1863e-04 - accuracy: 1.0000\n",
            "Epoch 783: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 3.2958e-04 - accuracy: 1.0000 - val_loss: 3.0735 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 784/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.0542e-04 - accuracy: 1.0000\n",
            "Epoch 784: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 8.0951e-04 - accuracy: 1.0000 - val_loss: 3.0713 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 785/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.4005e-04 - accuracy: 1.0000\n",
            "Epoch 785: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 8.0054e-04 - accuracy: 1.0000 - val_loss: 3.1834 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 786/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.6735e-04 - accuracy: 1.0000\n",
            "Epoch 786: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 7.7552e-04 - accuracy: 1.0000 - val_loss: 3.0852 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 787/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.4943e-04 - accuracy: 1.0000\n",
            "Epoch 787: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.7535e-04 - accuracy: 1.0000 - val_loss: 3.1366 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 788/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.2133e-04 - accuracy: 1.0000\n",
            "Epoch 788: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 4.1664e-04 - accuracy: 1.0000 - val_loss: 2.9839 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 789/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 789: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.8819 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 790/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.2192e-04 - accuracy: 1.0000\n",
            "Epoch 790: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 4.6529e-04 - accuracy: 1.0000 - val_loss: 2.8423 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 791/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7161e-04 - accuracy: 1.0000\n",
            "Epoch 791: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.4952e-04 - accuracy: 1.0000 - val_loss: 2.8526 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 792/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.7242e-04 - accuracy: 1.0000\n",
            "Epoch 792: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 5.2229e-04 - accuracy: 1.0000 - val_loss: 2.9876 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 793/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000    \n",
            "Epoch 793: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.3051 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 794/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.6839e-04 - accuracy: 1.0000\n",
            "Epoch 794: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 8.0153e-04 - accuracy: 1.0000 - val_loss: 3.2253 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 795/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.2748e-04 - accuracy: 1.0000\n",
            "Epoch 795: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 7.6995e-04 - accuracy: 1.0000 - val_loss: 3.0409 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 796/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 796: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.9194 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 797/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.4216e-04 - accuracy: 1.0000\n",
            "Epoch 797: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 4.9512e-04 - accuracy: 1.0000 - val_loss: 2.6350 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 798/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.0953e-04 - accuracy: 1.0000\n",
            "Epoch 798: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.6532e-04 - accuracy: 1.0000 - val_loss: 2.6790 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 799/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.0415e-04 - accuracy: 1.0000\n",
            "Epoch 799: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.7825e-04 - accuracy: 1.0000 - val_loss: 2.5782 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 800/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 800: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.5086 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 801/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.3109e-04 - accuracy: 1.0000\n",
            "Epoch 801: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.9413e-04 - accuracy: 1.0000 - val_loss: 2.3547 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 802/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.5594e-04 - accuracy: 1.0000\n",
            "Epoch 802: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 1.5594e-04 - accuracy: 1.0000 - val_loss: 2.4439 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 803/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.7260e-04 - accuracy: 1.0000\n",
            "Epoch 803: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 6.0107e-04 - accuracy: 1.0000 - val_loss: 2.6531 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 804/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.8910e-04 - accuracy: 1.0000\n",
            "Epoch 804: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 7.4493e-04 - accuracy: 1.0000 - val_loss: 2.7129 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 805/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.9688e-04 - accuracy: 1.0000\n",
            "Epoch 805: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.7540e-04 - accuracy: 1.0000 - val_loss: 2.7850 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 806/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.5786e-04 - accuracy: 1.0000\n",
            "Epoch 806: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 7.2290e-04 - accuracy: 1.0000 - val_loss: 2.5474 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 807/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.8176e-04 - accuracy: 1.0000\n",
            "Epoch 807: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 5.2200e-04 - accuracy: 1.0000 - val_loss: 2.4139 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 808/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6504e-04 - accuracy: 1.0000\n",
            "Epoch 808: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.2907e-04 - accuracy: 1.0000 - val_loss: 2.3622 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 809/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.8702e-04 - accuracy: 1.0000\n",
            "Epoch 809: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.4688e-04 - accuracy: 1.0000 - val_loss: 2.6118 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 810/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.9812e-04 - accuracy: 1.0000\n",
            "Epoch 810: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 8.0398e-04 - accuracy: 1.0000 - val_loss: 2.5968 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 811/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3079e-04 - accuracy: 1.0000\n",
            "Epoch 811: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.9567e-04 - accuracy: 1.0000 - val_loss: 2.6822 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 812/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000    \n",
            "Epoch 812: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.6995 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 813/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5680e-04 - accuracy: 1.0000\n",
            "Epoch 813: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.6132e-04 - accuracy: 1.0000 - val_loss: 2.8829 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 814/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.2204e-04 - accuracy: 1.0000\n",
            "Epoch 814: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.0672e-04 - accuracy: 1.0000 - val_loss: 2.7268 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 815/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.8539e-04 - accuracy: 1.0000\n",
            "Epoch 815: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 4.8539e-04 - accuracy: 1.0000 - val_loss: 2.7532 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 816/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 816: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.6260 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 817/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.8012e-04 - accuracy: 1.0000\n",
            "Epoch 817: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 7.2889e-04 - accuracy: 1.0000 - val_loss: 2.7342 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 818/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3416e-04 - accuracy: 1.0000\n",
            "Epoch 818: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.1295e-04 - accuracy: 1.0000 - val_loss: 2.5924 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 819/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1667e-04 - accuracy: 1.0000\n",
            "Epoch 819: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 5.8067e-04 - accuracy: 1.0000 - val_loss: 2.7385 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 820/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1976e-04 - accuracy: 1.0000\n",
            "Epoch 820: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.0085e-04 - accuracy: 1.0000 - val_loss: 2.6923 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 821/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.9951e-04 - accuracy: 1.0000\n",
            "Epoch 821: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.3427e-04 - accuracy: 1.0000 - val_loss: 2.7615 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 822/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.3270e-04 - accuracy: 1.0000\n",
            "Epoch 822: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.8949e-04 - accuracy: 1.0000 - val_loss: 2.7721 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 823/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.7810e-04 - accuracy: 1.0000\n",
            "Epoch 823: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.5613e-04 - accuracy: 1.0000 - val_loss: 2.6941 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 824/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.5487e-04 - accuracy: 1.0000\n",
            "Epoch 824: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.5390e-04 - accuracy: 1.0000 - val_loss: 2.5866 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 825/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.7172e-04 - accuracy: 1.0000\n",
            "Epoch 825: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.6091e-04 - accuracy: 1.0000 - val_loss: 2.6810 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 826/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.2128e-04 - accuracy: 1.0000\n",
            "Epoch 826: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 4.7363e-04 - accuracy: 1.0000 - val_loss: 2.7169 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 827/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.0914e-04 - accuracy: 1.0000\n",
            "Epoch 827: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 5.6382e-04 - accuracy: 1.0000 - val_loss: 2.6713 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 828/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.0125e-04 - accuracy: 1.0000\n",
            "Epoch 828: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.6379e-04 - accuracy: 1.0000 - val_loss: 2.5308 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 829/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0129e-04 - accuracy: 1.0000\n",
            "Epoch 829: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.8993e-04 - accuracy: 1.0000 - val_loss: 2.6228 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 830/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6137e-04 - accuracy: 1.0000\n",
            "Epoch 830: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.6410e-04 - accuracy: 1.0000 - val_loss: 2.8237 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 831/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.4645e-04 - accuracy: 1.0000\n",
            "Epoch 831: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 6.9068e-04 - accuracy: 1.0000 - val_loss: 2.7203 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 832/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.4253e-04 - accuracy: 1.0000\n",
            "Epoch 832: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 7.5427e-04 - accuracy: 1.0000 - val_loss: 2.5788 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 833/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1492e-04 - accuracy: 1.0000\n",
            "Epoch 833: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 6.5502e-04 - accuracy: 1.0000 - val_loss: 2.6614 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 834/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.7056e-04 - accuracy: 1.0000\n",
            "Epoch 834: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 4.4628e-04 - accuracy: 1.0000 - val_loss: 2.5965 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 835/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.2464e-04 - accuracy: 1.0000\n",
            "Epoch 835: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 3.8873e-04 - accuracy: 1.0000 - val_loss: 2.5296 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 836/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.0159e-04 - accuracy: 1.0000\n",
            "Epoch 836: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.1692e-04 - accuracy: 1.0000 - val_loss: 2.5710 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 837/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5328e-04 - accuracy: 1.0000\n",
            "Epoch 837: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 3.4207e-04 - accuracy: 1.0000 - val_loss: 2.7743 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 838/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.7638e-04 - accuracy: 1.0000\n",
            "Epoch 838: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.8357e-04 - accuracy: 1.0000 - val_loss: 2.8700 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 839/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6123e-04 - accuracy: 1.0000\n",
            "Epoch 839: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.3823e-04 - accuracy: 1.0000 - val_loss: 2.5677 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 840/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.6125e-04 - accuracy: 1.0000\n",
            "Epoch 840: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 3.6125e-04 - accuracy: 1.0000 - val_loss: 2.7360 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 841/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.5006e-04 - accuracy: 1.0000\n",
            "Epoch 841: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.5187e-04 - accuracy: 1.0000 - val_loss: 2.4908 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 842/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 842: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.3962 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 843/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5811e-04 - accuracy: 1.0000\n",
            "Epoch 843: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.3334e-04 - accuracy: 1.0000 - val_loss: 2.5866 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 844/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.8116e-04 - accuracy: 1.0000\n",
            "Epoch 844: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 3.8116e-04 - accuracy: 1.0000 - val_loss: 2.6171 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 845/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 845: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.5275 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 846/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.9248e-04 - accuracy: 1.0000\n",
            "Epoch 846: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 2.7514e-04 - accuracy: 1.0000 - val_loss: 2.4383 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 847/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.3801e-04 - accuracy: 1.0000\n",
            "Epoch 847: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 2.3801e-04 - accuracy: 1.0000 - val_loss: 2.6191 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 848/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.7464e-04 - accuracy: 1.0000\n",
            "Epoch 848: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 4.7464e-04 - accuracy: 1.0000 - val_loss: 2.7325 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 849/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6696e-04 - accuracy: 1.0000\n",
            "Epoch 849: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.6584e-04 - accuracy: 1.0000 - val_loss: 2.4903 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 850/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.2209e-04 - accuracy: 1.0000\n",
            "Epoch 850: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 3.2209e-04 - accuracy: 1.0000 - val_loss: 2.5151 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 851/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.8811e-04 - accuracy: 1.0000\n",
            "Epoch 851: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 5.8811e-04 - accuracy: 1.0000 - val_loss: 2.2909 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 852/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.2355e-04 - accuracy: 1.0000\n",
            "Epoch 852: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 6.2355e-04 - accuracy: 1.0000 - val_loss: 2.3336 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 853/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.7685e-04 - accuracy: 1.0000\n",
            "Epoch 853: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 2.7685e-04 - accuracy: 1.0000 - val_loss: 2.3106 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 854/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.8968e-04 - accuracy: 1.0000\n",
            "Epoch 854: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 79ms/step - loss: 5.3216e-04 - accuracy: 1.0000 - val_loss: 2.3702 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 855/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.9103e-04 - accuracy: 1.0000\n",
            "Epoch 855: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 2.9103e-04 - accuracy: 1.0000 - val_loss: 2.5673 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 856/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.1846e-04 - accuracy: 1.0000\n",
            "Epoch 856: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 9.0318e-04 - accuracy: 1.0000 - val_loss: 2.3927 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 857/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9089e-04 - accuracy: 1.0000\n",
            "Epoch 857: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 1.8197e-04 - accuracy: 1.0000 - val_loss: 2.4167 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 858/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1932e-04 - accuracy: 1.0000\n",
            "Epoch 858: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 5.1242e-04 - accuracy: 1.0000 - val_loss: 2.6353 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 859/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6861e-04 - accuracy: 1.0000\n",
            "Epoch 859: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 2.4170e-04 - accuracy: 1.0000 - val_loss: 2.5893 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 860/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4372e-04 - accuracy: 1.0000\n",
            "Epoch 860: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.0829e-04 - accuracy: 1.0000 - val_loss: 2.6224 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 861/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.4362e-04 - accuracy: 1.0000\n",
            "Epoch 861: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.0384e-04 - accuracy: 1.0000 - val_loss: 2.6634 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 862/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 862: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7185 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 863/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.5403e-04 - accuracy: 1.0000\n",
            "Epoch 863: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 5.0054e-04 - accuracy: 1.0000 - val_loss: 2.6035 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 864/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.5637e-04 - accuracy: 1.0000\n",
            "Epoch 864: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.1461e-04 - accuracy: 1.0000 - val_loss: 2.5176 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 865/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.0690e-04 - accuracy: 1.0000\n",
            "Epoch 865: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.1006e-04 - accuracy: 1.0000 - val_loss: 2.6364 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 866/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.9720e-04 - accuracy: 1.0000\n",
            "Epoch 866: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.9043e-04 - accuracy: 1.0000 - val_loss: 2.6564 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 867/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7093e-04 - accuracy: 1.0000\n",
            "Epoch 867: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 4.2423e-04 - accuracy: 1.0000 - val_loss: 2.6907 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 868/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.3028e-04 - accuracy: 1.0000\n",
            "Epoch 868: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 5.6248e-04 - accuracy: 1.0000 - val_loss: 2.5793 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 869/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3795e-04 - accuracy: 1.0000\n",
            "Epoch 869: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.1845e-04 - accuracy: 1.0000 - val_loss: 2.6791 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 870/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0784e-04 - accuracy: 1.0000\n",
            "Epoch 870: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 1.9862e-04 - accuracy: 1.0000 - val_loss: 2.5696 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 871/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1365e-04 - accuracy: 1.0000\n",
            "Epoch 871: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 3.6075e-04 - accuracy: 1.0000 - val_loss: 2.4050 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 872/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.8428e-04 - accuracy: 1.0000\n",
            "Epoch 872: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.4675e-04 - accuracy: 1.0000 - val_loss: 2.5213 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 873/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5074e-04 - accuracy: 1.0000\n",
            "Epoch 873: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.2494e-04 - accuracy: 1.0000 - val_loss: 2.4520 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 874/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.5929e-04 - accuracy: 1.0000\n",
            "Epoch 874: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.1213e-04 - accuracy: 1.0000 - val_loss: 2.5104 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 875/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.2009e-04 - accuracy: 1.0000\n",
            "Epoch 875: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.8784e-04 - accuracy: 1.0000 - val_loss: 2.5198 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 876/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5376e-04 - accuracy: 1.0000\n",
            "Epoch 876: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.2011e-04 - accuracy: 1.0000 - val_loss: 2.5899 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 877/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8340e-04 - accuracy: 1.0000\n",
            "Epoch 877: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 2.6423e-04 - accuracy: 1.0000 - val_loss: 2.5729 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 878/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6374e-04 - accuracy: 1.0000\n",
            "Epoch 878: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.3774e-04 - accuracy: 1.0000 - val_loss: 2.6614 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 879/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.0508e-04 - accuracy: 1.0000\n",
            "Epoch 879: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.9357e-04 - accuracy: 1.0000 - val_loss: 2.6817 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 880/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.8386e-04 - accuracy: 1.0000\n",
            "Epoch 880: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 7.8386e-04 - accuracy: 1.0000 - val_loss: 2.5730 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 881/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8602e-04 - accuracy: 1.0000\n",
            "Epoch 881: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.2455e-04 - accuracy: 1.0000 - val_loss: 2.6046 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 882/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.2836e-04 - accuracy: 1.0000\n",
            "Epoch 882: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.8000e-04 - accuracy: 1.0000 - val_loss: 2.7508 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 883/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8356e-04 - accuracy: 1.0000\n",
            "Epoch 883: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.7710e-04 - accuracy: 1.0000 - val_loss: 2.8203 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 884/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.2203e-04 - accuracy: 1.0000\n",
            "Epoch 884: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.1587e-04 - accuracy: 1.0000 - val_loss: 2.8154 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 885/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.1207e-04 - accuracy: 1.0000\n",
            "Epoch 885: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.3007e-04 - accuracy: 1.0000 - val_loss: 2.7532 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 886/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.3497e-04 - accuracy: 1.0000\n",
            "Epoch 886: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 4.9197e-04 - accuracy: 1.0000 - val_loss: 2.7237 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 887/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3679e-04 - accuracy: 1.0000\n",
            "Epoch 887: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.4634e-04 - accuracy: 1.0000 - val_loss: 2.7368 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 888/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.7430e-04 - accuracy: 1.0000\n",
            "Epoch 888: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 8.6917e-04 - accuracy: 1.0000 - val_loss: 2.8854 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 889/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.4171e-04 - accuracy: 1.0000\n",
            "Epoch 889: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.9243e-04 - accuracy: 1.0000 - val_loss: 2.8212 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 890/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.2148e-04 - accuracy: 1.0000\n",
            "Epoch 890: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 2.0535e-04 - accuracy: 1.0000 - val_loss: 2.9007 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 891/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 891: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 9.0766e-04 - accuracy: 1.0000 - val_loss: 3.0780 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 892/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6389e-04 - accuracy: 1.0000\n",
            "Epoch 892: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.4089e-04 - accuracy: 1.0000 - val_loss: 2.9793 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 893/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.4386e-04 - accuracy: 1.0000\n",
            "Epoch 893: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.8638e-04 - accuracy: 1.0000 - val_loss: 2.9043 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 894/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6039e-04 - accuracy: 1.0000\n",
            "Epoch 894: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.3855e-04 - accuracy: 1.0000 - val_loss: 2.7303 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 895/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1574e-04 - accuracy: 1.0000\n",
            "Epoch 895: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.9456e-04 - accuracy: 1.0000 - val_loss: 2.8018 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 896/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.8824e-04 - accuracy: 1.0000\n",
            "Epoch 896: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.7674e-04 - accuracy: 1.0000 - val_loss: 2.6930 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 897/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.7630e-04 - accuracy: 1.0000\n",
            "Epoch 897: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.0821e-04 - accuracy: 1.0000 - val_loss: 2.5258 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 898/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.2017e-04 - accuracy: 1.0000\n",
            "Epoch 898: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.8649e-04 - accuracy: 1.0000 - val_loss: 2.6019 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 899/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.7515e-04 - accuracy: 1.0000\n",
            "Epoch 899: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.0486e-04 - accuracy: 1.0000 - val_loss: 2.4416 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 900/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8948e-04 - accuracy: 1.0000\n",
            "Epoch 900: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 2.6709e-04 - accuracy: 1.0000 - val_loss: 2.3551 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 901/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.8342e-04 - accuracy: 1.0000\n",
            "Epoch 901: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.7701e-04 - accuracy: 1.0000 - val_loss: 2.3558 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 902/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 902: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 9.5041e-04 - accuracy: 1.0000 - val_loss: 2.5104 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 903/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.0023e-04 - accuracy: 1.0000\n",
            "Epoch 903: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.2419e-04 - accuracy: 1.0000 - val_loss: 2.7293 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 904/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4139e-04 - accuracy: 1.0000\n",
            "Epoch 904: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.2926e-04 - accuracy: 1.0000 - val_loss: 2.8114 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 905/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.8061e-04 - accuracy: 1.0000\n",
            "Epoch 905: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.7715e-04 - accuracy: 1.0000 - val_loss: 2.7272 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 906/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3793e-04 - accuracy: 1.0000\n",
            "Epoch 906: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.2827e-04 - accuracy: 1.0000 - val_loss: 2.4338 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 907/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4159e-04 - accuracy: 1.0000\n",
            "Epoch 907: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.0677e-04 - accuracy: 1.0000 - val_loss: 2.4673 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 908/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6075e-04 - accuracy: 1.0000\n",
            "Epoch 908: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.8782e-04 - accuracy: 1.0000 - val_loss: 2.4062 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 909/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6194e-04 - accuracy: 1.0000\n",
            "Epoch 909: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.5031e-04 - accuracy: 1.0000 - val_loss: 2.3738 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 910/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.4607e-04 - accuracy: 1.0000\n",
            "Epoch 910: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 1.3927e-04 - accuracy: 1.0000 - val_loss: 2.3170 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 911/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9370e-04 - accuracy: 1.0000\n",
            "Epoch 911: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 1.7865e-04 - accuracy: 1.0000 - val_loss: 2.3107 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 912/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.6577e-04 - accuracy: 1.0000\n",
            "Epoch 912: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 8.7216e-04 - accuracy: 1.0000 - val_loss: 2.3154 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 913/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.3206e-04 - accuracy: 1.0000\n",
            "Epoch 913: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.7922e-04 - accuracy: 1.0000 - val_loss: 2.2381 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 914/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4053e-04 - accuracy: 1.0000\n",
            "Epoch 914: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.6743e-04 - accuracy: 1.0000 - val_loss: 2.3894 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 915/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.6857e-04 - accuracy: 1.0000\n",
            "Epoch 915: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 5.3064e-04 - accuracy: 1.0000 - val_loss: 2.4439 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 916/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000    \n",
            "Epoch 916: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.3319 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 917/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1024e-04 - accuracy: 1.0000\n",
            "Epoch 917: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.9552e-04 - accuracy: 1.0000 - val_loss: 2.4303 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 918/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4505e-04 - accuracy: 1.0000\n",
            "Epoch 918: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.1086e-04 - accuracy: 1.0000 - val_loss: 2.3768 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 919/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.5004e-04 - accuracy: 1.0000\n",
            "Epoch 919: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.0914e-04 - accuracy: 1.0000 - val_loss: 2.4952 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 920/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7657e-04 - accuracy: 1.0000\n",
            "Epoch 920: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 3.5004e-04 - accuracy: 1.0000 - val_loss: 2.5452 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 921/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.5972e-04 - accuracy: 1.0000\n",
            "Epoch 921: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 5.5972e-04 - accuracy: 1.0000 - val_loss: 2.4750 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 922/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4942e-04 - accuracy: 1.0000\n",
            "Epoch 922: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.1809e-04 - accuracy: 1.0000 - val_loss: 2.4166 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 923/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.6866e-04 - accuracy: 1.0000\n",
            "Epoch 923: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.2445e-04 - accuracy: 1.0000 - val_loss: 2.5233 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 924/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.2041e-04 - accuracy: 1.0000\n",
            "Epoch 924: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.8195e-04 - accuracy: 1.0000 - val_loss: 2.5094 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 925/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2036e-04 - accuracy: 1.0000\n",
            "Epoch 925: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.8993e-04 - accuracy: 1.0000 - val_loss: 2.6388 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 926/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.7539e-04 - accuracy: 1.0000\n",
            "Epoch 926: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.8451e-04 - accuracy: 1.0000 - val_loss: 2.6410 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 927/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9463e-04 - accuracy: 1.0000\n",
            "Epoch 927: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 1.7380e-04 - accuracy: 1.0000 - val_loss: 2.8366 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 928/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1910e-04 - accuracy: 1.0000\n",
            "Epoch 928: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 2.9350e-04 - accuracy: 1.0000 - val_loss: 2.9835 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 929/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.0297e-04 - accuracy: 1.0000\n",
            "Epoch 929: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 7.2511e-04 - accuracy: 1.0000 - val_loss: 3.0253 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 930/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.1946e-04 - accuracy: 1.0000\n",
            "Epoch 930: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 6.4798e-04 - accuracy: 1.0000 - val_loss: 2.9237 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 931/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.5807e-04 - accuracy: 1.0000\n",
            "Epoch 931: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 6.0241e-04 - accuracy: 1.0000 - val_loss: 2.8449 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 932/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6304e-04 - accuracy: 1.0000\n",
            "Epoch 932: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.2813e-04 - accuracy: 1.0000 - val_loss: 2.9245 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 933/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3436e-04 - accuracy: 1.0000\n",
            "Epoch 933: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.6679e-04 - accuracy: 1.0000 - val_loss: 2.8027 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 934/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2011e-04 - accuracy: 1.0000\n",
            "Epoch 934: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 3.4770e-04 - accuracy: 1.0000 - val_loss: 2.8709 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 935/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6038e-04 - accuracy: 1.0000\n",
            "Epoch 935: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.4274e-04 - accuracy: 1.0000 - val_loss: 2.9106 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 936/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.4269e-04 - accuracy: 1.0000\n",
            "Epoch 936: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.8677e-04 - accuracy: 1.0000 - val_loss: 3.0027 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 937/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0452e-04 - accuracy: 1.0000\n",
            "Epoch 937: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.0345e-04 - accuracy: 1.0000 - val_loss: 3.3283 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 938/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.8841e-04 - accuracy: 1.0000\n",
            "Epoch 938: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.5932e-04 - accuracy: 1.0000 - val_loss: 3.2403 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 939/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6506e-04 - accuracy: 1.0000\n",
            "Epoch 939: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.3964e-04 - accuracy: 1.0000 - val_loss: 3.3236 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 940/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8717e-04 - accuracy: 1.0000\n",
            "Epoch 940: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.3631e-04 - accuracy: 1.0000 - val_loss: 3.3902 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 941/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6718e-04 - accuracy: 1.0000\n",
            "Epoch 941: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.5066e-04 - accuracy: 1.0000 - val_loss: 3.1364 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 942/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 942: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.9460 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 943/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.6773e-04 - accuracy: 1.0000\n",
            "Epoch 943: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 8.6773e-04 - accuracy: 1.0000 - val_loss: 2.8333 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 944/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.6393e-04 - accuracy: 1.0000\n",
            "Epoch 944: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 6.8166e-04 - accuracy: 1.0000 - val_loss: 2.7712 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 945/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.0514e-04 - accuracy: 1.0000\n",
            "Epoch 945: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 7.3228e-04 - accuracy: 1.0000 - val_loss: 2.7406 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 946/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.7556e-04 - accuracy: 1.0000\n",
            "Epoch 946: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 5.1436e-04 - accuracy: 1.0000 - val_loss: 2.6292 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 947/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.4245e-04 - accuracy: 1.0000\n",
            "Epoch 947: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.4565e-04 - accuracy: 1.0000 - val_loss: 2.6276 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 948/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.3938e-04 - accuracy: 1.0000\n",
            "Epoch 948: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.2286e-04 - accuracy: 1.0000 - val_loss: 2.5842 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 949/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.9672e-04 - accuracy: 1.0000\n",
            "Epoch 949: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.5957e-04 - accuracy: 1.0000 - val_loss: 2.4330 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 950/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.7356e-04 - accuracy: 1.0000\n",
            "Epoch 950: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.7356e-04 - accuracy: 1.0000 - val_loss: 2.6916 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 951/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.1480e-04 - accuracy: 1.0000\n",
            "Epoch 951: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 1.1480e-04 - accuracy: 1.0000 - val_loss: 2.6542 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 952/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6073e-04 - accuracy: 1.0000\n",
            "Epoch 952: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.3400e-04 - accuracy: 1.0000 - val_loss: 2.5922 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 953/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.8449e-04 - accuracy: 1.0000\n",
            "Epoch 953: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.1359e-04 - accuracy: 1.0000 - val_loss: 2.5586 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 954/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1485e-04 - accuracy: 1.0000\n",
            "Epoch 954: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.9308e-04 - accuracy: 1.0000 - val_loss: 2.7941 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 955/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.7278e-04 - accuracy: 1.0000\n",
            "Epoch 955: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 1.7662e-04 - accuracy: 1.0000 - val_loss: 2.6066 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 956/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9576e-04 - accuracy: 1.0000\n",
            "Epoch 956: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 1.8278e-04 - accuracy: 1.0000 - val_loss: 2.6620 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 957/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5762e-04 - accuracy: 1.0000\n",
            "Epoch 957: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.2509e-04 - accuracy: 1.0000 - val_loss: 2.4020 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 958/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6334e-04 - accuracy: 1.0000\n",
            "Epoch 958: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.3778e-04 - accuracy: 1.0000 - val_loss: 2.3700 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 959/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.8751e-04 - accuracy: 1.0000\n",
            "Epoch 959: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 1.8366e-04 - accuracy: 1.0000 - val_loss: 2.3245 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 960/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.2741e-04 - accuracy: 1.0000\n",
            "Epoch 960: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.1355e-04 - accuracy: 1.0000 - val_loss: 2.2529 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 961/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.2254e-04 - accuracy: 1.0000\n",
            "Epoch 961: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 1.1477e-04 - accuracy: 1.0000 - val_loss: 2.4198 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 962/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.0720e-04 - accuracy: 1.0000\n",
            "Epoch 962: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.4636e-04 - accuracy: 1.0000 - val_loss: 2.4457 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 963/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.1839e-04 - accuracy: 1.0000\n",
            "Epoch 963: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 2.1752e-04 - accuracy: 1.0000 - val_loss: 2.5720 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 964/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1431e-04 - accuracy: 1.0000\n",
            "Epoch 964: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.2784e-04 - accuracy: 1.0000 - val_loss: 2.6248 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 965/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.4580e-04 - accuracy: 1.0000\n",
            "Epoch 965: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.9969e-04 - accuracy: 1.0000 - val_loss: 2.7614 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 966/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5923e-04 - accuracy: 1.0000\n",
            "Epoch 966: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 3.4265e-04 - accuracy: 1.0000 - val_loss: 2.7428 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 967/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.7889e-04 - accuracy: 1.0000\n",
            "Epoch 967: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 6.6370e-04 - accuracy: 1.0000 - val_loss: 2.8067 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 968/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.7919e-04 - accuracy: 1.0000\n",
            "Epoch 968: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 1.7919e-04 - accuracy: 1.0000 - val_loss: 2.4148 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 969/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.3804e-04 - accuracy: 1.0000\n",
            "Epoch 969: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 4.3804e-04 - accuracy: 1.0000 - val_loss: 2.2619 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 970/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.9564e-04 - accuracy: 1.0000\n",
            "Epoch 970: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 1.9564e-04 - accuracy: 1.0000 - val_loss: 2.2892 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 971/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.4372e-04 - accuracy: 1.0000\n",
            "Epoch 971: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 2.4372e-04 - accuracy: 1.0000 - val_loss: 2.3615 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 972/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.6577e-04 - accuracy: 1.0000\n",
            "Epoch 972: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 2.6577e-04 - accuracy: 1.0000 - val_loss: 2.4413 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 973/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.2666e-04 - accuracy: 1.0000\n",
            "Epoch 973: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 3.2666e-04 - accuracy: 1.0000 - val_loss: 2.2822 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 974/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.9418e-04 - accuracy: 1.0000\n",
            "Epoch 974: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 3.9418e-04 - accuracy: 1.0000 - val_loss: 2.4548 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 975/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.9241e-04 - accuracy: 1.0000\n",
            "Epoch 975: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 1.9241e-04 - accuracy: 1.0000 - val_loss: 2.4383 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 976/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.8764e-04 - accuracy: 1.0000\n",
            "Epoch 976: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 1.8764e-04 - accuracy: 1.0000 - val_loss: 2.5350 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 977/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.8445e-04 - accuracy: 1.0000\n",
            "Epoch 977: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 2.8445e-04 - accuracy: 1.0000 - val_loss: 2.4618 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 978/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.3860e-04 - accuracy: 1.0000\n",
            "Epoch 978: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 1.3860e-04 - accuracy: 1.0000 - val_loss: 2.2905 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 979/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.4142e-04 - accuracy: 1.0000\n",
            "Epoch 979: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 6.4142e-04 - accuracy: 1.0000 - val_loss: 2.6083 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 980/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 980: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.5436 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 981/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.7715e-04 - accuracy: 1.0000\n",
            "Epoch 981: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.6298e-04 - accuracy: 1.0000 - val_loss: 2.3932 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 982/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1416e-04 - accuracy: 1.0000\n",
            "Epoch 982: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.6798e-04 - accuracy: 1.0000 - val_loss: 2.4877 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 983/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.8566e-04 - accuracy: 1.0000\n",
            "Epoch 983: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 4.8566e-04 - accuracy: 1.0000 - val_loss: 2.4780 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 984/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.7093e-04 - accuracy: 1.0000\n",
            "Epoch 984: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 1.7438e-04 - accuracy: 1.0000 - val_loss: 2.2164 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 985/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.5845e-04 - accuracy: 1.0000\n",
            "Epoch 985: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.0244e-04 - accuracy: 1.0000 - val_loss: 2.1438 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 986/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.4479e-04 - accuracy: 1.0000\n",
            "Epoch 986: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.3577e-04 - accuracy: 1.0000 - val_loss: 2.0447 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 987/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.0007e-04 - accuracy: 1.0000\n",
            "Epoch 987: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.7754e-04 - accuracy: 1.0000 - val_loss: 2.0991 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 988/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.9224e-04 - accuracy: 1.0000\n",
            "Epoch 988: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.7278e-04 - accuracy: 1.0000 - val_loss: 2.2187 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 989/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.8678e-04 - accuracy: 1.0000\n",
            "Epoch 989: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 1.8678e-04 - accuracy: 1.0000 - val_loss: 2.1791 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 990/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.9931e-04 - accuracy: 1.0000\n",
            "Epoch 990: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.6798e-04 - accuracy: 1.0000 - val_loss: 2.0740 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 991/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6097e-04 - accuracy: 1.0000\n",
            "Epoch 991: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 2.3704e-04 - accuracy: 1.0000 - val_loss: 2.1413 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 992/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.4906e-04 - accuracy: 1.0000\n",
            "Epoch 992: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.3083e-04 - accuracy: 1.0000 - val_loss: 2.1938 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 993/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4681e-04 - accuracy: 1.0000\n",
            "Epoch 993: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.2444e-04 - accuracy: 1.0000 - val_loss: 2.2652 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 994/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.3513e-04 - accuracy: 1.0000\n",
            "Epoch 994: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.3513e-04 - accuracy: 1.0000 - val_loss: 2.3256 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 995/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.5438e-04 - accuracy: 1.0000\n",
            "Epoch 995: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 4.0860e-04 - accuracy: 1.0000 - val_loss: 2.2672 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 996/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.7319e-04 - accuracy: 1.0000\n",
            "Epoch 996: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.9695e-04 - accuracy: 1.0000 - val_loss: 2.3193 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 997/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.7630e-04 - accuracy: 1.0000\n",
            "Epoch 997: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 2.4923e-04 - accuracy: 1.0000 - val_loss: 2.3780 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 998/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.5550e-05 - accuracy: 1.0000\n",
            "Epoch 998: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 9.7030e-05 - accuracy: 1.0000 - val_loss: 2.4215 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 999/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.3206e-04 - accuracy: 1.0000\n",
            "Epoch 999: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.6039e-04 - accuracy: 1.0000 - val_loss: 2.3555 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 1000/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.5691e-04 - accuracy: 1.0000\n",
            "Epoch 1000: val_loss did not improve from 1.06157\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.3934e-04 - accuracy: 1.0000 - val_loss: 2.2753 - val_accuracy: 0.5000 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x=XF,\n",
        "                    y=yF,\n",
        "                    batch_size=4,\n",
        "                    epochs=1000,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks,\n",
        "                    shuffle = True,  \n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fXsVS3brAkTW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXsVS3brAkTW",
        "outputId": "b66a99ac-5c6b-48b7-e2df-5ae3858e4146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "model.save(\"/content/drive/MyDrive/CSC 514/\" + model_name + \"_last_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289542a4",
      "metadata": {
        "id": "289542a4"
      },
      "source": [
        "## Plot loss and accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f30a2e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5f30a2e7",
        "outputId": "0ce25c3d-38ce-4a6d-db9d-b92866726eae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "save_fig_dir = \"/content/drive/MyDrive/CSC 514/plots/\"\n",
        "if not os.path.exists(save_fig_dir): os.makedirs(save_fig_dir)\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "history_dict = history.history\n",
        "\n",
        "# Plot loss curve\n",
        "loss_values = history_dict[\"loss\"]\n",
        "val_loss_values = history_dict[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "ax[0].plot(epochs, loss_values, \"r\", label=\"Training loss\")\n",
        "ax[0].plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "ax[0].set_title(\"Loss curve\")\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].legend()\n",
        "\n",
        "# Plot accuracy curve\n",
        "acc_train = history_dict[\"accuracy\"]\n",
        "acc_val = history_dict[\"val_accuracy\"]\n",
        "ax[1].plot(epochs, acc_train, \"r\", label=\"Training accuracy\")\n",
        "ax[1].plot(epochs, acc_val, \"b\", label=\"Validation accuracy\")\n",
        "ax[1].set_title(\"Accuracy curve\")\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"accuracy\")\n",
        "ax[1].legend()\n",
        "\n",
        "fig.savefig(os.path.join(save_fig_dir, model_name + '.png'))    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989bf07f",
      "metadata": {
        "id": "989bf07f"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "068fbd17",
      "metadata": {
        "id": "068fbd17"
      },
      "outputs": [],
      "source": [
        "# Uncomment to load the model\n",
        "# model = resnet50(input_shape=XF.shape[1:], n_classes=n_classes)  \n",
        "# model.load_weights(checkpoint_loc + '//' + 'best_model.h5') \n",
        "# model.compile(loss=loss, optimizer=Adam(learning_rate = 1e-5), metrics=metrics)\n",
        "\n",
        "\n",
        "# model = tensorflow.keras.models.load_model(model_name + \"_last_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample test"
      ],
      "metadata": {
        "id": "4mTauRbWORRE"
      },
      "id": "4mTauRbWORRE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "#model = tf.keras.models.load_model(\"burn.model\")\n",
        "filepath = '/content/drive/MyDrive/CSC 514/Burns_BIP_US_database/Testing set/24.jpg'\n",
        "test_img = prepare(filepath, IMG_SIZE, DIM)\n",
        "test_img = test_img/255.0\n",
        "prediction = model.predict(test_img)\n",
        "pred_img_class = np.argmax(prediction[0]) + 1 # one added because python starts from class 0\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(np.squeeze(test_img))\n",
        "\n",
        "print(\"Class: \", pred_img_class, CATEGORIES[int(pred_img_class - 1)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "FoLXNQ4UNxGY",
        "outputId": "a6364cd7-66a5-4f34-d3cd-0ff6ee4355bc"
      },
      "id": "FoLXNQ4UNxGY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Class:  2 deep\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "score = model.evaluate(Xt, yt_onehot, verbose=1) \n",
        "\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy: ', score[1]*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sds55hUoOV5c",
        "outputId": "37a01fac-8760-4b67-a0aa-e5a4221cf97d"
      },
      "id": "sds55hUoOV5c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 282ms/step - loss: 2.5968 - accuracy: 0.4324\n",
            "Test Loss: 2.596816301345825\n",
            "Test accuracy:  43.24324429035187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0c5ab2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c0c5ab2",
        "outputId": "63906b70-3297-40e6-e9c0-7437bb9e8fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 80ms/step\n"
          ]
        }
      ],
      "source": [
        "# Prediction\n",
        "prediction = model.predict(Xt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(prediction, axis = 1)"
      ],
      "metadata": {
        "id": "vnryhtpWnfDZ"
      },
      "id": "vnryhtpWnfDZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE4i-boeVRqd",
        "outputId": "3eb90ab2-5af7-4263-b8bf-a459ba7af8b2"
      },
      "id": "uE4i-boeVRqd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 2, 1, 0, 1, 1, 0, 0, 1, 1, 2,\n",
              "       1, 2, 1, 1, 2, 1, 0, 2, 1, 1, 2, 1, 2, 0, 1, 2, 0, 1, 1, 1, 2, 0,\n",
              "       0, 1, 1, 1, 0, 1, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT_yVmk9VUfu",
        "outputId": "6de64c15-2a92-4dec-877f-e18a18119597"
      },
      "id": "vT_yVmk9VUfu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 2, 2, 1,\n",
              "       1, 2, 0, 1, 1, 1, 2, 0, 1, 0, 1, 0, 0, 2, 0, 2, 0, 1, 0, 0, 1, 1,\n",
              "       1, 2, 1, 0, 0, 2, 0, 1, 2, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pleXzSCJh92C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pleXzSCJh92C",
        "outputId": "c9f03a39-227b-4294-fea6-5c62317539a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[12 15  6]\n",
            " [ 6 18  3]\n",
            " [ 1 11  2]]\n"
          ]
        }
      ],
      "source": [
        "# Create confusion matrix\n",
        "conf_mat = confusion_matrix(yt, pred)\n",
        "print(conf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fqhGQ_5icLJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fqhGQ_5icLJ",
        "outputId": "9bd56829-8497-4ca1-d58e-5ffb9d71b4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             superficial  deep  full\n",
            "superficial           12    15     6\n",
            "deep                   6    18     3\n",
            "full                   1    11     2\n"
          ]
        }
      ],
      "source": [
        "# Pandas view of confusion matrix\n",
        "df = pd.DataFrame(conf_mat, index = CATEGORIES, columns = CATEGORIES)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nq32MT20qin2"
      },
      "id": "Nq32MT20qin2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}