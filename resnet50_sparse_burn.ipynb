{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mou58/burn_injury_classification/blob/master/resnet50_sparse_burn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0Y_AC6u0a9bs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y_AC6u0a9bs",
        "outputId": "fc2f9a58-7b6e-4dbe-b8aa-323c490d7ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07589ece",
      "metadata": {
        "id": "07589ece"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization \n",
        "from tensorflow.keras.layers import Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import random\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef0e8fe",
      "metadata": {
        "id": "4ef0e8fe"
      },
      "source": [
        "## Implement the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25481f96",
      "metadata": {
        "id": "25481f96"
      },
      "outputs": [],
      "source": [
        "def conv_block(input, filters:int, kernel_size:tuple=(1,1), strides:tuple=(1,1), \n",
        "               padding:str='valid', bn_axis:int=3, activation:str=None):\n",
        "    \n",
        "    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(input)\n",
        "    x = BatchNormalization(axis=bn_axis)(x)\n",
        "    \n",
        "    if activation is not None:\n",
        "        x = Activation(activation)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def resnet_block(input, filters:list, kernel_size:tuple=(3,3), strides:tuple=(1,1), convolutional:bool=False):\n",
        "    \n",
        "    x_shortcut = input\n",
        "    \n",
        "    x = conv_block(input, filters[0], (1,1), strides, 'valid', 3, 'relu')\n",
        "    x = conv_block(x, filters[1], kernel_size, (1,1), 'same', 3, 'relu')    \n",
        "    x = conv_block(x, filters[2], (1,1), (1,1), 'valid', 3, None) \n",
        "    \n",
        "    if convolutional: x_shortcut = conv_block(input, filters[2], (1,1), strides, 'valid', 3, None)\n",
        "        \n",
        "    x = Add()([x, x_shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def resnet50(input_shape, n_classes):\n",
        "    \n",
        "    input = Input(input_shape)\n",
        "    \n",
        "    # Stage 0: Zero padding\n",
        "    x = ZeroPadding2D((3, 3))(input)\n",
        "    \n",
        "    # Stage 1\n",
        "    x = conv_block(x, 64, (7,7), (2,2), 'valid', 3, 'relu')    \n",
        "    x = MaxPooling2D((3,3), strides=(2,2))(x)\n",
        "    \n",
        "    # Stage 2\n",
        "    x = resnet_block(x, [64,64,256], (3,3), (1,1), True)\n",
        "    x = resnet_block(x, [64,64,256], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [64,64,256], (3,3), (1,1), False)\n",
        "    \n",
        "    # Stage 3\n",
        "    x = resnet_block(x, [128,128,512], (3,3), (2,2), True)\n",
        "    x = resnet_block(x, [128,128,512], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [128,128,512], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [128,128,512], (3,3), (1,1), False)\n",
        "    \n",
        "    # Stage 4\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (2,2), True)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    x = resnet_block(x, [256,256,1024], (3,3), (1,1), False)\n",
        "    \n",
        "    # Stage 5\n",
        "    x = resnet_block(x, [512,512,2048], (3,3), (2,2), True)\n",
        "    x = resnet_block(x, [512,512,2048], (3,3), (1,1), False) \n",
        "    x = resnet_block(x, [512,512,2048], (3,3), (1,1), False) \n",
        "    \n",
        "    # Stage 6: Average pooling\n",
        "    x = AveragePooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    # Output layer\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(n_classes, activation='softmax', name='fc')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=input, outputs=x, name='ResNet50')\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e2a4c8",
      "metadata": {
        "id": "55e2a4c8"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d24e2cd",
      "metadata": {
        "id": "4d24e2cd"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "# This function creates image dataset after preprocessing images. Image can be grayscale or rgb\n",
        "def create_image_data(CATEGORIES, img_size, dimension, DataDir):\n",
        "    database = []\n",
        "    for category in CATEGORIES:\n",
        "        print(category)\n",
        "        path = os.path.join(DataDir, category)\n",
        "        class_num = CATEGORIES.index(category)\n",
        "        for img_name in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                if dimension == 1:\n",
        "                    img = cv2.imread(os.path.join(path, img_name), cv2.IMREAD_GRAYSCALE)\n",
        "                elif dimension == 3:\n",
        "                    img = cv2.imread(os.path.join(path, img_name))\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                else:\n",
        "                    print('Please select dimension either 1 or 3')\n",
        "                img = cv2.resize(img, (img_size, img_size))\n",
        "                database.append([img, class_num])\n",
        "            except Exception as e:\n",
        "                pass\n",
        "    return database"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "# This function preprocess single image\n",
        "def prepare(filepath, IMG_SIZE, dimension):\n",
        "    if dimension == 1:\n",
        "        img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "    elif dimension == 3:\n",
        "        img_array = cv2.imread(filepath)\n",
        "        img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "    else:\n",
        "        print(\"Please select dimension either 1 or 3\")\n",
        "    new_array = new_array.reshape(-1, IMG_SIZE, IMG_SIZE, dimension)\n",
        "    return new_array"
      ],
      "metadata": {
        "id": "KlMJQae7HX9L"
      },
      "id": "KlMJQae7HX9L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 200\n",
        "DIM = 3\n",
        "CATEGORIES = [\"superficial\", \"deep\", \"full\"]\n",
        "\n",
        "n_classes = len(CATEGORIES)\n",
        "\n",
        "TRAINDIR = '/content/drive/MyDrive/CSC 514/Burns_BIP_US_database/Training set'\n",
        "TESTDIR = '/content/drive/MyDrive/CSC 514/Burns_BIP_US_database/Test'"
      ],
      "metadata": {
        "id": "YFJcqSasHdRz"
      },
      "id": "YFJcqSasHdRz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668bcd26",
      "metadata": {
        "id": "668bcd26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b4026f-f33b-4491-e260-0fc928dc6aa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "superficial\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:02<00:00,  3.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deep\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:01<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of the training data (20, 200, 200, 3)\n",
            "Training labels [1 0 2 2 1 0 0 0 2 2 1 1 2 0 0 1 2 0 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Create training dataset\n",
        "training_data = create_image_data(CATEGORIES, IMG_SIZE, DIM, TRAINDIR)\n",
        "\n",
        "# Randomize the dataset\n",
        "random.shuffle(training_data)\n",
        "\n",
        "XF = [] # XF will contain training image data\n",
        "yF = []# yF will contain corresponding image class\n",
        "\n",
        "for features,label in training_data:\n",
        "    XF.append(features)\n",
        "    yF.append(label)\n",
        "\n",
        "XF = np.array(XF).reshape(-1, IMG_SIZE, IMG_SIZE, DIM)\n",
        "print(\"\\nShape of the training data\", XF.shape)\n",
        "\n",
        "# Normalize data\n",
        "# XF2 = XF/255.0\n",
        "XF = tf.keras.applications.resnet50.preprocess_input(XF)\n",
        "\n",
        "yF = np.array(yF)\n",
        "print('Training labels', yF)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize samples"
      ],
      "metadata": {
        "id": "L7G__DMII5es"
      },
      "id": "L7G__DMII5es"
    },
    {
      "cell_type": "code",
      "source": [
        "idx = int(np.random.randint(low=0, high=len(XF), size=1)) # randomly choose an integer\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(np.squeeze(XF[idx]))\n",
        "plt.title(CATEGORIES[yF[idx]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "o8T7D9bmIfF7",
        "outputId": "f61b3552-612b-4fef-dbac-b4153448d118"
      },
      "id": "o8T7D9bmIfF7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'superficial')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAEICAYAAABf40E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3xVRfqHn0mlExAUEUQQRFSaoKLoKkVR1MXVVXHZIIqoq6AuFizY17auXbGjLgr+XLHgitJU1pW1oNgA6b2FHhLSbu7398d7LrlJbkLKTT8Pn/lw75w5c+bcnHnPzDvvvK+ThI+PT90lpqob4OPjU7X4QsDHp47jCwEfnzqOLwR8fOo4vhDw8anj+ELAx6eO4wsBn6jgnOvsnPvRObfHOXedc+4F59ydJTjvE+fcpSW8xmrn3MDyt9YnnLiqboBPreEW4HNJPUpzkqSzKqg9PiXEHwn4lAvnXOhF0g5YWJVt8SkbvhCoBTjnxjnnNnhD8SXOuQHOudedc38LK3Oac2592PfVzrnbnHOLnHM7nXOvOefqhR0/xxve73LOzXPOdStw7jjn3M9AunPuM6Af8KxzLs05d0SE6w/x6kt1zq1wzp3p5X/hnLvC+3y4c+4z59x259w259xbzrmkCv3xfHwhUNNxznUGRgPHSWoMDAJWl/D0YV75w4EjgPFenT2BicBVwAHAi8A051xi2LmXAGcDSZL6A18CoyU1krS0QBuPB/4J3AwkAb8roo0OeAhoDXQB2gL3lPBefMqILwRqPrlAInCUcy5e0mpJK0p47rOS1knaATyAdWyAK4EXJX0jKVfSG0AW0Cfs3Ke9czNKcJ2RwERJsyQFJW2Q9FvBQpKWe2WyJG0FHgdOLeG9+JQRXwjUcCQtB27A3pgpzrm3nXOtS3j6urDPa7A3MNj8/kZvKrDLObcLeyu3LuLc/dEW2K9gcs4d5LV/g3MuFXgTaFGK6/iUAV8I1AIkTZZ0MtZ5BTwCpAMNwoq1inBq27DPhwIbvc/rgAckJYWlBpKmhF+2FE1ch0059seDXr1dJTUB/oxNEXwqEF8I1HC89fn+3nw9E8gAgsCPwGDnXHPnXCtstFCQa51zbZxzzYE7gP/z8l8GrnbOneCMhs65s51zjcvYzFeByzyFZYxz7hDn3JERyjUG0oDdzrlDMB2CTwXjC4GaTyLwMLAN2AwcCNwGTAJ+whRwM8nr4OFM9o6txIbrfwOQNB8YBTwL7ASWAyPK2kBJ3wKXAU8Au4G52KilIPcCx3plPgbeK+s1fUqO852K1E2cc6uBKyTNruq2+FQt/kjAx6eO4wsBH586ToUJAefcmZ712nLn3K0VdR2fsiHpMH8q4AMVpBNwzsUCS4HTgfXAd8AlkhZF/WI+Pj7loqJ2ER4PLJe0EsA59zYwBIgoBFq0aKHDDjusgpri4+MD8P3332+T1LJgfkUJgUPIb1G2HjghvIBz7krMPJVDDz2U+fPnV1BTfGorq1at4uijj456vcnJyVxyySUMHjy40LFNmzbRtGnTqF+zMnDOrYmUX2X+BCS9BLwE0Lt3b3+d0qfEvP/++1x77bUEAgEyMkqydaF0TJo0iXfffTdi3bVxSb2ihMAG8puktvHyfHzKxauvvsr999/Ppk2bKuwaGRkZFSJcqisVtTrwHdDJOdfeOZcADAWmVdC1fOoIzzzzDI8++ihr1kQc1fqUkQoRApIC2B73GcBi4B1JvtcZnzLz7LPP8sILL7BkyZIqbcfNN9/Mtm3bqrQN0abC7AQkTZd0hKTDJT1QUdfxqf289tprPP300yxaVPUrzK+88gq7d++u6mZEFd9i0Kfa8+ijj7Js2bKwnDaYc6KqYebMmWzfvr3Krh9tfCHgU61ZsGABmZmZBXKPApKrojkAXHPNNSxevLjKrh9tfJfjPtWWdevWMXDgQHbs2FHgyEwvlYf6mOsFH38k4FPtkERaWhodO3aMIADKgwMaAo2AgZTHaVFGRgY5OTlRalfV4gsBn2qDJILBIIFAgMaNG5OdnR3lKxyOOS7aAXxUrprOOOMM3nnnnWg0qsrxhYBPtWHt2rXExsaSkJAQhdpOxtwmHgn0LnAsBvgDkA00i8K1aja+TsCn4gkCDaApTckiq8hi0TPJ/Qnr/KMww9W7vEaEhv8xwNtALDAYGx38F6g9Gv/S4AsBnwolPT2dDh06QBakklpBV4klv1X6Adij/QzW4QuOLFxY3nOYg+O/YYKh7lm3+9MBnwpFEikpKaSQUgG1twG+8j4fCBzkpdC7rQmmBCyOplhQpJuAWVjQpbqFPxLIRya5uY9y7rnzCI1MXwLaPvQQ9ChVsF0fYOvWrQwbNqwUZyQCJ2Id8hzgQ8weIHwEcRF5jo/rA8cB/y53Wy0sQyvgRsz1xY/A7VGot/rjC4EwMjKyueGGiXzyyep9eTcDzeLj4ZBDGDx4MOeee26Vta/GsGAVq1/6gDvTfmDWrFklOCEJaIm5nWiNzdMnYKEOCw7lU7E5/xosONFpwJlRajhAJy9189oVibswD++1A18IeOzatYunnnqKl15anS///wA+suWkZcuW7bNfb9asGVdeeWXlNrKaIIlHH320aEXewnWsn/QJb7KyBLUdD/weC3X4MOZ86u/esX9Q2KBnPvArJjA6APeXuv0l4xDgL0UcS8X0DbUDXwgA27ZtY8qUKdxzzz3FlpszZw5z5swBoFWrVqXyMNOkSRPOOuus8jSzytm7dy8feQJx3LhxUar1NGzI/08gBxuG/xih3O+w4fpiLDbJRZgeoCoYB3zEN99Az55w1FFV1IwoUeeFwLZt25g6dSrXXXddqc7bvHkzQ4cOLXH5Qw89lLfffhvnHH369Nn/CdWMtDT47LN0hg59HPg2CjU2wjr9BkwA3FngeHPMqCfECKA7FpgoB7gvCm0oD0fzzDO7OOig9Rx1VJsqbkv5qPNCYNasWVx99dUVfp21a9dy0kknERcXV2BHnI0q6tWrV+FtKCt79+7liy9SGDIE4DWgvH796gFdsKhpb2FxUw/B1ulDm4WOw/YH1MOW/KYDE7GpQ0GBURW8CAxj585VbNt2Ky1a1NzgyXVbCGRlQSW7kQoEArRv3z5f3owZMzjxxBO9b/HEUi9fOGHKGga0PGRBZg7kKJuPP/6ISy5JxpR06V6BOCBQxspPwIb1oWXDQcB4bJgdCoUww/t/MPAkthxY3ajHY489x+5de3j5lRerujFlR1KVp169eqkqyH3gAb1pliLVKF2hgQQVIGApIaBAoHAKBnKlQFDKjdKPEZACAXn1BxW4OVfXtQoInhC0EowVrA1r53mCmCLuwXkpGr9HikDVNtXnLV03ckKU/ggVCzBfEfpfnTYWehj4c5nPboitZYdztpdfVo4CejKb2cSF/mXHERdXOP0r7kSI+6BoBXZpEBAPTeIgLq4dcXE/EPfoozy9+Ujgr9iwfSL5DWk+wExxC9IReAj4pkD+HyiZbVoccF5p76DKuINLeJKKn05WJGUWAs65ts65z51zi5xzC51z13v59zjnNjjnfvRSYefttYJ0TEkVzkTg4AhlT8PWtvfHxZTUYm0Y35HIRSS+kkhi4v7SdyQmDi3iWFMS60GiYC9gUcp7Yp14uXe1czBB8CH5lXWRWA7cgQnEM8LyPySy0AhxAqYPSAPe8T5HYyNRRXAkMM/77HDl2JJcHSiPTiAA3CjpB+dcY+B751zIMuQJSf8of/OqAgecBXyCvSKLo+Dxo4BdEcrFeMeWAJ29vLOA54FTMcOX54HhmB18C2xJ7D8F6jkD+BLIIICAAAQPheyDMAfPLbB1dmHLaaH23YutrRfcmpsIDCiQF+p4xwILsEhyj3plNwFdI9xfQXIxY5rPw/KKEwBDMIVjYtj5rSO0t7owj6pR1FQMZRYCkjZhTwWS9jjnFmMq3hqOsI5WnABIwjr1vAL524GTsI54CxaE6QOsQ8VhHXMQ8Dj2EB2C2avneJ9D6sAu2LJZSAknzILtybDvIeK9uldgGuuWEdr+OXkd6hJMCQewB3iiiHuMx97o52D29euAeyj5Tjt59xViIGb8Ew88ApyL/YZgNv4Ft/SGLPLuwJYDW5XwupVB86puQFSJyuqAc+4w8saQfYHRzrnhmHnXjZJ2RjgnXxiy6sWe/RxP98p8iD3Q4cLgN+/4W9gA+zjYN2dsgFnDhVuXdIpQfz2gXdh3AZ96ZYv6k7Ui/1QkpF1/EriQPPncFhvOgnXSouzjF2N2/K2w+7vFu7fi6IC98VdjdgBHAj9ggvFRbBQRg418mpH/dwgnhjz3YS0o2nzXJxqUWwg45xoBU4EbJKU6557HbDnl/f8YcHnB81Sjw5DlYOasZ2HD59uxofMT5L3NX8M6xVjMow3Yz92tDNdz2LC8OOqTt34fXr6ed81I1o3xRbTnb8AXQA9gDibovopQLpwO2HTmaExvsBv4DPt97sIMfUJz59HAEcXUVZL79YkW5RICzrl4TAC8Jek9AElbwo6/THS2eEWd6dOnM3fu3LCceOztu7yIM0IcjRmsHOCdcwY2RF6IDWxCQqAZNmw8NYqtLgunlKLso5iAex7YiCkz12Fv7nbe/0URwEZAR2CCZTs2NTmBwh36j6Vok09FU2Yh4JxzwKvAYkmPh+Uf7OkLwNaFfi1fEyuGDz74gJkzwz3WxmPD1/0JgR7Ag2HfY4BI22UvLF8DKxVhDjV+xjpzyKf/gZiSrgOwdj91rMW2Wx2CCYEDyNvy61OdKc9IoC+28+MX51xox8ftwCXOuR7Yk7WaGuOlIYjtDjuJ/HP8YzGlW8GoM9nY7RU3rK1JDAP2cNxxDVm0CNJDOknmYSOAgtF/Ctr2gwmCddQpErAZVw2mzHYCkv4ryUnqJqmHl6ZLSpbU1cv/fdiooNqwadMmUlMLurrKxOb1r2ND+vbYz/MW0Msrk4S9HTMwhdetwBZqNkHMNqAh4HjySTjssPDjR2Oa/YL0pvDjcwCRNefCBGkNU/2UhIOw1cwaTJ3cOzB8+HBmz54d4cgebB3/bOB9TBsfg3WQppiLkdsxJdnZ2BTiFkxw1FSDkSzMym8YEEvfvgWP/xOzGizIZxRe+78dU/qFCI2uhAnSzdT412ZBghRvAlEDqHtCIJf9vJCEGQoFgVVeXsGo6n2JbBRUU4nDOnukgWEHTLkXmiLFev8PwZZIwzcRvYGNBEZgv+NabETlsB++pgrKYtiAzQprMHVPCHTFlsCLJBbTkNfCBzYi9TH9RlH3Oxjr6Odh4971YccuwQRBaPvvz+TpgVcAY6LdWJ8KoM5tIOqOrXzn5yRsWJyFGfjUFQEApt+oR9FjWodZ92VhrzwXlt7EFKYhW4MnsW1ZH2BWjwu98woGFK0tXAr3PgQvVHU7ykedEgKdO49m4dLDEHMLHAn5oc/ELOpqoQKrWPZnox/y3R9fID/Oy3eYZeThXt5Z2Pz/J+94qExtYjDwNsQGa/x4uoY3vxR068aOFWvIzU0j/1vvbCwABZgCsKCAqK0sxsw4ErzP5emkfbDRQsgwKpG8zUC1kdN5661rOPbYx2u0R6EQdUYIDFqyhN252di6/9WYwgvMNn4P5lngTfLs6ms7WdiuxnqU/55vxNbKJmI6gpOxnYvF8CDwP8obF7RKmDTpXgYPPoqkpNqxp6HWTweCwSCXXXYZswMBb0/bJsySOQ5b9luPmbf2w9vKUAf4Ebg7ivV1wnYCHo8J0+JMpScC22yFcWkUm1BJPPccDBlyUq0RAFDLhUBWFtx3n3j99a8JBkPz/E3Ykt8SzCioMfYWuxgzdqkLrKfwsmc06ANcBvQvpkxzriKOjrOxP0ElceihMOZ6SCyHYc/48XD55dC49rgSAGqzEEiD7Dfg3nsd5oMrmcLz1KOwlQGwra8XVGIDK5uFFDbprQf8qZLbcR5tScrvSLWCiUmEJm3h8GPtc1m5+26oxk6hy0yt1QlkboevrgKTc6GYAjuxBcK9VdWsKuRnbOWjbVheErYHrAJoBByDyZ4C7hnGAzH1wQVAOYVPjTYx9WDVTrj9fshYtf/y4cTFQcgRtKttCxwetVIIZGTAt6ttoSo/07Dc5UTeX1+buSTscwPMVLgCNdvtgWcg5iEI/mxXW7GCfYFeEw+GwG7IKamjonIQ2G2pLDRpAv8p6OWtllErhcCPP8JppxV19JNKbEl1JAtT4C3bX8Gy44BF4C6ERqtgj+DbXdCqFWR7JgkZJQlTWMXExEApIs3VWGqlTsCRZ+HuU5CXgZKHTysTzTEdYU/7KkHz5nkCoLrjgDgHXbrAyhogrMpLrRQCfTDLdZ8q4neYr9apVd2QsnFpS8i+Bn75papbUjnUSiHwHfteQj4RmY75AyjAcZhXsWRMf1q6GK15/Ac4BQ65wPYR1rgR9WBwT9VeRWBBaqUQCGDrAD6RGAVMJmIcwY8wk4nnMbupdpjzoI8xmXGm9z2Uilou2wF8DZsWQNd0SK0h5hfPAztuhWefo07NJ2ulYtCnOBKxWAInFj7UF/gvZkmdhbkYaIapEI7BOkZ4eIDuwFOY+4Bwb+QCjoLcF2BDd1ANcL0waRL8/nhocgDliyRXA4mGy/HV2EpwLhCQ1Ns51xzzOnkYtv/0okixB3yqikZeCuMozGN6M8/DQFvI/Rbr3PWB8NAQwhwILcW20W6NcInVwI0QrAHKmUmT4JxzoEntsQQuFdGaDvTzfAyGJpq3AnMkdcJml7dG6To+FUUK5hgo3VyqBOtjmy0zbQBQn7DRv4B/YXOuaUR2srQH+LqC2xwFJmAjgFq0FaDUVNR0YAgWhRPs0foCCz5fY7jrLogtZl64fTs8/XTltafC2Ybtos6GwPuYE+WbgaPM6+IZWL8fkQMPvEbe239/8UmrMXfdBSNioX4N0VlUFNEQAgJmOucEvOhFFjoozMvwZkzdlI/qEIasa1fo1SvysTvvNJPRotiyBVK3QXCyTZ1rBQnYuO1lzGfoSGARJLxmqoAg0DYHC0hUg3EOLr10/4K+ruCk8nnRcc4dImmDc+5ALLrmGGCapKSwMjslFYw4uY/evXtr/vz55WpHOIsWwZgSuLe75BK44opyXGgPBM6zEKNg5qWBCEr3GkUnbJ/RbZii8H3yfK5Uc2IxxVRR1AdOjIPYfjBjRt1ZAgzhnPs+bMq+j3KPBCRt8P5Pcc69j9mkbglFInLOHYzNOCuNo46COYUdCUafxhA3J89n4QknwIIFkFMJm2IqjJA1cTTdDVQS9YG0Yo6dEg8zepAX69QHKKdi0DnX0DnXOPQZmzr+iqmLLvWKXYq5m6n1fPMNdO8OzeNsZF1XaNIEmsdXjEOxJEr+kBYlAAB6x8GMbsC35W9TbaO8qwMHAf91zv2E/bwfS/oUczl7unNuGRa+5uFyXqfG8N13sP1U+JOrnZZYcQU+xwP/mQvbB8FVsbbpJposjYNj4qMwdD8RiN6Ms1ZRrumApJWYyUjB/O3AgPLUXaOZDa8Nh7aTLDZ7baE1ZhoQsjDYTV4MZjrDU5dC29Vw881RvOhG+LElnHE6RAwa5VNuauPLqnrwGtwzHl6p6nZEgXOBbwDiwR0HSc52BNbPwYwKugN/B86P/rUPtstSnv4fTyHTKJ8wfCFQUcRCzB2Q/FzNV4h8AoxtAIsHQv3PYNV2aNkSMnOwsaTDnqQYW5WZNClKF+4FuUdC/d/KZ8r/B8zGwScyvhCoSOpBQjKcvhxm1mCNdAD4zkHP7dCxOxzbC3bvzvMSFE5iIjRoYDYYy5fDzz+X4YIxwHJo/KHJl/eCEXc6lJjpmMmDT2T8DUQVTWOo3xj6Hgxffln+6i68EDZvLn89pSU7A1YuBNLz8oaMgcwVMPFl6NTJ8qZMgfvug0aN4PDDIS1MZd+wC0z5BzRrYnqDr4syKw4Cl5onSKXCuD97zj1uAXbBwJVw/vlwzTUla3sa8MlPMHQovP12KW+8DuALgUqiQQM4+eTy1/P885DudcS334Z//7v0dfToYcLkjrugQXvLe/FuGDkysvefY4+FsWML59/wAmz/BsY9Bvf8Fbp1hpS9sHIXNEmFK+6A7HU2PUhOhrjGZktx4AHF2Oo7zCHhXyB3BBAw2wuAkYfDnm3wfSp80rV095yaCh9/DMOGmZXgG2/UPWOhovCFQA3jvPPyPh98MPTpU7iMZGbPRZGWZm/Wxo3grjus3w0bBlddVVgIxDaCrHrw2y5ocS1cH1b/uMshazC89DY81BCuvwx0AigZtk+BaVPh+uGw0vM2fOsV0LABPPOMefDdsQO+TcSWHf4v7KKZUO9PkHl5/rZs+hD27oVVKeBmmGB6/PGS/W6HAVckQEZ7eOghaNvWRiy+2bAvBGo0/ftbKogEa9ZEnrMvWgT/+5/N15s0gSTPzPnVVyObPB/TFdp0gBcnwlXXQk4A3phuU5K77oIPvwBehrcnQ98eEOgBOQIEielw001w0z8g4UC4YThMfRceeQQuuADqh7Ymho8KBKwH9yqFbICnTwe6Q4cTYWRDuHCECZThw+34a69BsIjgym2B0XHwxoFW5sEHLSBJcUKgaVO48FzMB8vlRZer8Uiq8tSrVy/5VDBLJW2UPnhJ6tFAatRIGjhQ6t9fMnEROV13nfTBB1LHjtLMmdK0f0t0kWbMsO9tTpBiEq3stddKV16Zd26rllbm0xlSwy7Sxx9LSUnFXM9JNCm+PQyVhv5st7R8udS0qRQM2nXi4oo+ryvSa8XVGyG1aSPNfFeaGWv1z5wpZWdX6V+xXADzFaH/lXsDUTSI9gYiH4/VsCzDezM/CQedDC4B3r4dnk6A996DzMz8Oylj6gEOgllAEK67zkYb4dOQkhCD2etnkD8GdLE0wCJ+v1t8sbPPhX88BevWwvnnwLx50K1b6dpXVr7+Oi8M2SFA05ZYKMtSsHMnbNpU9PFOnSC+YBT4KFBhG4h8qiECtsG2ZOj3LWzw5vn3HGJLeLd5UXiO7k4hV4MNjoCYeNi7AgK7LJBLZqYp8naVwk1YENPuN6JQAKKiaYHFK/3Aa1dc4fYBzPgKvrwQgtmQlgXd+lM4jspuoAI2coXrYJ4HLrgN6t1W8viEaWkwcaJNk4rihx+gTRubriVWRoT3SMODyk7+dCC6BINSppMaFjfcbSjRs/jhcGyspXPOkf73v9INpUudnERHKSFdSjjZy+srUdQQ30k0ljizwMOULZEpcVoFtzcsXXaZlJlpKSsr8t8kO9uO33hjyeudMsXOycmJznNBEdOBKhcAki8Eok0wKDlXzAOWJNF1/w/hE09ITz5ZSZ2pj9TOa38g4OVlSrQoonxfKSYoNVHYgxSUGCRRr5LaHCG1aGG/f8HUp0/Z67z22vx1KVi256IoIeBbDNZFdgHVLLBGvoAxMUAAmiTY8mVEvoLgEZAantcAqITYhuEktoH67fO+b9tmHqkKpiINo0rAhAl59bSJA6IcGt0XArWUXbvMQKk83HorjKskz5DfAd0w/cEBDnbHwnpnEc2KZAXQBtMbNAEyYdV70KltcSdFl6wNkLE6f14wWDiVBymvnk1BaJJu+oImTWBvEziyicXfLCu+EKiFOGcPiFsEtIb334cz/wIcWLp6srIsARx9tNkXRI3XgGHe5yTI7WzmvQ2BH7GXXWOKGQkACIuY1JF92sfGDaLv06BY5KVKvNweYM8eS132wPI9kFucX7X94AuBWsycdjBvGgwYAA2TgBuACOa/JWHtarjl2ig27hEsVtw1QBocs87cGWYCV3lFfve7EqxICFiT9/XMM81Qqq6wluL9KpYEf4mwFnMCgGcDkDgULmsKZMNrcdj+/1KQlg7/+6Ho44m3QO5GCHyKuS/fH79hTgLOBK6FRt+YVd+fMc/nYDYKP/5ob7yS4publB5/JFAHeAno0Q2ubQejO8GIU0pfh4i4ZG+0guAG0HqgNOHH/wf8G1hlX/dio4FsLFrNeX/01snHYp6Pw8ODJWKh0e4pxfVCxEC9hyn/0z8U6FfOOqoBZR4JOOc6k3/bRwfgLswSfBR54SlulzS9zC30KTUSvPgBjDwX4uOsc50BpM+Dn36C+kWsDIwcmWep9umnsHp1CS8YCznvYeaBpWE3+1wGbTkC3nzX8nQ5/P0FuO9KGDEC0sbCv8fBes83QceOMPBsLHRaMhbZAnjpAwgOBt4jclSkEDE2cskcTynMGSNQn4rxrlrJlFkISFoC9ABwzsUCGzBBfhnwhKR/RKWFPmXimvHQKgbqJcLRjWBzFrw0wUyFXZztDswt4J73qafMUWpmpgmL1attk80BXWBBKvbmjsSG8rd31ToYfy/QHvMpdg3MvBwGnAWshG9+g/V7gA7QeyQ87wW2y8mBOUPs88tfYmbHMyleCAgCMyik0OvaFfYGYcVKSibQXiv5/VVnoqUTGACskLTG+Zu0qwXHxMKd42HJEohvB9lbIbDbjsXUh8RWsHdZ/nMWLYKLL4aUsCgR550HQ56CKxfBioshJgjBRRXQ4AwspNlb7HNdexvg7oFlu20pjkOB67D9zB7pe+Gs88nrtH8swbVyIf2swtl/ugnWNoDnH6NGxFGMFtESAkOBKWHfRzvnhmNOnm9UhIjE1SEMWa3FwZyfIbgTenWFDcvzH87dA3sjKNuOP37f6fteknv3wtFb4fmDYNBsaJgBe9oXODEOU1GXZ6nMAXvA3QItD7RoNZ9shUO+gGNOhU2NIWE8pJ9pUdO3eKelxgBdgGKUloUoYk/CvYDOxEY2NUgIHIA5Uy0zkcwIS5OwOBvbsPiDYLEIYjG1ywPAxP3V4ZsNR5egpCRJ9JNIKL2ZavN4KTEmLC9J4hiZvX6kc47xypTH5LaRxBCpUZYUCEpkWNvXpUhHSvpYUoake3Nk+wPkmQlvLvk1nJMSGkgcZ98TEy250L2+IXFndMyHKzOtR9I8SYHinwsq0Gz4LOAHSVs8obJFUq6kIBba8vgoXMOnlOwux1t55v/grN+HZezCNPg9izjhV4qfg5eULdjTFMSUbtm2bPib4CJBU8HdfycvttVKoFXJqz/8cNi8HjNPxJYeMzOh4wVE3RS3MhGgk0DTynZ+NKYDlxA2FQjFIJTGYekAACAASURBVPS+/gF7RHwqE2HjwzJakfQ7DjIFtPMy1mAORksz5C4tacAmSPupwF76s4E4SF+MrSbcCrxZtkusxJawCvEjZoY3gkq1/osWocn0e0Ap3T4AUYhFCJzuXT/E351zvzjnfsZWUf9anmv4lJFyLH1JoI7Ygv3astfjMJP+ErMWaG3X38f3WOSTXeR10KmYo4JSOhIJroBdbSIc6Ab8F9NsFaQZprCsxigslYXyhiFLx/QS4XnJ5anTp3JJTLQVhPbt8zpfOqDbsRDlLwPrC5/XuCfs+ZFin7yQnXuJEWbUEE5BYfYcNspJp/QUqL8HNgDZuBhbVWiOTXm2kyf8dhMh0F7twrcYrOM4Z7YA4Qhsqe4D8oLKdwY+yiuzdyklevVEfXSdStS2Cy/pC2tTIZCDGRxlARfhrVl5BDFhWIvx9w7UcbKz4Y9/LDAEBxuCB8gzA96C+dPyyC3LmziM22+Hnj3hww/hzZLM8Q/BXtu7y3fdcHK/wzYrhDRYm9mvf8PaiC8E6jjBoFkRFsKzJjwZGAI2jP4p73B5Aw+ffLLZIJR4x18G5jOwP2ZR+FY5GxAibHRDOqaDqGP40wGfYonFQgM0AA6Og+sPtO/RYPbsvNBsjv0YvOzABFFT4CQqJAIyYJuUSqXNrPn4QqC2kYvtzCsjAwaYQ5IQc4ExwJhsuGc7ZA+170XRATi7FZx9stmSF8U33+QfBThKOCzdgAmCEqw5de8ORxxRkkrDqI9JvAgMHmwuvqqaGGzlFGwn9tnY792qFDYT+YhkQVTZybcYjCIZUrCB5JBiymJ9tl469tjIx1q2lObNy58Xg9QZqXt7qXt36bHuksZK+lVae1TJrpmUJHXqVIKyCRJ3SeyQeK3w8W7dzDvykUdacJV77pGuvjp6lnk5ORbspEsXqWHDirMA3F9KQPoB+xunIf3KGi2Y/LN27txZ7KOB7224bhDMkjYeaw9IY+9BcUhJsVKDcFPgItIPP0hdS+CJGKSDkDq2lBa1kjSzQEOypLXzS1bPuedKH31kn11xZQ+VeFhiQuTja9ZInTtLy5ZJF1wg1a9v+fXqSa1amSfg8nS+deukxo3N/fpxx1WdEAhPK5G6tRyrVq2O0PTp04t9NnwhUEfIyCj8oDRE+uAo6cY20X0A0+tJ2lhEQ76T1iaWrj6H1KScbdq2zX6DvXulyy+30GRDh9r3X36p+k4b7XQg0srl0t69mQoEit88UJQQ8HUCtYjQo1GQdCDnXtBFUb7gdoq03Vcv0NLSVScKuBAvIz16mKflvn1h/HgL4d6ggfkLqG2sAE7uCA0anMRHH3203/KRqAZqDp9okZJi4cojceGFFXDBRpiiLsI158+HE04o/vQJEyA11VybRyIjw0KIbyuJz8IIjBxZtvNqCoEANG1iPhW+++47evUqmy8PfyRQy4g0Eog23tZ/6gvCTQLHjoWGDS31+x002k9bbrgB7ryzmOuU0T/NggVw6qmRjzVtWrqYisXxage4uXV06iqKBx+0+IX/+lfhY85BylYT/M7FUGaHPpHmCJWdfJ1A+Vm82JRfRc0d678oNb5SqheluWhrL21uJam1NLx1YY15sUq+klyjtRRTAmVmeGrVys5LKMKPgnN2PBq/QbNYqVEx7VuzxlYpSlvvN99IPXrY5yZNrL3Nmxcul9taUrq0cWPRMRDDoQidgD8dqCXk5MDmzUUfz3oYstPL51cznI3e/6dvNiOf5RTe01PeQcnGjfsvU5DifgOw7lOWeiOxM2yr9gknwHPP2eecHDjxRDjkkMKjmQRsJJVVTL2tRkLCSvucmmopXx0O5h0DbiIMGgDbs2HSm9ClS9nuwxcCdYTgqoqpt5qFNKwyVq6E++6zz6GwY3/4A0zMgGuBGx+Bzp1t/j1zBnz1Ndx9t01NRozIX9cVv0JxOtWA4L71wP1wzU1AXNG6oJLgC4E6RuzJEHskZL9S1S0pG3FNwcVDTpiy0Dl4/HG46aayh+NKuBZyv4HcMgYvadbM3sTPPgr3tDGjzdNOg9/3txWPsy7O66jt2kP/02HIENs/kZICt9ySV9es/VwrCEzbCUyDyZNNB/PYY3DGGWVbAfGFQF2kBjmEHjUKpk6FHTugf3/o0ReWrYKPCuw8LLeT6z9ivbWMQuDQQ+GGyyH7nxAXa9GTbrjB4iJeXqBst26WwJYux4zJLwRKw4QJkJAATz4JHTqUTQj4qwO1gK074MsSuv7K/S9kv1yx7Ykmp5ySF1354ovhL1fAkUeSzyegZB2utKOAc8/Nc2WW/RXklsNvwNat8M0X0LMBjNtgMRwqIzDqLbfYva8/nIhLtSXBFwI1nK1b4d1pcPMDVd2SimH4cFgf5tlo2ucw4T8Qc3LZ6ouNNT8GYCOMRo28A+OBL0pYydHkD4kG/PRTKkOvWsiTK6Bnw4hnRSQnAAsKKACOOQaOPRaSkkpeD6OAI0tRPowSCQHn3ETnXIpz7tewvObOuVnOuWXe/828fOece9o5t9w597Nz7tiyNc2nJLz3HlxzWeFAIrWZxidA29tLf14scERj+PbbyJr7EvMSFgJtH+kkJs6m14GjmRYHU0uxc3FHKpw0CkiA1q2tTa++CtOmwUknlaJNfwY+L0X5MEo6Engd27UYzq3AHEmdgDnedzCn0Z28dCX5/NH4+JSd7GxLg3LglczSn9/ewaIGNhpYsgyycspoXNWXfW7LjSmccsrzvPT557QJwDFLIp+WhblE2JsD6elmEbmPHvDbEpv6nHACtGkD0yspgmeJFIOS/uOcO6xA9hDgNO/zG9hgapyX/0/POOFr51xSATfkJSJkEhGiMuZXNZXQG61MD3QNYkyYI4M3Hi79+a6j7WfICUCj/l5mqTyhFls7wWCQFi1i2LIlconhwDRsfT/zL3D0MTBrJvCtHW9SRbEPytO1Dgrr2JuxyENg3uDCVSzrvbxSMWaMSezYWOjTpxytrOVceaUpxH78sapbUr055RTzqrwLSBQWZuxryhybIT8jmT37Jrp27cq2bXmKzIK8jTlkSb4SZubC6vfg4HP3U/WhwGHRaGPRRGWJUJKcc6V6DxUVi7BPH/j5Zxv2Gd8yf/6HNGjwAA1jYGvvsErew9xE11HGj4fdu+GZZ6KwRFbL+eorW08XlN+UMYy774YDD3RMnTqARx/9nl69IEumqF+P6SFCDL8X3j4CtBdeHwPZQfIcuRZFJXg6Lo8Q2BIa5jvnDibPOfUGLHpUiDZECF4t6SVMxULv3r0FtgV04cLTCQRuwWKaAGxEmktGBmQCreeFVXIU+cYy11xjHaOukJNjwvLzz+GiaG8Trg70JM+m4X5gMmV2MBoMFpiDR4nHH7epakZGLIMG5XX5gjOCc4HZSyDwJBCE3JK2pRKmeOWZDkwjLyrcpcCHYfnDvVWCPsDukugD+vSBhQshEHgWM0a90avqNGAiYL/HppywtAU2bcpLzzxjgmT48HLcVQ1j2lQYc0XZt9tWaybBZ1OgwxRsLP1JGeq4EvhbdJsVzvDhcP31tuz44oteZgA0HAiaT4ONG+Ex4JRDgHiIPx3qP1dxbSotJRoJOOemYL2xhXNuPXA38DDwjnNuJBatLvQumg4MxvaU7AUu21/9y5dbcEizue6MxZk6AljtfT7N+148KSmW1qyBc86xvJgY821fW4fLW7bD1igF46h23AwPdYaUW7HYAGUJB/Y5hdb0o8ns2RbFaeNGePppLzMIfARDfg/f3gjZSTAJ+G07kAqB7yBYjQKalHR14JIiDhVyKOutClxbmkY0aQJ/+xvcccftpKamAr2xMIb/wmTJVaWpjl274OOP7bNz+bXKAKNHe1ZntQARJd1WdeQTmDUf82D0WxnOHwbsxF5LFcSSsOXAuXPDDnjPYL0TwQ2AuZNhww9AFmgt5JYjxmMkbrnFvCuXhWqxd+DAA61jbt+eyHPP1WPrVmHitDfQAnM0XzakvC2eIXJy4Npr8+y3axqTJ9ub59tvq7ollcBW7DVaFgJeqkpehJezYe0MCFagQVd2dtk3TzlVg8Xl3r17a/5827lx440wefJUNm92QC/y4mNHl6uvhtNP33+5omjYEAYNil57SsOJJ8LXX1fNtX2qL++9Z9uXi8I5972k3gXzq8VIIJzHHoNg8AImT/6AlJRfqCgh8MILlspKmzamazi2ko2if/rJ3E35lI5GjSzy8i++A4RCVEs7vCeegAsvPA84p6qbUiTr10O/frBuXV4KRsttTzHXPOcc+PXX/Zf1yU/PnrZhqCo4+GB7adSvXzXX3x/VUgjUFFJTbR95KG3YYG/p3DQseGYUSUszpxXhO+p8Sk4waA48ok1Cgq0OFMesWbB4MZx1VvSvHw2q3XSgJhMyfPwMOPV1ILn48s6VbOlSMi+5FT3SqM189ZXZkESbO++Egw4y8+2iOOaYyPnO2d829H9V4Y8EKoD+QOyIvL0PRaXdu6u2nT7l5847ixcARTFokOl3DjywYkYopaHajgSeeMLmUbfdVtUtqThKE0XWHwXULmbNgt69bWmvVM5DKoBqKwTi46tHGOiKJKs4v9M+tZpgMG+TXFU/B/50wMenjlNthcCECZ/y6quzq7oZPj61nmo54J4wASZMOJTffovdf2EfH59yUS1HAtOnw8KFR2E7Cgsi4H1q8baZAuQCjxC9AGI+PvmpdkJg5kzzDVA8KzFh8C3mXmgTFql9dYW2rWoIAiUMKuBTZk5rAu0SqroVpef88+Gi+lCeBYZqNR34cb7tJlxW7G4rhzkcAQvY9DnmzGkLkEiFO2SrdOKB/6vqRtR6LmwOiw+A+fEWy2HrCmgP/FTVDdsPo0dD33Q48b9lr6PajATWrIELBq9jxbLV2Bs9PBRrDhDJdc4dwGzgROA8zNu5j0/RxMRAu3aWwrk/E44ZBVOmwFVXWXyRp+OgbduI1VQpzuW1v39/+OEeyDqsHBVGilde2alnz56qV2+v4HBBI0Gs4JmwWOzLBWOiElPeT3U7NW0qpaZKqZLiEqVGjSzNmCGNGCElJOSV7dRJWr266ttcMNWrJ+3eLTmXP/+991QswPxI/a9ajAQWLFhFZmYyFtFhD3lxToTNidsDTxdxto9Pydm9G5o0h3aCQG9YtQF27YEHHrRdobeNs+FxDLBihTjsMFNAV4uO4pGZaXtJpLx4HCXdhxKJanJvh2OuxAqqN6ZhXoVOqPQW+dRe4jCPZXHfwOEBOACb+ydgzjMDQKAfLFy4BDiQeC+vOi5Yp6Za+LLvvrNQ52Vif0N1zNVvCvBrWN6jmNe3n7H1uiQv/zAgA/jRSy+UZDoAvQRNBQmCHwXZgoAgV+bFPavKh2B+qvnpcKTMJCkz04bHcXHS5u1SpqTMbCkQkO69815dMfwKKVsKBoPKzMzSnj3ZSiBBkFPqa27dKrVrV/Y21/NSUcdD05f58ZI+qLjpwOsUjkM4CzhGUjdgKRC+zWeFpB5eurokgqh7d6hXrz+m+RemEY/FBioJXvLxKR+uPSQuy9v/vxFoCSSeABe1HsLUqe8Qx43ExzzOl19/yUknnURiYgIJCXFks55NxNIYm7Qe79V5G/B4Mdc88khzOFMafugKKV9CyjlwORZvozACDmTp0r2ktIJun1C4l5aUkr2pOYywkUCBY38A3tpfueJSr169tGjROrVosUiQ4Um5fwner/K3R9FpteCEatAOP5U0dexY4NW4UNJxkmZLq3qv0tWtr1bLFs+pWVOpXbvZSkhI0IknnqisrGxBF+UQUFOkjkj1GSb4SC2RDi50rSxBFy/tEUhvvSUtXChdfnnxbWz0nbR2sKRfJa2VbhxRVNmgwCktLU1aIulsu4+yjAQKZZRBCHwE/DmsXDqwAJgLnFJMnVcC84H5hx56qCTpi4OlDkgwWdBfcHeEm88VnK6qnyKkCz6r8gfbTyVPHTtKeyRdICkY6hnNZJ3oO+mXmb9o2LC7BAPVo8dIvfrqq4qLi9OAAYMEMzWAoD5CmvmGdPTRfQVv7qv72GOll19eLbhAcXG5mjlzpmJiZio0hZg3zy53yy2h9nwrGCWQ4uOlmTMtfZYjZSyQdK907/mFpxLNmkkffSSZEJipfv0CGjhwqAY2XqpzukvTplayEMAW6t8nz2txInCA97kXFkmtyf7q79Wrl7VyktS9jQSXCxoLuno/1PWCCYJnBVfIxkKXC3ZV+YPlp+qZxo6VTjopf16TjtLwdClxlDRqlOkA9gkBj++++06XXnqp+vfvr8WLFwsaCF6WdbqrtPPJdGm31LdvX0E/wb8F0iGHLNSYMWP04ov/1CuvmD4hJuZKwV5BuBB4V6ecMkl/ve4TJdJMjRuP1Suv5F1/9Ghp1Kh/alTn63VY4ijZ8z9K8LRAatVK2rs3/309wps6lK2KQ3rqidILgTJbDDrnRmCeQAd4F0BSFhaGHUnfO+dWYKGD5peo0j/DyJ3w9NPxLF9+MBbs+GWgHtABWy4MRaGYiAVCvgE4sKy34VNLGTzYXLOfempe3s4D4IUsiHnZolnwHPBXeOGdFzgr+SzatWtH7969GTNmDAsWLPDOEjFsZxzAuBZMSH+Sv+T+heTkZHbtWsMBBzSgYcMf+eSTp5k9+2uefvppAoEADz30d6SXGTv2UV591TyMvv8+1K/fkLFjYznmqA5k7b2Ad6e8yMjtBwG3AHDAAfDEE2/whz904pLzW1gTvoflM5vyLpCTBo8+mgP8A7gVcAxjGP8HHHcB9DquDD9WWUYCmApiEdCyQLmWQKz3uQMWiLR5iUcCHmef/b7gT4K2gjaCMwQUkV4TbK/gN8vPgs1V/nbzU8nTbG9+/IukySulyZOlxydLvCLVO0B6C2nyJFsp+Gunv2rh+IWaN3mefvnll33P4YYNG3T++ecr3sUrODSoKW9OUXJysjZv3ixJ+tOfvtLf/varPvvsMw0dOlSjRo3S1KlTlZmZqdDzuXv3bo0ZIz37rHTCCdLDD1vdGzdu1AsvvKCrhg6VkpPzPf+jR4/WsmXL9n1f8swS3cRkOaarPlJMTJaGDr1EENSFSK/yvq4etF3fflv0KKC4kUChjAgCYAq2QycHi7Y8EosNto4CS4HABcBCL+8H4NySCJnCQmCtTAgg+J1sOFaw8zcXOMEAwfcV/FC9Iphf5Q+2n0qWjkWa/5SkL6V7v1wj7vxSJsj3Ki7hfxo0Surb18puny6pjaRjpOSWybr33nslSTt37tT8+fO1bds2xcfHKxgM6pRTTtGePXv2Paf33HOPXn/9dW3atEm//PKLVi5cqbNbnJ1PCMyYMUNffvmlLrlkt/r2ld580879+pOvlXxMqPPnWmPzNBX5ePfdd9WlSxdBd8XHS6eeKn35pVkMzkI667ghevvtt7Vt27aKEQKVkcKFwLp169Sv3x2C42UdvagRwECZXcFJMiVL1T98fqr61C5BWt5WUj1JLaWnOjylDh06qMPBQ9SBOeqRlLTvWevUSdrZQVKcpM+lG5Nv1FP3PqXU1FS99+67GjhwoHbu3KkjOnWSli+XgpE76dtvv63k5GRpqaReUlZWll0zLH3//ff5T/pC0pmhL5mSOsiEQWQ++miO4Gy1aiHtXSiZjmKFDiNXm76Szj3jXM2YMaPI8yWpxgiB7t27C46TLRHer6KFQHiaXuUPn5+qR9p2qqSF0p7f7VHWxKy8HvDf/0pNmyrYtq127dqVl79rl1IP36mcz3Okq6SMezL00nPP6Zz+/fPKZGfbpoOcnGI7WYhgMKidO3cqWITQKC2ZmZl6++2dglQdiLQJCYJqQpLSm6RLTSUt2H89NUII5OTkeEKgJB0/lGIEn3gPQa6XgjKLw6p/KP1UGSkoW4rL0bZtQQU6B9SPAXrllVeVk5OTL23dulWJiYn7vgdbtNCRsbGaM2eOcnNzdffddys2JlYD+w8sW48NBpWZnq7Y2FhlZmZGRRA888wzsqnvsYoNu9dschTcnZM3QgmouMGEaoQQOPjgg1U6AYDgJ+8hkOA9wT8EvwmurAYPp58qJ+Uo9Dxs27ZNnTt3FswWjFDB5yUpKUnZ2dn7vm/fvl3BYFCnnnqq3njjDQWDQQUnBhXsX8bOm5IiJSR4S4Qx2r17d9nqCcOEwNnqTlAb2ZjvftJAWr/eCp4nqQxLhNVkA1FJaAPsKJC3FuiKmRsDDMGWDI/A1n98aisnEVos3k1sbGOysrLIysqiefPmAHz6KWRlvcxdd2Xxpz/ZsUWLsoAtxMXF7SvfrFkznHPMmjWLYcOG4ZzDDXe4T0u5JW/2bOjena1AI8A5R0ZGBo0bNy7R6ZKoV68eaQWizY4bN44lS5aQlfUBr3z7M8cfdPy+tmdlZdEgK8t2EAF/5I88XZbdtpEkQ2Wnko0E2shMMJ1gq2CbbOh/h2w0UNVvo5KkHTJlZ3Pl34xylkwHUtXtqzkpDikJCXIVG7tNwaAZ0jRvLn377Q5lZ2dr9OjRql//ISUkSM2bf6WkpOZKch2kA7VPEX/44Ydr+fLl+96WDz30kEaPHl3Kd7WkrCz99vXXatasmRK8kUDLli2VmpoqSRo0aJCaN2+up556KuLpwWBQzrl8qw+SlJaWprS0NH366adq2rSpWrVqFfH8fv36acqUKdq7d2+RTaQmTAeWLVumI488UtbpxwtuUZ4QiBN0kgmBYNgDcYJsd/gU2VJiZ9nKwl37eZDGCx6vxAd3s+AmwS/e/YQLgdUq3voxKLhOefsq6nY6+2yzxQdzCLJwYUCdO3fWL79kKylJWrxYuvTSS9WkybOCe3TuuTdr8eJ0LV68WEsWL1HOrznq3LmzcnJytHTpUmVl5SkQU1JStHHjxiI7UmGe0DvvdNZll12mzMxMzZs3b58QCJ8O9OnTR4AeDhkKFCAYDGrx4sXKzY0wqX9T2jNsjxYvXqx58+apW7duhYqsWrVKqcNTpdeLbmmNEAKS1L37NzKT4FMFHVV4RBASAoNkRkJfyzr9BMHDyhMY7QUPFPMwjRKcLPiLzGvRBsG55Xg4Txekhn1/SPCGTGk5VrBUJsSyVFgIhKf53nnheUHZOrev7ATpoIPMVh+k2FipXz/TCWSddprmfrRLaWlS//79dfPNEzV79kr9+uuv+TpDSCeQnZ29L++GG27Y7xJbJF599XYdeSQaONAUiSkpKfuEwJw5c3TmmWeqf//+atLkDcGpevjhh/XJtGm6e9w4SdKOHTs0aNCgQvVec801mjt3rp5//nn1P+Ip3X625W/cuFEx9eqpfzCo/gMGqH///urfv7+WLFmiO8+7U+8+8W6RbS1KCFQrR6MA9957PHffPZuffvoC8yocIh7z/BZy/fgFkI05HGkOTPK+Hwf8BdgFPAuEey3tDiwBRnvff8Zmlk28Ov5TjpYPI/+W55Ow+MQvAVsxA8qbgVH7qedgIruv+A+2J2u41966xj8wOzTYsuUEtmyxXeq5uXuZ+/loJjKRq78YxdPvBmjY0M7o0gUGDGi/r4YtW7Zw36338UzwSSYCsVdcYUEuGjZkwYIF9OzZM/KlAwEYNQpeecUiyQIPPvgg/fr1Y8UKaNnyFM466yzuuOMObrjhBgIBuPxygLeYM2cODzzwAGvWnElq6qH8619L+HfCW6xbu4s1W9YTH38ff7rwT+gyMZKRTHh+AuPHj+edd95h8eLF9OvXj4OPO51/zoWNl0FGRlOCOS/y2eXAnD9j70a46abm/PTTqfz11FIEuPSodkJgyBBISzuZJUsa8PXX05k1a5Z3pDFwPbbD+l+YP/7HMcXgZqA1ppJpDVyGCYGdBWp/A7N2PgATMCFnpunAO0CncrT80gLff4c5QU3z2tzYu/Y/91NPay8VpAXwMLZzu64JgQnA85xzzkr27oXPPvseSCGJJMYkXknMLYcyghFc9cBVPIo9BYVYA6mPpzL59ck8zZOsA9S27T7/XMOHD6dbt27wxRfmg2zIELZt28aECROIAca3bZvPf9f06dP54Ycf2Lp1K0cccQRdu3bl+uuv54YbbicYvIPXXwdoC9zKhRdewXvvNWPFigP5/vskbA9MI15/PZ6GDTvSptVw7ntdvEZbWh/yCK+88gq7d/+Rzz9vTXx8f+Li+rB+PV6dDYDh5uWDEfva89FHAAOYMcN8GJxZGt8CkYYHlZ0Kmg2HmD59upKTkzV48GDlKQaReSJy3uejBE1k04HXBbcWM5Rs4Z1zjeBE7zOClrLdic97Q+/PZUrH8HP/K/hAsCgsL9MbuodSpmx68rlgk2Cq4CWv7AZBsmCYd80cmR4jNIX4j/av4GwsWFXlw/HKT10E6MUX+2jatGQlJydr0KBBakkHTWma97yMGDHCFGsfSP279NfEiRMlSStWrNAbt7+hv/N3NaCBXnevKzk5WYFAQFOmTMm/jDd1qvTSS5Js70BycrJGjBix7/CUKVOUmpqq+++/X8nt26sr6NRTT9UTTzyhq6++Ws8++4ZgUr72P/bYVB1+eMq+752QBhR5r1d4z8nyMv9e111XuulAVDpxRQmBEIsXL9bxx58j+FR5HfdkwUEyz8TI5t1XyXQFK70fJCD4NSw188r+Q3BOWF09ZXN2KW/FISD4SuY3QILRXt3PylygZQm+89oRas9umbLxDsEPsg47V/k7d7bgAJkQaOr9sRcI/iB4zCuzwztvnvc96H1voLomBPogNdgnBF7UmjVrtGjRIn355ZeCDmrYMEdz587dZ5Tz1VfS3Lbfqic99wmBGTNm6JRTTtmX+vXrt698s2bN9OaSJdrpPWdz587Vb7/9tu/Zy86W/vMf+zx37lw1bNhQy5cv14IFC7Rl9Gjd3ratWrVqpbFjx2rVqlWeAjBBIJ2CdMpJUmLiRd7zJ8EKXdlqiT49RoqtkN9sua67bpkiUaOFgCRtXLtRHeigvI77q+ACmYvycMXhQTJljmFqAgAAH1hJREFU3xaZNv2asFSvQNlQShJcIlgm2CgTCEHZUt5iwXqZkvFlmSv0U7y882WdGu//Fd6562TC40lBa8FgWaffLBMu13j/h4TAxbK96S945/+fTCl6nPKEwBGy0c83qnqHKuVNawRpgi1KStqqjh2lNm3Cjwe932GpUtoH1CX+TEFH/e1vT2vs2LE655xz9MGUD9SOkxUTs0CdO3fe16l795Y6djxDHTt21Lvvvqvt27dr09Kl0rp1EZ+rZs2aiZlL9PlSKfm8ZDVv3lzjxo1Tenq6Vq9erZ07pc6dpaVLg4qJsRfO8uXLdfHFF+vDDz/U7bffrpEjR+6rLyUlRSEh8BtS7gKp3SESrFWrVqnq2PERjR8zXl+9mqbuCWvUsX2ODvfuNf+qV1l+06WCS3VdEUOBGi8EtE5KS0pTUlKSInfkgp16hGz6sCMsHbCf8+KVN1zfIhs5LJZ5OeouGz30kE0l5ss6cYryhEBopHGiYI73x5kreFAmNJIL/OEOVd6b/VFZx48RnFmgXNBrfyOvLYurqPOWN+1Q06a5ionpKpiu+vxVd99yt7KyzO9/XrnAvr9JyrIUdem8W40aZSshYZQSExPVoEED9aWvlrNcTZvmzQd27NhRaIntkUce0ZVJSdLgwZKk3Nxc7dixY19KSmon+F6fN8rWVUlX6ZF7HpHSpNmzZ6tz585SrpS1xf4GSd7zE7IrSEtL09ixYzUy+VIpda8CgYCWLVumkBAAaQ1SW3apUaN+mjRpkjJ2ZEjPSWr4iQJdumjn0qXKTErynuuCU9BIKVcmQAvmHytoKEjSVVeN044dFpugdgkBj9zcXMXFxe2nMyOor/xD/pKkJrLRQ3jeL4LTiigfG/Z5r/KEAMq/zJcrswUYobx9DSGpH/r8J5kAOFxwn5eX7aWg92DFeel/VdyZy5oStWnTJvXo0UPOfaJnCSj7pmxNnpwtGymZXXxcXGDf3zglNkVd6apZn8zSqJGjdNttt2nKlCn6XdzvtDzWhEAwGFR2drbi4+O1bfM25WTnKDs7O+Ka+44dOxQXF+fVHydbZj5Sn3/+hQKBgLLvyFb28IBmzJitI444Qtmbc5TutT+DbDWhiZYsWaLs7GwNHTpUEKPLaK/sAXfpp59+8v72Dbz7Df3t2unLL+fpj38cqgd5WNlXZCv73//WApxatmyp7OxsZWdny7lQ+UgdP/ScpAgu8r6Hlz9OpoPKEyQ9e+bf81RrhIBkhhWJiYlFdMzKSh1lb+eijocLgdkyI6bQ5o8/KG/N/+r/b+/M46Sozr3/PTMDyjaMgKiADEtQcUVAZBFBRQVvLmgEXAGJ1+VFr8HEFy9xQaMh5r1CiPGiiYpOUES8alDUaFwuIspHQIw44igO+0W2YZMd+vf+8VRN18x0T/esPUt9P5/z6a7qU9XnVNV56pznPM9zZMrGawUPe3kiglzvPP5bJRJIqW7MFRMCkUhEgwf7dfevV2/BajVt2lSRiN3jhg0bavPmzYp0jUhIN2FCwH8GVq40IXDoUMB3gK06mZMFFOoEYj0/kUhEGRn+tYzoww+lUaNGeee5wbtnyIaE/vUPCn0/Payi8S5aBe5TmkxPlC3T70QUNYBrLYuFUfx8u2NcN/MdMN0R3nmv8L77vYKI4HLBH4oc27ZtHRUCkhl8tG7dOsZFrM4U66EICoHZMkMmP5//9nHe5w5FJfxUwRzv5v1DcLpMP9FQRXsCq4o9IO96D2KqG3hyQiA9faMyMrppnntbBx8/rMmTJ2v48OFasOCQfCHg4wsBdZXEpTqM02GvJ5CRkaH09PQiQmAPexT5IRK/J7BOKugd7AkcFEhffy0dOSKNGuW/Xf01L/weipV/376Dysw8qDwOqm9hL+2wTAhcJFMAt5LNEmVo//79atbsGH333Xc699xgz88/7yFFe3vBXt+tshmrDNnws5v3zKTLdF67FJ0d+1E2rBwkEzTFhxRblZl5Xd0UApJFfykoKNCgQYNKaYypSn29m1eaoNgum9XoJJtW8hV+B2VvEF/fkCV7E2TIhizBICoHZUORXJlFYqobemmpwHtId6oJB5XVSDr66H1q0GCmmjbNEmSWFALNN+vctHP1wauvSwUFmvLww2rcuLHMXPzzIkLgwKYDMV1pZ8+eraysy5SV+b3at2jvKe/865qlzMyVysr6FzVs+GKp5c/Kss9MuuldPtVwbpLNND0la4SHvXtqOpysrBaCtcrMPF3p6f49m+T975WBc+8vLIulIbKhZQ/Zi2SnbDoZwfHe+bfJLFyvl70YdskUzFnF0ilq3PjHuisEfNavX6+8vDxL017US/QqpeFVVzpKpmgsLc92mdRv4d3cLl66wXs4NgfyrlN0JsSfOgym/So5t3ynqj7+YnnTjbLQ8n/RkCE/10svvSSgpBBYtFldO3fVu+++K0natm2bpkyZIugnOFg4O5CXl6dIJKJzzjlH33//vSTp7rvv1rRp0zRz5kz17NlfeXkHtHLlSkUikcLnJTMzU5AnWCtrbMmUPV8LXtyra//1WpmyubVMDzDEu96nyp/h+PzzI2rbNltwoqZNe1N5eZv1m9/kyRTF/vmK66E+8+7lGkWHB28oKgTknX+DrOfRSaZ0fsuryz2Bc7VR48aqmBAg9jJkD2BBRP0Yg5cFfpuIxSDMAy6tDiFQhE07tOPDZfrwww9LpuaXKpvGcRpkKlI/2RRicUXnIO9Gb/fy4N3Yht73sxVd82CubCwYXMXZT9fIBEmqG3wwPSIzpPqn4Fe6+upfKzc3V5988oliCoG+fdW1ceNCISBJs2bN0umn99OsWRsKr8+AAQM0YMBBZWS00IoVKyTZvH9+fr5mzpyp/v37a926dV4+S4cOHbIpQvI0DenDidLo0c/I/D5Kr0e3brcVGY726NFDzz03R9YD9IdwF6hfvwGe/uosde36vgYMkLp0mSv4v965CmRTzv69f1slHcVelQ0PmwiGKSoEBsr0A36v5k3vt2kC1KVLF3344SeFdg4VEQLnA91jCIG7YuQ9FTPuPwpbSvh7vOjD1SYESuOlj/W3nFk67bTTytlo46cZM2YoJydHOTk5AU/I8iZfCGyVRVxG1tCfUdQ6cbrgjzJNcWvZ2gx7ZIqhUbJu90cxHijJDJnuKWcjrlh67LGFatfOf7t9qtNOW6JRoxbp9ttv15QpU0oIgRGg6ZMna8OGDXrmmWc0Z84c5efn6+2339auXbuUk5OjZ5991u4BB5TDbN069FbljsqV5kuvvfaaJvTrp7/3769vv/1W9sa2a3jddaPVsGFD/Z48bZwoaaV0333LZELgdzrhBGn69Hh16Sfrxf2rYIBOOOFdDR0aVOb6isEnZW/rx2XTzpK9racKJspsR5ys6/+cfD2FDQO+8+7hcO+8DWSzR6O8VPS5uYNXlYM0hq/V++wcvfHGG0Ue/3hCIKHvgKSPnHMdEuXzGAbMlq0/sMo5txJbtu3TJI+vWkb2Yxj92J12hPxp0/ho6VLeD/x8lEvj1+1OgX+7qsynHj16NOmec4kk1qxZwwcffMD8+fPLWdjNwP/DgjoDXIV1ro5424uAU4DmgWPSsDUY/GUe+nuff8ECRYM5XAmYBzxczrKVn9Wr+3LwoL/Vm9zcj1mR+x7n9T6bu+66iEmTJhXJ/zLwwLBhtGnThkWLFtGqVStGjBhBx47mGDR69GgOH44wduxaviedNK7i5de3sodGdNj5Gh/t/CMFCxdyVPv2zHvsMczJazQAL7ywFriHtbRk+gYg51Xmz2+BRdSfy8GD95Of3xiL7x9kGhbQpjXmn3IyGzd24vXXp2C2/b/28j2A2ff/DlgKbAIuAc7D7tU/gCbAfZjvyiiiC4W3xNbbaEbUV6QlcAUZTOO+dvcBD9rpG9ivI+nKKcCpdGXtSV356U+TuycVcSC63Tk3GnvifiVpO9AWezp91nv7SuCcuxlbioz27dtXoBhl5/rrr4fDh5k3fz4nBvY3Skvn/s5nw8Rx5V/sHRgzZgwAZ5xxRuHDCnDgwAFefPHFJM7wv9iiK1MC+/ZgDkT7geMx56eLsOXc52IP42vASCwC0/8AAzCPxE2Y49Ia4ELgMkxeH8Dk88By1LJ8TJ0K8DegJ31ox0EWsqPDCs7tNY6XX47vXPXOO+/QKi+P7q1akZ+fX0S4RiIAJ/Lbwj23MRPg9Q+wDmlHlq8FHv8RuDZw1nsBLwbVX8EWLE/Hrsc2tm37GY8+Ogpz+MkB2mPX9E+YcPYFtMPuwUZMCNzj7b8PgOHDRzB//la2bFmHeZamYV6mP/HyTQR+g3kFBZ+7dZhX7LX4TkfQloyMDO6/5X7LcjeFQsCnp5eSJlb3oHii5OIjxxFdNvi3wAxv/+N46xJ6288AwxOdv9qGAylm586dGjhwYAWHCi28rmMrwXiZNWIHmZ4gSza//Z5snLk30H2dJLM2PEk20yCZRvk31T4k6NXrRrVosUi/QLqJRzRy5Ejl5uaqb9++atSokd577z2999573jTeecrNXaW7Bt2leVwoTXxK77//vi688EL16RN0AkOVa079uWCM992PAdFPpqjrpJL3pbtsuNZKNhTz78MhvfCC1LXrYm/7KpWMf1lcMRhMtwTO9Wc1bNhQgwcPTvywxYAqXJC08DdMpE0M/PYO0CfR+euLEJDMvsECYZZXCJwnM1dGNpNwmUwZVTzfyTIdwUqZmfEqr9H7hifJNITtXkqUb5OKCpzS09y5Uv/+/vYjGjx4sFavXq3169erS5cugfKfLNimefOkFX2lFWzU5ombC6/lypUrlZaWFriele1TsV8WtHZV4Lp+JvNojXd/jpEpPf3txd71D0bRvjlw/m9VuhAomuKFF0uGShUCwAmB73diegCIRv3wFYP51CTFYA3Bjz/nnEv65sdPHRRbCGyRKad8xWIP2Sq6v5cJgUMyxeM2mXCIZYn4vIq7xcZO98qs2crT0P4oaKqePXtr69bDWr16jaJWd8Xzjtf48b/W1q1btXXrVi1durSYxWBlC4FvZNOy11fCfcqUKQwbyTxSdwoWeHXdoKgAaVjqeVIiBIi9DNlMYDkWmuf1YkLhHmxWIA8Ykuj8qodCwKdly0QOTaUl56WOKhobIZgKFBUCweSboDqZBvvyOI0uVvJNnyvTfHmOrIez2msIpeV9RKYlt5SZ2VJ79+5V1QiB9YKx3nkrKrA/kc3kPCKz8vSnBYPnPSK4XhlkqgFpakC6GtCgSGrXrl25n7cK9QSqOtVXISBJnTrFGl8mk3rJ7Me7lePYf5HFXBhS7AFMpmHsk7lQR+3uK7fhlTVtD9ShokIgnm+G7+9REUEQFAIRmT1Apmw4V/QerEESEyQ+KdpSzqrYsxZPCNS48GIhyfIZpklWOY59C/g7pnE+GthNUa10aRyFTWf5tAfeALqVoxyVQXOskwqxYzMmptnXsPdWOPLRWKAzvlbfKMAmuH7E6n4kxhmSZRZ2nd+DwrmM1kTLfzQ7dhSQ2aQpNq1Y7J6Uf8KqVGrR4iN1k2XLlnHuueeW8+jyCAD/uCPe534sfuExwN4kjnVYY/OfyFwszmOqcNhMdwblbSW7e8HchXAV07FgsC8AQ71fs7AYluVtKoOBRsCDwOnAY8BqbCpxLTZBmQGkU1CwhczMJrgMICMNMly0apalSgiFQIrJzMwkIyPVHbKdXiqPUMkk/tNZgK0IFQHGEX3j1TB+hJuOwJs0xnpGQ4EnscZ/M9ADK38kiZM1AAYFtv+ECdn5WOTrhzBjIof1Yhp7+RzNmzfHVcA+pbyEQqAGMH36dC6++OJUF6MYEyna7Y/FRcDWGPtzgX/HIjDfgFkuziFxI9qOvTGrkt2YwU9RgbcR6/DD08AQzDBrN/AqpueeU+SY6dOns2DBAkaPvh9b9s7nMPB5YPtJzLDrFCysfQFmKBQ1BGvQABYsqJB9WoVI9SsoBDjzzDOZNGkSYxo3hrlzAfPOurvMZ8rA1lZYWo5SzMTWaziCmRN/g9mElcZNmNlrkKXYxNEZmO/ZMdhb1C9faRwCvk2+yOVCmKVkcV7GZrjXAgsxH7hVRMPWR9fBnDp1KldeeSWtW7emWbN2XHJJb7766iMeeeQR7/zBNTNnY9fzFEyo7MUsBU8DIDMT/vxnOO+8yqthWQmFQA2hX79+9Dt0CM45B7DlSsreeU4HOoO+LaLbepInWV/oOxCPFdhscEdgn7fvTezt3QN7exbn6hj7mmFvw0VY1/h0bGGWZGgCjEgyb7IsxxpfX287DVP03YuZ6qZjis2nMSHYGlv34X8x/4qSDB06lNatWwNw1lkdOOusDuTnn0zTpk3Nuvt3xY+4HtMtYK4D50d/adwYro51GauRUAjUJAYOtAQcS9QCvcwIcxMI9Hiffv9pVq9eXcpBk73PozA7+NWYl/g/sYc4lhCIxUmYjdgHmFPpBK8g7xcW6oorfsbixZ+xfv164EzMxwxMCFye5P8ky36KKjwbYL4Bd2AOPulYd32V99kLuNUr607gv5P6l06dOnHPPfeYvNlcSsbhmK6wJhFr3rC6U322E6guHnzwQWVnZ5djfvtERRd33ScLffZFKfYBm2TBLR5QdMHXiLd9iWCQ/vnP7zV48GDv/CNV9atKF8jMfRd4236Y+LcVdd0dIHMxXikz5nlX5gtQ8poEVzGuTRDHTiBUDNYT7r//foYPH16OI6/AlHX7MI/DIdhY/2uiir4fiA4hPsW64JMwi3IwTfgkzJXkH+Tnix9/bMHxx3eldevleGtqVQB55VmH6TRWYv3yDd7+t4FfYcOSb7zyNPPqkkd04LUB84EbivXb/w9FlX51k1AI1COaNGlCq1atyMwsz1qGX2NuyA2BPwAXYAq23dg02DJsQdijsHnxeOzmiiuG8vHH1/PLX37NLbcswWYitngp1mwDWKM+GOe3I9hw5V6sL34dpqG/F1PATcAa+lqgDyYEBnllnYy5EINN3b1JtOH7hlB1nFjdg+pO4XCgennzzTfVoEGi+Id+uk3m7fapt93Y2/bTDTJHmwcFryvx8uljZXH53vCO/63Mf6GBzCy3qbe/+HEPeUORRF3/nysav2+sov4RDb3/yJJ5V47w0nbv/4IhvkpPdW04kHIBoFAIpIRly5Yl/dAnn84SPKuoHiBoi+9/jpW55gb9FkZ6guRSbztNZXNoipf3T7LFTK/ztr/1zp8ROG6ULHZf8vX0g5X6S5/VFuIJgXA4UE85CxJOGpadG/FDd1nAqQxsaiyCaeX3Y1NxlwO3AbcAdwE/w2YS3vGOjWBThclMku7CrPwU47fPsanP4hzxzt8QeB4baiTPSSedREZGRpJRomo+oRCop7gzzuCEnTvZtGlTZZ4Vs4X37QIiRB1uLsPComVhisMRmEHRQ4G8QeYW2zeMqALxZUxxB2a2vMX775YUNdTxBcNLXr6zA/vnYvYMsYTHdZiHfEnWrVvH9u3b2b59OyNGVLZNQ2oI7QTqK+nppGVm0qppU/Lz8+ncubOND8tFH0wjD6Ys3Ffsd4f1APz5+oewWYfzsbd4LA4DJwe2N2GWfFuAdsBH3vEfEQ3EuZSigVf/E5uVeA34ZYzzgyk7F1HUlmCut70K2EFUeEDTpk3LqVituYQ9gXpOWloaHTp0YOHChRx9dLwGmYhGRJ2IrgUuBX7ube/DouseiwWo6oCZMw7E7Or7UNR1tzHwsff9r5jBzhpsKLENM+O9GPgj5k7dJ5DaBsrx71jj7uD9dzx+V+z3fwN+gRk3XYOZRtdtQiEQgnOOPn368Pzzz3PMMceU4wxfYW/MGVjjOdbbB9al/wQLmX6Vl68DFjlX2Nz8d4FzHcQaOJjbbXH35tcwV98rsQjAt2CNfRE2dXmd9z9zsF7Df2OWj7OxUOFgj/1s7/MdiuoETsTCsrfxzrmEIox9HDbtSHRBahXhcCCkkCuvvJJNmzYxefJkNmzYUIYjfTvZZZjNQKwo8y97n8di5sFjsQa+DzMdPg+YjnXT/byvBI7PxpSA32CN8xfYXP9DgTyfYIZBvj3By94x27EewvZAXt+JqANFY3a/i9lE/Bi7qt06wlENYv9WS0koBJxzM4CfApslne7te4nogC0L2CGpm7dIyQrMDAtgkaRbK7vQIVXHuHHj2LFjB0899VQCX4N4LPZSPP7L+2yGjeG3YkY5xb0Ri5NByY7rDqK9BjB/h2Dkn7cD378OfI9gvYYIJsCCxywstRQzmm2i0YvR9RF69epFjx49Si96TSfWvGEwEWMZsmK/TwHu9753iJevtBTaCdQ8Lr/88iqwIwimrrJltstz7NUyf4AZCfJlltkGoKxpzJgxWr58eapvV1JQFcuQOQuDMhJTsYaElIFY8/fJshfzUvx5gnxtsW5/fgX+q3RycnLIyMjg6aefrrL/qGoqqhjsD2ySFNTsdHTOLXPOzXfO9Y93oHPuZufcEufcki1btlSwGCGVTfPmzWnUqDQfgFSyHHgWM/YpjRVUpQCoK1RUCFyDRaLw2Qi0l3Q2NjE7yzkXc1JV0l8k9ZTU89hjS5vCCUkFzz33HDfffHOqixGHc7BovecnyhiSBOWeHXDOZWD2noVaEdlqxAe870udc99jLllLYp4kJKRczCFx/MOQZKlIT2AQ8I2kQhN059yxzrl073snoAthfyykWtlMUUvDkEQkFALOuRexSBEnO+fWO+du9H66mqJDAbD+2ZfOuS8wK41bJRUQElKptAX6xfmtA1UfrLQYb2HLFdRSkpkduCbO/hti7HuFohYeISFVwA/EDz6SzAIqlcvsTbPZl7uPF3ih2v+7MgjNhkPicuedd3LnnXcmzljtHCF22PDUsCeyh3kL53F1qsMGl5NQCITEJTs7m3HjxjFhwoRUF6XGs2vXLt566y3Gjh2b6qKUmVAIhJTKT37yE/r27Zs4Ywi7d+9m3rzYaxXUZEIhEJKQjphZaEhi9u3bxxNPPJHqYpSJUAiEJOTM445jQirXyapF7Nmzh/Hjx6e6GGUiFAIhiendG6ZNS5wvpFYSCoGQkEpGEhs3bqxAuLbqJRQCISGVzKFDh2jTpg2RSKKl2GsGoRAICakq/OUPajihEAgJqSr6N4S9u1NdioSEQiAkpIrI/EzsOpI4X6oJA42GhFSACRMmcMcdd5TYL4ns7OwUlKjshEIgJKSMzJ07l7ZtLaJymzZtOOGEE0rkkcTixYtp0iRRANXUEwqBkKTo3LkzzzzzDDfeeGPizHWM9PR0Xnkl6hx70UUXJWzczjm6d+9e1UWrFEIhEJIUWVlZjBw5kl27dhXumzBhAocOJbNoaO2jS5cujBs3DrBVmoYNG5biElUdoRAISZqmTZsWMYldv349h155glnr97L1cCkH1hIuvvhiunbtCpgQuP3221NcouohFAIh5ebRRx+FrG245QWs2v8dy5evYdWq6g/qkQwtW7akX7940YiM8ePHc8EFF1RTiWoOoRAIqRj3Puut8DeD3//+b8yZU/ryZV9++SWHD1dPtyE7O5uWLVsC0L17d5566qlq+d/ahqsJ9s09e/bUkiVhQOL6QLdu3SjLOhM//PBDmcxvjz/+eNLSzPxl6tSpXHXVVWUuY13FObdUUs/i+8OeQEi18sUXX5Qpf3Z2Nlu3xosnWJKvvvqq8O0fkhyhEAip0axZsybVRajz1IjhgHNuC7ZIfPIiv/bQirpZL6i7daur9cqWVGK5rxohBACcc0tijVdqO3W1XlB361ZX6xWP0IEoJKSeEwqBkJB6Tk0SAn9JdQGqiLpaL6i7daur9YpJjdEJhISEpIaa1BMICQlJAaEQCAmp56RcCDjnBjvn8pxzK51z/5Hq8lQU59xq59xy59wXzrkl3r4Wzrl/OOe+8z6PSXU5E+Gcm+Gc2+yc+yqwL2Y9nPGYdw+/dM7VaEf6OHV7wDm3wbtvXzjnLgv8NtGrW55z7tLUlLrqSKkQcM6lA/8FDAFOBa5xzp2ayjJVEhdI6haYa/4P4H1JXYD3ve2aznPA4GL74tVjCNDFSzcDNX0drucoWTeAP3j3rZuktwC85/Fq4DTvmOnec1tnSHVPoBewUlK+pIPAbKAuRm8YBuR433OAy1NYlqSQ9BFQUGx3vHoMA/4qYxGQ5ZwrGXOrhhCnbvEYBsyWdEDSKmAl9tzWGVItBNoC6wLb6719tRkB7zrnljrnbvb2HSdpo/f9B+C41BStwsSrR125j7d7w5kZgSFbXalbXFItBOoi50nqjnWRb3POnR/8UTYnW+vnZetKPQI8AXQGugEbgSmpLU71kWohsAE4MbDdzttXa5G0wfvcDLyGdR03+d1j73Nz6kpYIeLVo9bfR0mbJB2RFAGeItrlr/V1S0SqhcBioItzrqNzriGmgHk9xWUqN865Js65Zv534BLgK6xOY7xsY4C5qSlhhYlXj9eB0d4sQW9gZ2DYUCsopsO4ArtvYHW72jl3lHOuI6b8/Ky6y1eVpDSegKTDzrnbgXeAdGCGpNxUlqmCHAe85pwDu7azJP3dObcYmOOcuxFYA4xMYRmTwjn3IjAQaOWcWw9MAh4hdj3eAi7DlGZ7gbHVXuAyEKduA51z3bAhzmrgFgBJuc65OcDXwGHgNkm1YF2h5AnNhkNC6jmpHg6EhISkmFAIhITUc0IhEBJSzwmFQEhIPScUAiEh9ZxQCISE1HNCIRASUs/5/1/XuPDYDD8RAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86823353",
      "metadata": {
        "id": "86823353"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1379a537",
      "metadata": {
        "id": "1379a537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18540ca9-e4ab-45c1-b181-c9d4b85311a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet50_2022-12-07_20-22-31\n"
          ]
        }
      ],
      "source": [
        "model_name = 'resnet50' + '_' + datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "print(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a2d4a9",
      "metadata": {
        "id": "39a2d4a9"
      },
      "outputs": [],
      "source": [
        "model = resnet50(input_shape=XF.shape[1:], n_classes=n_classes)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c944dbd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c944dbd6",
        "outputId": "efacf8e7-3fce-4917-ec08-d8dd0b4eca4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ResNet50\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 200, 200, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " zero_padding2d (ZeroPadding2D)  (None, 206, 206, 3)  0          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 100, 100, 64  9472        ['zero_padding2d[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 100, 100, 64  256        ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 100, 100, 64  0           ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 49, 49, 64)   0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 49, 49, 64)   4160        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 49, 49, 64)   36928       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 49, 49, 256)  16640       ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 49, 49, 256)  16640       ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 49, 49, 256)  1024       ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 49, 49, 256)  1024       ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 49, 49, 256)  0           ['batch_normalization_3[0][0]',  \n",
            "                                                                  'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 49, 49, 256)  0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 49, 49, 64)   16448       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 49, 49, 64)   36928       ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 49, 49, 256)  16640       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 49, 49, 256)  1024       ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 49, 49, 256)  0           ['batch_normalization_7[0][0]',  \n",
            "                                                                  'activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 49, 49, 256)  0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 49, 49, 64)   16448       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 49, 49, 64)   36928       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 49, 49, 64)  256         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 49, 49, 64)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 49, 49, 256)  16640       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 49, 49, 256)  1024       ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 49, 49, 256)  0           ['batch_normalization_10[0][0]', \n",
            "                                                                  'activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 49, 49, 256)  0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 25, 25, 128)  32896       ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 25, 25, 128)  512        ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 25, 25, 128)  147584      ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 25, 25, 128)  512        ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 25, 25, 512)  66048       ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 25, 25, 512)  131584      ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 25, 25, 512)  0           ['batch_normalization_13[0][0]', \n",
            "                                                                  'batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 25, 25, 512)  0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 25, 25, 128)  65664       ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 25, 25, 128)  512        ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 25, 25, 128)  147584      ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 25, 25, 128)  512        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 25, 25, 512)  66048       ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 25, 25, 512)  0           ['batch_normalization_17[0][0]', \n",
            "                                                                  'activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 25, 25, 512)  0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 25, 25, 128)  65664       ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 25, 25, 128)  512        ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 25, 25, 128)  147584      ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 25, 25, 128)  512        ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 25, 25, 512)  66048       ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 25, 25, 512)  0           ['batch_normalization_20[0][0]', \n",
            "                                                                  'activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 25, 25, 512)  0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 25, 25, 128)  65664       ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 25, 25, 128)  512        ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 25, 25, 128)  147584      ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 25, 25, 128)  512        ['conv2d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 25, 25, 128)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 25, 25, 512)  66048       ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 25, 25, 512)  2048       ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 25, 25, 512)  0           ['batch_normalization_23[0][0]', \n",
            "                                                                  'activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 25, 25, 512)  0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 13, 13, 256)  131328      ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_25[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_23[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 13, 13, 1024  525312      ['activation_21[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_26[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_27[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 13, 13, 1024  0           ['batch_normalization_26[0][0]', \n",
            "                                )                                 'batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 13, 13, 1024  0           ['add_7[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_26[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_30[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 13, 13, 1024  0           ['batch_normalization_30[0][0]', \n",
            "                                )                                 'activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 13, 13, 1024  0           ['add_8[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_29[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_33[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 13, 13, 1024  0           ['batch_normalization_33[0][0]', \n",
            "                                )                                 'activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 13, 13, 1024  0           ['add_9[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_32[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_36[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 13, 13, 1024  0           ['batch_normalization_36[0][0]', \n",
            "                                )                                 'activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 13, 13, 1024  0           ['add_10[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_37[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_35[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_39[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 13, 13, 1024  0           ['batch_normalization_39[0][0]', \n",
            "                                )                                 'activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 13, 13, 1024  0           ['add_11[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 13, 13, 256)  262400      ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 13, 13, 256)  590080      ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 13, 13, 256)  1024       ['conv2d_41[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 13, 13, 256)  0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 13, 13, 1024  263168      ['activation_38[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 13, 13, 1024  4096       ['conv2d_42[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 13, 13, 1024  0           ['batch_normalization_42[0][0]', \n",
            "                                )                                 'activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 13, 13, 1024  0           ['add_12[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 7, 7, 512)    524800      ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 7, 7, 512)    2359808     ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 7, 7, 2048)   1050624     ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 7, 7, 2048)   2099200     ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 7, 7, 2048)  8192        ['conv2d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 7, 7, 2048)  8192        ['conv2d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_45[0][0]', \n",
            "                                                                  'batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 7, 7, 2048)   0           ['add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 7, 7, 512)    1049088     ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 7, 7, 512)    2359808     ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 7, 7, 2048)   1050624     ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 7, 7, 2048)  8192        ['conv2d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_49[0][0]', \n",
            "                                                                  'activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 7, 7, 2048)   0           ['add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 7, 7, 512)    1049088     ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 7, 7, 512)    2359808     ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 7, 7, 512)   2048        ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 7, 7, 512)    0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 7, 7, 2048)   1050624     ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 7, 7, 2048)  8192        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_52[0][0]', \n",
            "                                                                  'activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 7, 7, 2048)   0           ['add_15[0][0]']                 \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 4, 4, 2048)  0           ['activation_48[0][0]']          \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 32768)        0           ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " fc (Dense)                     (None, 3)            98307       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,686,019\n",
            "Trainable params: 23,632,899\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbefa37",
      "metadata": {
        "id": "dfbefa37"
      },
      "source": [
        "## Compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8dadd1",
      "metadata": {
        "id": "9f8dadd1"
      },
      "outputs": [],
      "source": [
        "loss = 'sparse_categorical_crossentropy'\n",
        "\n",
        "metrics = ['accuracy']\n",
        "\n",
        "model.compile(loss=loss, optimizer=Adam(learning_rate = 1e-5), metrics=metrics)\n",
        "\n",
        "checkpoint_loc = '/content/drive/MyDrive/CSC 514/checkpoints/' + model_name\n",
        "log_path = \"/content/drive/MyDrive/CSC 514/logs/\"\n",
        "\n",
        "# Create checkpoint directory if does not exist\n",
        "if not os.path.exists(checkpoint_loc): os.makedirs(checkpoint_loc)\n",
        "if not os.path.exists(log_path): os.makedirs(log_path)\n",
        "\n",
        "checkpoint_path = os.path.join(checkpoint_loc, 'best_model.h5')\n",
        "\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100, verbose=1, mode='auto',\n",
        "#         restore_best_weights=True)\n",
        "\n",
        "callbacks = [\n",
        "    # tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=1),\n",
        "    \n",
        "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1,\n",
        "                      monitor='val_loss',\n",
        "                      patience=10,\n",
        "                      min_lr=0.00001,\n",
        "                      verbose=1,\n",
        "                      mode='auto'),\n",
        "    \n",
        "    tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                      monitor = 'val_loss',\n",
        "                      verbose = 1,\n",
        "                      save_best_only=True,\n",
        "                      save_weights_only=True,\n",
        "                      ),\n",
        "    tf.keras.callbacks.CSVLogger(os.path.join(log_path, model_name + '.csv'), separator=',', append=True),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49748766",
      "metadata": {
        "id": "49748766"
      },
      "outputs": [],
      "source": [
        "# Uncomment to load the model\n",
        "# model = resnet50(input_shape=XF.shape[1:], n_classes=n_classes)  \n",
        "# model.load_weights(checkpoint_loc + '//' + 'best_model.h5')\n",
        "# model.compile(loss=loss, optimizer=Adam(learning_rate = 1e-5), metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa95d6d",
      "metadata": {
        "id": "8fa95d6d"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca9ddec7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca9ddec7",
        "outputId": "1d5982e9-4d0f-4f4a-c76c-b67a2ae0611a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.8940 - accuracy: 0.3889\n",
            "Epoch 1: val_loss improved from inf to 0.62514, saving model to /content/drive/MyDrive/CSC 514/checkpoints/resnet50_2022-12-07_20-22-31/best_model.h5\n",
            "5/5 [==============================] - 18s 792ms/step - loss: 1.8940 - accuracy: 0.3889 - val_loss: 0.6251 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 2/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.2676 - accuracy: 0.3889\n",
            "Epoch 2: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 1.2676 - accuracy: 0.3889 - val_loss: 1.0588 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 3/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.9950 - accuracy: 0.5625\n",
            "Epoch 3: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.8913 - accuracy: 0.6111 - val_loss: 1.1129 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 4/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.0540 - accuracy: 0.5556\n",
            "Epoch 4: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 1.0540 - accuracy: 0.5556 - val_loss: 1.1190 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 5/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.9808 - accuracy: 0.5556\n",
            "Epoch 5: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.9808 - accuracy: 0.5556 - val_loss: 1.0958 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 6/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5950 - accuracy: 0.8125\n",
            "Epoch 6: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.6595 - accuracy: 0.7778 - val_loss: 1.1835 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 7/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.9021 - accuracy: 0.6875\n",
            "Epoch 7: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.8218 - accuracy: 0.7222 - val_loss: 1.2161 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 8/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5637 - accuracy: 0.8750\n",
            "Epoch 8: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.5366 - accuracy: 0.8889 - val_loss: 1.2282 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 9/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5513 - accuracy: 0.8125\n",
            "Epoch 9: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.5029 - accuracy: 0.8333 - val_loss: 1.2600 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 10/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.6613 - accuracy: 0.7500\n",
            "Epoch 10: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.6162 - accuracy: 0.7778 - val_loss: 1.2890 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 11/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3015 - accuracy: 0.9375\n",
            "Epoch 11: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.4598 - accuracy: 0.8333 - val_loss: 1.3311 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 12/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3924 - accuracy: 0.8750\n",
            "Epoch 12: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.3590 - accuracy: 0.8889 - val_loss: 1.3690 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 13/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.4405 - accuracy: 0.8750\n",
            "Epoch 13: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.3923 - accuracy: 0.8889 - val_loss: 1.3237 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 14/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.4059 - accuracy: 0.8750\n",
            "Epoch 14: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.4213 - accuracy: 0.8889 - val_loss: 1.3078 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 15/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5708 - accuracy: 0.7500\n",
            "Epoch 15: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.5863 - accuracy: 0.7222 - val_loss: 1.3395 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 16/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3201 - accuracy: 0.9375\n",
            "Epoch 16: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.4017 - accuracy: 0.8333 - val_loss: 1.3527 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 17/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5422 - accuracy: 0.8125\n",
            "Epoch 17: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.5017 - accuracy: 0.8333 - val_loss: 1.3396 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 18/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2010 - accuracy: 1.0000\n",
            "Epoch 18: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.2221 - accuracy: 1.0000 - val_loss: 1.3478 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 19/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.5581 - accuracy: 0.7500\n",
            "Epoch 19: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.5619 - accuracy: 0.7222 - val_loss: 1.3942 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 20/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3650 - accuracy: 0.8750\n",
            "Epoch 20: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.3328 - accuracy: 0.8889 - val_loss: 1.3427 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 21/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3558 - accuracy: 0.9375\n",
            "Epoch 21: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.3942 - accuracy: 0.8889 - val_loss: 1.2951 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 22/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2874 - accuracy: 0.9375\n",
            "Epoch 22: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.2702 - accuracy: 0.9444 - val_loss: 1.3233 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 23/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2719 - accuracy: 0.8750\n",
            "Epoch 23: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.2661 - accuracy: 0.8889 - val_loss: 1.3492 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 24/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3393 - accuracy: 0.9375\n",
            "Epoch 24: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.3159 - accuracy: 0.9444 - val_loss: 1.3636 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 25/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3327 - accuracy: 0.8750\n",
            "Epoch 25: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.3102 - accuracy: 0.8889 - val_loss: 1.3484 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 26/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.3393 - accuracy: 0.9375\n",
            "Epoch 26: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.3035 - accuracy: 0.9444 - val_loss: 1.3304 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 27/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.1376 - accuracy: 1.0000\n",
            "Epoch 27: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.1376 - accuracy: 1.0000 - val_loss: 1.3490 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 28/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2185 - accuracy: 0.9375\n",
            "Epoch 28: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.2678 - accuracy: 0.8889 - val_loss: 1.3687 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 29/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1903 - accuracy: 0.9375\n",
            "Epoch 29: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.2571 - accuracy: 0.8889 - val_loss: 1.3018 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 30/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1709 - accuracy: 0.9375\n",
            "Epoch 30: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.1606 - accuracy: 0.9444 - val_loss: 1.3008 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 31/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2135 - accuracy: 0.9375\n",
            "Epoch 31: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.2209 - accuracy: 0.9444 - val_loss: 1.3433 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 32/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9444\n",
            "Epoch 32: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.2822 - accuracy: 0.9444 - val_loss: 1.4081 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 33/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.8889\n",
            "Epoch 33: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.2664 - accuracy: 0.8889 - val_loss: 1.5090 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 34/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2555 - accuracy: 0.9375\n",
            "Epoch 34: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.2287 - accuracy: 0.9444 - val_loss: 1.6524 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 35/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2947 - accuracy: 0.8750\n",
            "Epoch 35: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.2636 - accuracy: 0.8889 - val_loss: 1.7161 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 36/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2382 - accuracy: 0.8750\n",
            "Epoch 36: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.2279 - accuracy: 0.8889 - val_loss: 1.5886 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 37/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1491 - accuracy: 1.0000\n",
            "Epoch 37: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1872 - accuracy: 1.0000 - val_loss: 1.5095 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 38/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1972 - accuracy: 0.9375\n",
            "Epoch 38: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.2062 - accuracy: 0.9444 - val_loss: 1.4752 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 39/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.2470 - accuracy: 1.0000\n",
            "Epoch 39: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.2269 - accuracy: 1.0000 - val_loss: 1.3490 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 40/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1468 - accuracy: 1.0000\n",
            "Epoch 40: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.1519 - accuracy: 1.0000 - val_loss: 1.1951 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 41/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0849 - accuracy: 1.0000\n",
            "Epoch 41: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0842 - accuracy: 1.0000 - val_loss: 1.0285 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 42/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1253 - accuracy: 1.0000\n",
            "Epoch 42: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1732 - accuracy: 0.9444 - val_loss: 0.9535 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 43/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1330 - accuracy: 0.9375\n",
            "Epoch 43: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.1352 - accuracy: 0.9444 - val_loss: 1.0692 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 44/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1938 - accuracy: 0.9375\n",
            "Epoch 44: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.1766 - accuracy: 0.9444 - val_loss: 1.1130 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 45/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1382 - accuracy: 1.0000\n",
            "Epoch 45: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.1263 - accuracy: 1.0000 - val_loss: 1.2212 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 46/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1947 - accuracy: 0.8750\n",
            "Epoch 46: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.1787 - accuracy: 0.8889 - val_loss: 1.1105 - val_accuracy: 0.0000e+00 - lr: 1.0000e-05\n",
            "Epoch 47/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1322 - accuracy: 0.9375\n",
            "Epoch 47: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.1235 - accuracy: 0.9444 - val_loss: 0.9838 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 48/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1258 - accuracy: 1.0000\n",
            "Epoch 48: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.1154 - accuracy: 1.0000 - val_loss: 0.8461 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 49/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1058 - accuracy: 1.0000\n",
            "Epoch 49: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0973 - accuracy: 1.0000 - val_loss: 0.8171 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 50/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0845 - accuracy: 1.0000\n",
            "Epoch 50: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.1170 - accuracy: 1.0000 - val_loss: 0.8156 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 51/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1482 - accuracy: 1.0000\n",
            "Epoch 51: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.1421 - accuracy: 1.0000 - val_loss: 0.8948 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 52/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0856 - accuracy: 1.0000\n",
            "Epoch 52: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0896 - accuracy: 1.0000 - val_loss: 0.8489 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 53/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0909 - accuracy: 1.0000\n",
            "Epoch 53: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.0946 - accuracy: 1.0000 - val_loss: 0.8859 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 54/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0921 - accuracy: 1.0000\n",
            "Epoch 54: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0839 - accuracy: 1.0000 - val_loss: 0.9124 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 55/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1078 - accuracy: 0.9375\n",
            "Epoch 55: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.1104 - accuracy: 0.9444 - val_loss: 1.0399 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 56/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1056 - accuracy: 1.0000\n",
            "Epoch 56: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0966 - accuracy: 1.0000 - val_loss: 1.1312 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 57/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1000 - accuracy: 1.0000\n",
            "Epoch 57: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.1025 - accuracy: 1.0000 - val_loss: 1.1498 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 58/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1111 - accuracy: 1.0000\n",
            "Epoch 58: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.1037 - accuracy: 1.0000 - val_loss: 1.1804 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 59/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0775 - accuracy: 1.0000\n",
            "Epoch 59: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.0748 - accuracy: 1.0000 - val_loss: 1.1683 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 60/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0955 - accuracy: 1.0000\n",
            "Epoch 60: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0960 - accuracy: 1.0000 - val_loss: 1.1983 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 61/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0478 - accuracy: 1.0000\n",
            "Epoch 61: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 1.1704 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 62/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0561 - accuracy: 1.0000\n",
            "Epoch 62: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0813 - accuracy: 1.0000 - val_loss: 1.1451 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 63/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0940 - accuracy: 1.0000\n",
            "Epoch 63: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.1056 - accuracy: 1.0000 - val_loss: 1.1370 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 64/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0909 - accuracy: 1.0000\n",
            "Epoch 64: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.1279 - accuracy: 1.0000 - val_loss: 1.3539 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 65/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0827 - accuracy: 1.0000\n",
            "Epoch 65: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0750 - accuracy: 1.0000 - val_loss: 1.5730 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 66/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1523 - accuracy: 0.9375\n",
            "Epoch 66: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.1362 - accuracy: 0.9444 - val_loss: 1.4677 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 67/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0871 - accuracy: 1.0000\n",
            "Epoch 67: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.0851 - accuracy: 1.0000 - val_loss: 1.4848 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 68/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0921 - accuracy: 1.0000\n",
            "Epoch 68: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0830 - accuracy: 1.0000 - val_loss: 1.5418 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 69/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0843 - accuracy: 1.0000\n",
            "Epoch 69: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0769 - accuracy: 1.0000 - val_loss: 1.5252 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 70/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0486 - accuracy: 1.0000\n",
            "Epoch 70: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 1.5983 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 71/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1123 - accuracy: 1.0000\n",
            "Epoch 71: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.1007 - accuracy: 1.0000 - val_loss: 1.6558 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 72/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0705 - accuracy: 1.0000\n",
            "Epoch 72: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0747 - accuracy: 1.0000 - val_loss: 1.6938 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 73/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0478 - accuracy: 1.0000\n",
            "Epoch 73: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0499 - accuracy: 1.0000 - val_loss: 1.8594 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 74/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0568 - accuracy: 1.0000\n",
            "Epoch 74: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0523 - accuracy: 1.0000 - val_loss: 1.8779 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 75/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0482 - accuracy: 1.0000\n",
            "Epoch 75: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0553 - accuracy: 1.0000 - val_loss: 1.8223 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 76/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0938 - accuracy: 1.0000\n",
            "Epoch 76: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.0878 - accuracy: 1.0000 - val_loss: 1.5780 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 77/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0451 - accuracy: 1.0000\n",
            "Epoch 77: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 1.5307 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 78/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0362 - accuracy: 1.0000\n",
            "Epoch 78: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0766 - accuracy: 0.9444 - val_loss: 1.5893 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 79/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0432 - accuracy: 1.0000\n",
            "Epoch 79: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0788 - accuracy: 1.0000 - val_loss: 1.4156 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 80/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0531 - accuracy: 1.0000\n",
            "Epoch 80: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 1.3370 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 81/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.1013 - accuracy: 1.0000\n",
            "Epoch 81: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0920 - accuracy: 1.0000 - val_loss: 1.3548 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 82/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0557 - accuracy: 1.0000\n",
            "Epoch 82: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0786 - accuracy: 1.0000 - val_loss: 1.4666 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 83/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 1.0000\n",
            "Epoch 83: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 101ms/step - loss: 0.0655 - accuracy: 1.0000 - val_loss: 1.6971 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 84/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0727 - accuracy: 1.0000\n",
            "Epoch 84: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0657 - accuracy: 1.0000 - val_loss: 1.8627 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 85/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0243 - accuracy: 1.0000\n",
            "Epoch 85: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 2.1778 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 86/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 1.0000\n",
            "Epoch 86: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 2.2284 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 87/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0618 - accuracy: 1.0000\n",
            "Epoch 87: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0564 - accuracy: 1.0000 - val_loss: 2.2158 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 88/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 1.0000\n",
            "Epoch 88: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 1s 114ms/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 2.3325 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 89/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 1.0000\n",
            "Epoch 89: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 1s 123ms/step - loss: 0.0617 - accuracy: 1.0000 - val_loss: 2.3661 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 90/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0728 - accuracy: 1.0000\n",
            "Epoch 90: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0653 - accuracy: 1.0000 - val_loss: 2.5259 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 91/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 1.0000\n",
            "Epoch 91: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 102ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 2.5101 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 92/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0682 - accuracy: 1.0000\n",
            "Epoch 92: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 2.4844 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 93/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0449 - accuracy: 1.0000\n",
            "Epoch 93: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 2.5774 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 94/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
            "Epoch 94: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 2.5644 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 95/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0416 - accuracy: 1.0000\n",
            "Epoch 95: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 2.5674 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 96/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0407 - accuracy: 1.0000\n",
            "Epoch 96: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 2.5068 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 97/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0250 - accuracy: 1.0000\n",
            "Epoch 97: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 2.4448 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 98/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 1.0000\n",
            "Epoch 98: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 2.4643 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 99/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0658 - accuracy: 1.0000\n",
            "Epoch 99: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0601 - accuracy: 1.0000 - val_loss: 2.3956 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 100/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0627 - accuracy: 1.0000\n",
            "Epoch 100: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0681 - accuracy: 1.0000 - val_loss: 2.5828 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 101/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 101: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 2.6598 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 102/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0134 - accuracy: 1.0000\n",
            "Epoch 102: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 2.6980 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 103/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0475 - accuracy: 1.0000\n",
            "Epoch 103: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 2.7581 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 104/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0245 - accuracy: 1.0000\n",
            "Epoch 104: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 2.5301 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 105/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0171 - accuracy: 1.0000\n",
            "Epoch 105: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 2.6198 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 106/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0466 - accuracy: 1.0000\n",
            "Epoch 106: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 2.7252 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 107/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0722 - accuracy: 1.0000\n",
            "Epoch 107: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 2.6902 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 108/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0427 - accuracy: 1.0000\n",
            "Epoch 108: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 2.7831 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 109/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000\n",
            "Epoch 109: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 62ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 2.8114 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 110/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0418 - accuracy: 1.0000\n",
            "Epoch 110: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 2.7922 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 111/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0185 - accuracy: 1.0000\n",
            "Epoch 111: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 2.8119 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 112/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0658 - accuracy: 1.0000\n",
            "Epoch 112: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0588 - accuracy: 1.0000 - val_loss: 2.9792 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 113/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000\n",
            "Epoch 113: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 3.1335 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 114/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0433 - accuracy: 1.0000\n",
            "Epoch 114: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0439 - accuracy: 1.0000 - val_loss: 3.1130 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 115/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0290 - accuracy: 1.0000\n",
            "Epoch 115: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 3.2312 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 116/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000\n",
            "Epoch 116: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 3.1710 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 117/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 1.0000\n",
            "Epoch 117: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.4252 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 118/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0451 - accuracy: 1.0000\n",
            "Epoch 118: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 3.6462 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 119/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0506 - accuracy: 1.0000\n",
            "Epoch 119: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 3.6017 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 120/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0414 - accuracy: 1.0000\n",
            "Epoch 120: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 3.6529 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 121/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 121: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 3.6464 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 122/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0275 - accuracy: 1.0000\n",
            "Epoch 122: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 3.6338 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 123/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 123: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 3.5595 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 124/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 124: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 63ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 3.6253 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 125/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0506 - accuracy: 1.0000\n",
            "Epoch 125: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 3.8055 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 126/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 1.0000\n",
            "Epoch 126: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0446 - accuracy: 1.0000 - val_loss: 3.5081 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 127/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0166 - accuracy: 1.0000\n",
            "Epoch 127: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 3.7147 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 128/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0194 - accuracy: 1.0000\n",
            "Epoch 128: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 3.8462 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 129/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
            "Epoch 129: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 3.8204 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 130/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 130: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 3.8304 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 131/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0223 - accuracy: 1.0000\n",
            "Epoch 131: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 3.9560 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 132/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 1.0000\n",
            "Epoch 132: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 4.0221 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 133/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 1.0000\n",
            "Epoch 133: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 3.6677 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 134/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 134: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 3.4702 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 135/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 1.0000\n",
            "Epoch 135: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 3.2630 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 136/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 1.0000\n",
            "Epoch 136: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 79ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 3.1391 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 137/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
            "Epoch 137: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 3.1934 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 138/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 138: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.1351 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 139/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 139: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 3.2176 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 140/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
            "Epoch 140: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 3.0145 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 141/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0206 - accuracy: 1.0000\n",
            "Epoch 141: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 2.9144 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 142/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0147 - accuracy: 1.0000\n",
            "Epoch 142: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 3.1443 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 143/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0261 - accuracy: 1.0000\n",
            "Epoch 143: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 3.3393 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 144/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 144: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 3.4081 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 145/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0197 - accuracy: 1.0000\n",
            "Epoch 145: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 3.3749 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 146/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0562 - accuracy: 1.0000\n",
            "Epoch 146: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 3.2308 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 147/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0261 - accuracy: 1.0000\n",
            "Epoch 147: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 3.4156 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 148/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 148: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.4108 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 149/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 149: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 3.2751 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 150/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 150: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 3.2524 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 151/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 151: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 3.2723 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 152/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000\n",
            "Epoch 152: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 3.4769 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 153/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0256 - accuracy: 1.0000\n",
            "Epoch 153: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 3.3627 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 154/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0247 - accuracy: 1.0000\n",
            "Epoch 154: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 3.4731 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 155/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0230 - accuracy: 1.0000\n",
            "Epoch 155: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 3.6759 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 156/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0121 - accuracy: 1.0000\n",
            "Epoch 156: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 3.8054 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 157/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 157: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 3.7955 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 158/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 158: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 3.7892 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 159/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 159: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.9593 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 160/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 1.0000\n",
            "Epoch 160: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0753 - accuracy: 1.0000 - val_loss: 4.0191 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 161/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 161: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 3.8880 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 162/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000\n",
            "Epoch 162: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 4.0019 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 163/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0223 - accuracy: 1.0000\n",
            "Epoch 163: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 3.8843 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 164/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0442 - accuracy: 1.0000\n",
            "Epoch 164: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 4.0010 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 165/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 165: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 3.7505 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 166/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 166: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 3.6997 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 167/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0132 - accuracy: 1.0000\n",
            "Epoch 167: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0882 - accuracy: 0.9444 - val_loss: 3.7350 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 168/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 1.0000\n",
            "Epoch 168: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 3.2766 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 169/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0457 - accuracy: 1.0000\n",
            "Epoch 169: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 3.3245 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 170/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0525 - accuracy: 1.0000\n",
            "Epoch 170: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0496 - accuracy: 1.0000 - val_loss: 3.1026 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 171/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0418 - accuracy: 1.0000\n",
            "Epoch 171: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 2.9242 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 172/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0203 - accuracy: 1.0000\n",
            "Epoch 172: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 2.9080 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 173/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 173: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 2.7527 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 174/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 174: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 3.0569 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 175/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0186 - accuracy: 1.0000\n",
            "Epoch 175: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 3.2017 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 176/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 176: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 3.4504 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 177/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
            "Epoch 177: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 3.5334 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 178/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 1.0000\n",
            "Epoch 178: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 3.3776 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 179/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0246 - accuracy: 1.0000\n",
            "Epoch 179: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 3.1752 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 180/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 180: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 2.9207 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 181/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 181: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 3.2150 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 182/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0301 - accuracy: 1.0000\n",
            "Epoch 182: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 3.1835 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 183/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 183: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 3.0529 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 184/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 184: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 3.1299 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 185/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 1.0000\n",
            "Epoch 185: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 2.9407 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 186/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 186: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 3.0704 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 187/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 187: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 3.2256 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 188/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 188: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.3424 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 189/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000\n",
            "Epoch 189: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 3.1925 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 190/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0189 - accuracy: 1.0000\n",
            "Epoch 190: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 3.3334 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 191/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 191: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.3694 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 192/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 192: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.4008 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 193/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 193: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 3.4960 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 194/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 194: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 3.4439 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 195/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 195: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 3.4588 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 196/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0260 - accuracy: 1.0000\n",
            "Epoch 196: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 3.5358 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 197/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 197: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 3.7185 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 198/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 198: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.8457 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 199/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 199: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 3.9204 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 200/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0132 - accuracy: 1.0000\n",
            "Epoch 200: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 4.0211 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 201/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 201: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 3.7588 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 202/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000\n",
            "Epoch 202: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 3.7285 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 203/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0297 - accuracy: 1.0000\n",
            "Epoch 203: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 3.4465 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 204/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 204: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.2310 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 205/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0139 - accuracy: 1.0000\n",
            "Epoch 205: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 3.0426 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 206/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 206: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.8313 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 207/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 207: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 2.9905 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 208/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 1.0000\n",
            "Epoch 208: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.1403 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 209/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 209: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.1766 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 210/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 210: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 3.3882 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 211/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 211: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 3.5189 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 212/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 212: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 3.3992 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 213/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 213: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.6441 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 214/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 214: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 3.6598 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 215/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 215: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 3.7864 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 216/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 216: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 3.8592 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 217/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 217: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.3168 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 218/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 218: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.1193 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 219/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 219: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.0568 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 220/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 220: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.2968 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 221/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 221: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 3.1205 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 222/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 1.0000\n",
            "Epoch 222: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 3.0395 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 223/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 223: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.0810 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 224/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 224: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.1397 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 225/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 225: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.0769 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 226/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 226: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 3.1238 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 227/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 227: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 3.0465 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 228/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 228: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 2.9797 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 229/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 229: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 2.7712 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 230/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0183 - accuracy: 1.0000\n",
            "Epoch 230: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 2.6899 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 231/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0166 - accuracy: 1.0000\n",
            "Epoch 231: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 2.8815 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 232/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
            "Epoch 232: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 3.5727 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 233/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0125 - accuracy: 1.0000\n",
            "Epoch 233: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 3.5552 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 234/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 234: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 3.4700 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 235/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0171 - accuracy: 1.0000\n",
            "Epoch 235: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 3.4542 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 236/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 1.0000\n",
            "Epoch 236: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 3.5542 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 237/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 237: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 3.8093 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 238/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 238: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 3.7826 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 239/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 239: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 3.7357 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 240/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 240: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.7842 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 241/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 241: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.6311 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 242/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0181 - accuracy: 1.0000\n",
            "Epoch 242: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 3.3212 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 243/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0223 - accuracy: 1.0000\n",
            "Epoch 243: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 3.2209 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 244/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 244: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 3.0213 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 245/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 245: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 3.3108 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 246/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 246: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.9774 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 247/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 247: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 3.1430 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 248/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 248: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.4197 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 249/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0172 - accuracy: 1.0000\n",
            "Epoch 249: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 3.2859 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 250/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 250: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 3.4570 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 251/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 251: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 3.6385 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 252/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 252: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.9205 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 253/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
            "Epoch 253: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 4.0933 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 254/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 254: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 4.0311 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 255/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0172 - accuracy: 1.0000\n",
            "Epoch 255: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 4.2644 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 256/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 256: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 4.3032 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 257/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 257: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 4.5418 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 258/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 258: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 4.2425 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 259/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
            "Epoch 259: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 3.8708 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 260/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 260: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.8480 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 261/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 261: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 79ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 3.5937 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 262/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0226 - accuracy: 1.0000\n",
            "Epoch 262: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 3.8024 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 263/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 263: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 3.7344 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 264/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 264: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.4943 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 265/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 265: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.5390 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 266/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 266: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 3.6082 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 267/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 267: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.6216 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 268/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 268: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.6273 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 269/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 269: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.6447 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 270/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 270: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 3.9291 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 271/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 271: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 3.7558 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 272/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 272: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.7307 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 273/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 273: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.9205 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 274/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 274: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 4.2252 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 275/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 275: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 4.3111 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 276/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 276: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.9702 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 277/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 277: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.2182 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 278/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 278: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.8900 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 279/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 279: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 4.0185 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 280/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 280: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.9823 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 281/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 281: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.8365 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 282/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 282: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 3.6834 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 283/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 283: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 3.6344 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 284/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 284: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 3.5461 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 285/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 285: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.3409 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 286/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 286: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.4859 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 287/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 287: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.5142 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 288/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 288: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.4258 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 289/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 289: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.3847 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 290/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 290: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 3.5244 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 291/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 291: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.3102 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 292/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 292: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.4864 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 293/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 293: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 3.5116 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 294/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 294: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.6659 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 295/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 295: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.4427 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 296/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 296: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.5868 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 297/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 297: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.8204 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 298/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 298: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.9024 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 299/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 299: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.1201 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 300/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 300: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.0755 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 301/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 301: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 4.0206 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 302/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 302: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 4.1828 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 303/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 303: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.6798 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 304/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 304: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.3708 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 305/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 305: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.2645 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 306/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000\n",
            "Epoch 306: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 3.4970 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 307/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 307: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.7259 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 308/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 308: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.8481 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 309/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 309: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.7800 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 310/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 310: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.8592 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 311/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 311: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 3.9292 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 312/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 312: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.9457 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 313/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 313: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 3.8485 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 314/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0127 - accuracy: 1.0000\n",
            "Epoch 314: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 3.8470 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 315/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 315: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 3.3364 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 316/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 1.0000\n",
            "Epoch 316: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 3.4737 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 317/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 317: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 3.5214 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 318/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 318: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.3232 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 319/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 319: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 3.6063 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 320/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 320: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.5522 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 321/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 321: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.5367 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 322/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 322: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 3.6399 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 323/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 323: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.8319 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 324/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 324: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.6820 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 325/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 325: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.7789 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 326/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 326: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 3.7676 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 327/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 327: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.7756 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 328/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 328: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.6986 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 329/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 329: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.7084 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 330/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 330: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.5110 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 331/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 331: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.5557 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 332/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 332: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.7200 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 333/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 333: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.8008 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 334/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 334: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.7477 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 335/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 335: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.6778 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 336/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 336: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.8079 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 337/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 337: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.7174 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 338/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 338: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.7207 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 339/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 339: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.7381 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 340/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 340: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.8589 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 341/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 341: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.8314 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 342/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 342: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.8096 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 343/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 343: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.6836 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 344/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 344: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 3.9424 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 345/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 345: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.7523 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 346/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 346: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.7473 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 347/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 347: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.5321 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 348/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 348: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 3.4811 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 349/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 349: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.1198 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 350/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 350: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.3007 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 351/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 351: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.1542 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 352/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 352: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.5840 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 353/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 353: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.6574 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 354/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 354: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.9225 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 355/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 355: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.0102 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 356/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 356: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.1520 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 357/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 357: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0614 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 358/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 358: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.0720 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 359/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 359: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 4.3828 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 360/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 360: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 4.0181 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 361/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 361: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 4.0298 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 362/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 362: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 4.0266 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 363/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 363: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.7052 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 364/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 364: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 3.5441 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 365/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 365: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.2180 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 366/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 366: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 3.1151 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 367/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0251 - accuracy: 1.0000\n",
            "Epoch 367: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 3.1499 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 368/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 368: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.0734 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 369/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 369: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.3258 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 370/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 370: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.2113 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 371/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 371: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.1845 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 372/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 1.0000\n",
            "Epoch 372: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 3.2612 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 373/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 373: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 79ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.4676 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 374/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 374: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.5683 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 375/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 375: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.6894 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 376/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.0960e-04 - accuracy: 1.0000\n",
            "Epoch 376: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 9.0960e-04 - accuracy: 1.0000 - val_loss: 3.5260 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 377/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 377: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.2788 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 378/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 378: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.1525 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 379/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 379: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.2678 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 380/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 380: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.4507 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 381/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 381: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 3.1070 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 382/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 382: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 3.5370 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 383/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 383: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.8599 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 384/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 384: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.3637 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 385/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 385: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.4952 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 386/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 386: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.4420 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 387/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 387: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.3899 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 388/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 388: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 3.3886 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 389/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 389: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 3.5871 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 390/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 390: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.9221 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 391/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 391: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 4.1557 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 392/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 392: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.0826 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 393/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 393: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.2246 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 394/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 394: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.2262 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 395/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 395: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.2451 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 396/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 396: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.2815 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 397/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 397: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.0967 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 398/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 398: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 4.1477 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 399/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 399: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 4.3134 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 400/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 400: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.4566 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 401/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 401: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.4055 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 402/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 402: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.5704 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 403/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 403: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.5277 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 404/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 404: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 4.3074 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 405/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 405: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 4.4653 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 406/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 406: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.2407 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 407/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 407: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 4.3584 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 408/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 408: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.5513 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 409/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 409: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.7715 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 410/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 410: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.7976 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 411/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 411: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 4.9294 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 412/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 412: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.6562 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 413/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 413: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.4014 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 414/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 414: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.5675 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 415/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 415: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.5346 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 416/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 416: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.3592 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 417/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 417: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.4470 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 418/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 418: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.6767 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 419/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 419: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.9597 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 420/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 420: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.9375 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 421/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 421: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.8029 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 422/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000    \n",
            "Epoch 422: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 4.8983 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 423/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 423: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 5.2140 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 424/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 424: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 5.0809 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 425/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 425: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.1197 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 426/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 426: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 5.1881 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 427/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 427: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 5.3350 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 428/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.8137e-04 - accuracy: 1.0000\n",
            "Epoch 428: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 9.8257e-04 - accuracy: 1.0000 - val_loss: 5.2379 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 429/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 429: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 5.3892 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 430/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 430: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 5.3761 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 431/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 431: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 5.3164 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 432/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 432: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 5.4436 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 433/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 433: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 5.2092 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 434/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 434: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 5.1833 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 435/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 435: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 5.3032 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 436/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 436: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 5.2452 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 437/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 437: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 5.1919 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 438/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 438: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 5.2000 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 439/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 439: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.9776 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 440/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 440: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.9614 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 441/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 441: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.9224 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 442/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 442: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.8165 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 443/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 443: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 5.0605 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 444/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.6359e-04 - accuracy: 1.0000\n",
            "Epoch 444: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 5.0221 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 445/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 445: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.8544 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 446/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 446: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.8251 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 447/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 447: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.9238 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 448/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 448: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.3982 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 449/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.2088e-04 - accuracy: 1.0000\n",
            "Epoch 449: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 8.2088e-04 - accuracy: 1.0000 - val_loss: 4.4535 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 450/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 450: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.5502 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 451/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 451: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.4724 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 452/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 452: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.5378 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 453/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 453: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.3605 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 454/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 454: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.4595 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 455/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 455: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.2454 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 456/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 456: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.2422 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 457/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 457: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 4.3018 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 458/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 458: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.2195 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 459/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 459: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1385 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 460/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.1258e-04 - accuracy: 1.0000\n",
            "Epoch 460: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 8.2542e-04 - accuracy: 1.0000 - val_loss: 4.1397 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 461/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 461: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.0538 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 462/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 462: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.2625 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 463/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 463: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1346 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 464/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.0766e-04 - accuracy: 1.0000\n",
            "Epoch 464: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 8.3311e-04 - accuracy: 1.0000 - val_loss: 4.0860 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 465/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 465: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1902 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 466/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 466: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 4.3224 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 467/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 467: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 4.4335 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 468/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 468: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 4.3767 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 469/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 469: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.3589 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 470/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.8555e-04 - accuracy: 1.0000\n",
            "Epoch 470: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 9.4273e-04 - accuracy: 1.0000 - val_loss: 4.4086 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 471/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 471: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 9.4827e-04 - accuracy: 1.0000 - val_loss: 4.6243 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 472/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.2980e-04 - accuracy: 1.0000\n",
            "Epoch 472: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.1772e-04 - accuracy: 1.0000 - val_loss: 4.5558 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 473/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 473: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 9.9275e-04 - accuracy: 1.0000 - val_loss: 4.4569 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 474/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 474: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.4188 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 475/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 475: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.5149 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 476/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000    \n",
            "Epoch 476: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.3756 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 477/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 477: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 4.5392 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 478/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 478: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.4899 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 479/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 479: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 9.7084e-04 - accuracy: 1.0000 - val_loss: 4.5218 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 480/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.3140e-04 - accuracy: 1.0000\n",
            "Epoch 480: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 8.3140e-04 - accuracy: 1.0000 - val_loss: 4.5187 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 481/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 481: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 4.4467 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 482/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 482: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.4787 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 483/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 483: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.2946 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 484/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 484: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 4.4932 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 485/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.6611e-04 - accuracy: 1.0000\n",
            "Epoch 485: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 9.6611e-04 - accuracy: 1.0000 - val_loss: 4.2167 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 486/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 486: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.7170 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 487/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.6998e-04 - accuracy: 1.0000\n",
            "Epoch 487: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 7.3153e-04 - accuracy: 1.0000 - val_loss: 4.9796 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 488/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.8194e-04 - accuracy: 1.0000\n",
            "Epoch 488: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.7987 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 489/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.0163e-04 - accuracy: 1.0000\n",
            "Epoch 489: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 5.4927e-04 - accuracy: 1.0000 - val_loss: 4.8752 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 490/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.6263e-04 - accuracy: 1.0000\n",
            "Epoch 490: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 7.6263e-04 - accuracy: 1.0000 - val_loss: 4.8999 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 491/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 491: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.7049 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 492/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 492: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.8624 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 493/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 493: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.6473 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 494/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 494: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.7940 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 495/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000    \n",
            "Epoch 495: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.9625 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 496/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 496: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 4.6793 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 497/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 497: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.8042 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 498/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 498: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.9534 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 499/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.6341e-04 - accuracy: 1.0000\n",
            "Epoch 499: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 8.6341e-04 - accuracy: 1.0000 - val_loss: 4.7851 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 500/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 500: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.5821 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 501/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 501: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.9223 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 502/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 502: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.7381 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 503/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 503: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.6523 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 504/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 504: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 4.9466 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 505/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 505: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 4.8349 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 506/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.4972e-04 - accuracy: 1.0000\n",
            "Epoch 506: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 6.0394e-04 - accuracy: 1.0000 - val_loss: 4.8596 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 507/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.4534e-04 - accuracy: 1.0000\n",
            "Epoch 507: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 9.4534e-04 - accuracy: 1.0000 - val_loss: 4.8502 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 508/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.6369e-04 - accuracy: 1.0000\n",
            "Epoch 508: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 8.9642e-04 - accuracy: 1.0000 - val_loss: 4.8790 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 509/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 509: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.7715 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 510/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.3269e-04 - accuracy: 1.0000\n",
            "Epoch 510: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 9.3269e-04 - accuracy: 1.0000 - val_loss: 4.6801 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 511/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 511: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 4.6458 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 512/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000    \n",
            "Epoch 512: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.6398 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 513/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 513: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 5.0147 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 514/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 514: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.6275 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 515/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 515: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.6727 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 516/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 516: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.7076 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 517/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 517: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.4711 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 518/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 518: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 4.7313 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 519/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000    \n",
            "Epoch 519: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.8332 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 520/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 520: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.7649 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 521/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 521: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.6404 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 522/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 522: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 9.8856e-04 - accuracy: 1.0000 - val_loss: 4.6799 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 523/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 523: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.5178 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 524/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 524: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.3806 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 525/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 525: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.5382 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 526/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 526: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.5636 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 527/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 527: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.8985 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 528/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 528: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.8970 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 529/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 529: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.7796 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 530/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 530: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.7860 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 531/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 531: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 81ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 4.7579 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 532/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 532: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 4.7944 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 533/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.9985e-04 - accuracy: 1.0000\n",
            "Epoch 533: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 7.9985e-04 - accuracy: 1.0000 - val_loss: 4.8138 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 534/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 534: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.8249 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 535/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 535: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 4.7265 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 536/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 536: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.6014 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 537/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000    \n",
            "Epoch 537: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.7133 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 538/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.5738e-04 - accuracy: 1.0000\n",
            "Epoch 538: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 7.5738e-04 - accuracy: 1.0000 - val_loss: 4.8122 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 539/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 539: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.6690 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 540/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 540: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.6205 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 541/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 541: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.7364 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 542/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 542: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.7111 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 543/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 543: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.9444 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 544/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.3113e-04 - accuracy: 1.0000\n",
            "Epoch 544: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 9.3113e-04 - accuracy: 1.0000 - val_loss: 4.9162 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 545/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 545: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.8797 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 546/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.9563e-04 - accuracy: 1.0000\n",
            "Epoch 546: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.6033e-04 - accuracy: 1.0000 - val_loss: 4.8821 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 547/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 547: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.9060 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 548/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 548: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.8176 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 549/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.6801e-04 - accuracy: 1.0000\n",
            "Epoch 549: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 9.0078e-04 - accuracy: 1.0000 - val_loss: 4.7608 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 550/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.8328e-04 - accuracy: 1.0000\n",
            "Epoch 550: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 5.8328e-04 - accuracy: 1.0000 - val_loss: 4.8202 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 551/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 551: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.7775 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 552/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 552: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.7426 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 553/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 553: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.8294 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 554/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 554: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.8674 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 555/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 555: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.9721 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 556/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.7623e-04 - accuracy: 1.0000\n",
            "Epoch 556: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 9.7623e-04 - accuracy: 1.0000 - val_loss: 4.7242 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 557/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000    \n",
            "Epoch 557: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.4820 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 558/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 558: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.3992 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 559/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 559: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.3790 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 560/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 560: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.6041 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 561/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.3545e-04 - accuracy: 1.0000\n",
            "Epoch 561: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 6.6306e-04 - accuracy: 1.0000 - val_loss: 4.4566 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 562/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.2870e-04 - accuracy: 1.0000\n",
            "Epoch 562: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.6567e-04 - accuracy: 1.0000 - val_loss: 4.7012 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 563/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 563: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.7279 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 564/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.1952e-04 - accuracy: 1.0000\n",
            "Epoch 564: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 6.8769e-04 - accuracy: 1.0000 - val_loss: 4.8254 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 565/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.3442e-04 - accuracy: 1.0000\n",
            "Epoch 565: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 6.2653e-04 - accuracy: 1.0000 - val_loss: 4.8835 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 566/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 566: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.8794 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 567/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 567: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.9040 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 568/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.7394e-04 - accuracy: 1.0000\n",
            "Epoch 568: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 9.8074e-04 - accuracy: 1.0000 - val_loss: 5.0037 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 569/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 569: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 5.0310 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 570/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000    \n",
            "Epoch 570: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.9911 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 571/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.3846e-04 - accuracy: 1.0000\n",
            "Epoch 571: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 5.2621e-04 - accuracy: 1.0000 - val_loss: 4.9194 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 572/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 572: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.8821 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 573/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.9192e-04 - accuracy: 1.0000\n",
            "Epoch 573: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 7.1943e-04 - accuracy: 1.0000 - val_loss: 4.9818 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 574/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.6976e-04 - accuracy: 1.0000\n",
            "Epoch 574: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 8.9117e-04 - accuracy: 1.0000 - val_loss: 4.8939 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 575/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6220e-04 - accuracy: 1.0000\n",
            "Epoch 575: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.5391e-04 - accuracy: 1.0000 - val_loss: 4.9350 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 576/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 576: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 9.9075e-04 - accuracy: 1.0000 - val_loss: 4.9065 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 577/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 577: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.7410 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 578/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.1134e-04 - accuracy: 1.0000\n",
            "Epoch 578: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 7.1134e-04 - accuracy: 1.0000 - val_loss: 4.8743 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 579/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 579: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 5.0341 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 580/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000    \n",
            "Epoch 580: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.9211 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 581/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.2335e-04 - accuracy: 1.0000\n",
            "Epoch 581: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 7.4249e-04 - accuracy: 1.0000 - val_loss: 5.0342 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 582/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.9693e-04 - accuracy: 1.0000\n",
            "Epoch 582: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.4874e-04 - accuracy: 1.0000 - val_loss: 5.0019 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 583/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 583: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 5.1578 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 584/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.7792e-04 - accuracy: 1.0000\n",
            "Epoch 584: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 9.7792e-04 - accuracy: 1.0000 - val_loss: 4.9476 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 585/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.3248e-04 - accuracy: 1.0000\n",
            "Epoch 585: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 5.7950e-04 - accuracy: 1.0000 - val_loss: 5.0175 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 586/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 586: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 4.9306 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 587/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.3799e-04 - accuracy: 1.0000\n",
            "Epoch 587: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 8.3799e-04 - accuracy: 1.0000 - val_loss: 4.8681 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 588/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.8414e-04 - accuracy: 1.0000\n",
            "Epoch 588: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 8.0451e-04 - accuracy: 1.0000 - val_loss: 4.8542 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 589/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 589: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.7627 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 590/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000    \n",
            "Epoch 590: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 4.8633 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 591/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.3228e-04 - accuracy: 1.0000\n",
            "Epoch 591: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.6211e-04 - accuracy: 1.0000 - val_loss: 4.8359 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 592/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.9473e-04 - accuracy: 1.0000\n",
            "Epoch 592: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 8.0327e-04 - accuracy: 1.0000 - val_loss: 4.7131 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 593/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.4920e-04 - accuracy: 1.0000\n",
            "Epoch 593: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.5321e-04 - accuracy: 1.0000 - val_loss: 4.9070 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 594/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.6778e-04 - accuracy: 1.0000\n",
            "Epoch 594: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 6.1538e-04 - accuracy: 1.0000 - val_loss: 4.7915 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 595/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.5184e-04 - accuracy: 1.0000\n",
            "Epoch 595: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 4.3954e-04 - accuracy: 1.0000 - val_loss: 4.8993 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 596/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 596: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.7388 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 597/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 597: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.8451 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 598/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.7814e-04 - accuracy: 1.0000\n",
            "Epoch 598: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 4.7814e-04 - accuracy: 1.0000 - val_loss: 4.8652 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 599/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1755e-04 - accuracy: 1.0000\n",
            "Epoch 599: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.9039e-04 - accuracy: 1.0000 - val_loss: 4.9514 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 600/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 600: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 5.0105 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 601/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.5202e-04 - accuracy: 1.0000\n",
            "Epoch 601: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 7.5202e-04 - accuracy: 1.0000 - val_loss: 5.0307 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 602/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 602: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 5.0718 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 603/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 603: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.9309 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 604/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.6039e-04 - accuracy: 1.0000\n",
            "Epoch 604: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 5.0786e-04 - accuracy: 1.0000 - val_loss: 5.0247 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 605/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 605: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 5.0798 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 606/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.2757e-04 - accuracy: 1.0000\n",
            "Epoch 606: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 6.5850e-04 - accuracy: 1.0000 - val_loss: 5.2131 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 607/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000    \n",
            "Epoch 607: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.0334 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 608/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 608: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.8888 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 609/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.8038e-04 - accuracy: 1.0000\n",
            "Epoch 609: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 5.8038e-04 - accuracy: 1.0000 - val_loss: 4.8486 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 610/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.8494e-04 - accuracy: 1.0000\n",
            "Epoch 610: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 64ms/step - loss: 7.3069e-04 - accuracy: 1.0000 - val_loss: 4.7842 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 611/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.3137e-04 - accuracy: 1.0000\n",
            "Epoch 611: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 6.8822e-04 - accuracy: 1.0000 - val_loss: 4.9075 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 612/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 612: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.8395 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 613/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.5763e-04 - accuracy: 1.0000\n",
            "Epoch 613: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 5.5763e-04 - accuracy: 1.0000 - val_loss: 4.8423 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 614/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 614: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.7720 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 615/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.4627e-04 - accuracy: 1.0000\n",
            "Epoch 615: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 8.4627e-04 - accuracy: 1.0000 - val_loss: 4.6712 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 616/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 616: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.5036 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 617/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.1200e-04 - accuracy: 1.0000\n",
            "Epoch 617: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 8.4523e-04 - accuracy: 1.0000 - val_loss: 4.5057 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 618/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.7747e-04 - accuracy: 1.0000\n",
            "Epoch 618: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 7.9043e-04 - accuracy: 1.0000 - val_loss: 4.6210 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 619/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 619: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.5499 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 620/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3913e-04 - accuracy: 1.0000\n",
            "Epoch 620: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.4264e-04 - accuracy: 1.0000 - val_loss: 4.6277 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 621/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6023e-04 - accuracy: 1.0000\n",
            "Epoch 621: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 5.2064e-04 - accuracy: 1.0000 - val_loss: 4.6860 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 622/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.6204e-04 - accuracy: 1.0000\n",
            "Epoch 622: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 4.6204e-04 - accuracy: 1.0000 - val_loss: 4.7112 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 623/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 623: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.5475 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 624/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 624: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 92ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6477 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 625/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.7308e-04 - accuracy: 1.0000\n",
            "Epoch 625: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 5.7308e-04 - accuracy: 1.0000 - val_loss: 4.6947 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 626/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.0753e-04 - accuracy: 1.0000\n",
            "Epoch 626: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 9.0753e-04 - accuracy: 1.0000 - val_loss: 4.6245 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 627/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.4812e-04 - accuracy: 1.0000\n",
            "Epoch 627: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 9.4812e-04 - accuracy: 1.0000 - val_loss: 4.6978 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 628/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.0220e-04 - accuracy: 1.0000\n",
            "Epoch 628: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 8.1942e-04 - accuracy: 1.0000 - val_loss: 4.6030 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 629/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.9209e-04 - accuracy: 1.0000\n",
            "Epoch 629: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 9.9209e-04 - accuracy: 1.0000 - val_loss: 4.6462 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 630/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.1359e-04 - accuracy: 1.0000\n",
            "Epoch 630: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 6.1359e-04 - accuracy: 1.0000 - val_loss: 4.6742 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 631/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2498e-04 - accuracy: 1.0000\n",
            "Epoch 631: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.5206e-04 - accuracy: 1.0000 - val_loss: 4.5414 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 632/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.8504e-04 - accuracy: 1.0000\n",
            "Epoch 632: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.0881e-04 - accuracy: 1.0000 - val_loss: 4.5089 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 633/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.8773e-04 - accuracy: 1.0000\n",
            "Epoch 633: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.2402e-04 - accuracy: 1.0000 - val_loss: 4.6548 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 634/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 634: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.7377 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 635/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000    \n",
            "Epoch 635: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.5746 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 636/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.8409e-04 - accuracy: 1.0000\n",
            "Epoch 636: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.8890e-04 - accuracy: 1.0000 - val_loss: 4.7385 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 637/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 637: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.8428 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 638/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.1713e-04 - accuracy: 1.0000\n",
            "Epoch 638: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.5316e-04 - accuracy: 1.0000 - val_loss: 4.7818 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 639/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 639: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.8217 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 640/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.4749e-04 - accuracy: 1.0000\n",
            "Epoch 640: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.2463e-04 - accuracy: 1.0000 - val_loss: 5.0101 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 641/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.3996e-04 - accuracy: 1.0000\n",
            "Epoch 641: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 8.8211e-04 - accuracy: 1.0000 - val_loss: 5.0955 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 642/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 642: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 9.7107e-04 - accuracy: 1.0000 - val_loss: 4.8858 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 643/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.1812e-04 - accuracy: 1.0000\n",
            "Epoch 643: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.9615e-04 - accuracy: 1.0000 - val_loss: 5.0592 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 644/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.5753e-04 - accuracy: 1.0000\n",
            "Epoch 644: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 6.5753e-04 - accuracy: 1.0000 - val_loss: 5.0344 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 645/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 645: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.8797 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 646/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.3368e-04 - accuracy: 1.0000\n",
            "Epoch 646: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 7.8035e-04 - accuracy: 1.0000 - val_loss: 4.9169 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 647/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.3339e-04 - accuracy: 1.0000\n",
            "Epoch 647: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 2.3339e-04 - accuracy: 1.0000 - val_loss: 4.8682 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 648/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 648: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.8345 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 649/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.0209e-04 - accuracy: 1.0000\n",
            "Epoch 649: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.3915e-04 - accuracy: 1.0000 - val_loss: 4.8251 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 650/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 650: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.7081 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 651/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000    \n",
            "Epoch 651: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.7423 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 652/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 652: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.7228 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 653/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 653: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.7947 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 654/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.8222e-04 - accuracy: 1.0000\n",
            "Epoch 654: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 8.0279e-04 - accuracy: 1.0000 - val_loss: 4.8669 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 655/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.3055e-04 - accuracy: 1.0000\n",
            "Epoch 655: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 8.3055e-04 - accuracy: 1.0000 - val_loss: 4.9021 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 656/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.9777e-04 - accuracy: 1.0000\n",
            "Epoch 656: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.9138e-04 - accuracy: 1.0000 - val_loss: 4.8629 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 657/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.7314e-04 - accuracy: 1.0000\n",
            "Epoch 657: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 8.7314e-04 - accuracy: 1.0000 - val_loss: 5.0913 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 658/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 658: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 9.4366e-04 - accuracy: 1.0000 - val_loss: 5.1661 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 659/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.4272e-04 - accuracy: 1.0000\n",
            "Epoch 659: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 6.4272e-04 - accuracy: 1.0000 - val_loss: 5.1212 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 660/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000    \n",
            "Epoch 660: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.9799 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 661/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.4365e-04 - accuracy: 1.0000\n",
            "Epoch 661: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 6.8691e-04 - accuracy: 1.0000 - val_loss: 5.0268 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 662/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.9962e-04 - accuracy: 1.0000\n",
            "Epoch 662: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 8.0622e-04 - accuracy: 1.0000 - val_loss: 4.8327 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 663/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4293e-04 - accuracy: 1.0000\n",
            "Epoch 663: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.0139e-04 - accuracy: 1.0000 - val_loss: 4.9644 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 664/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.4819e-04 - accuracy: 1.0000\n",
            "Epoch 664: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 4.6497e-04 - accuracy: 1.0000 - val_loss: 5.1361 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 665/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.0906e-04 - accuracy: 1.0000\n",
            "Epoch 665: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 3.0906e-04 - accuracy: 1.0000 - val_loss: 4.8590 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 666/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.9004e-04 - accuracy: 1.0000\n",
            "Epoch 666: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.0407e-04 - accuracy: 1.0000 - val_loss: 4.8873 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 667/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.2868e-04 - accuracy: 1.0000\n",
            "Epoch 667: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 5.7564e-04 - accuracy: 1.0000 - val_loss: 4.8724 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 668/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.5281e-04 - accuracy: 1.0000\n",
            "Epoch 668: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.4306e-04 - accuracy: 1.0000 - val_loss: 4.6283 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 669/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.9340e-04 - accuracy: 1.0000\n",
            "Epoch 669: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 8.9054e-04 - accuracy: 1.0000 - val_loss: 4.6788 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 670/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 670: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.4387 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 671/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.6430e-04 - accuracy: 1.0000\n",
            "Epoch 671: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 5.3560e-04 - accuracy: 1.0000 - val_loss: 4.5111 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 672/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.1102e-04 - accuracy: 1.0000\n",
            "Epoch 672: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 4.0021e-04 - accuracy: 1.0000 - val_loss: 4.5269 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 673/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.6519e-04 - accuracy: 1.0000\n",
            "Epoch 673: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 5.1432e-04 - accuracy: 1.0000 - val_loss: 4.5460 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 674/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4856e-04 - accuracy: 1.0000\n",
            "Epoch 674: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.6043e-04 - accuracy: 1.0000 - val_loss: 4.5765 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 675/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5961e-04 - accuracy: 1.0000\n",
            "Epoch 675: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.2775e-04 - accuracy: 1.0000 - val_loss: 4.8297 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 676/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.3720e-04 - accuracy: 1.0000\n",
            "Epoch 676: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 2.6564e-04 - accuracy: 1.0000 - val_loss: 4.7692 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 677/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.5240e-04 - accuracy: 1.0000\n",
            "Epoch 677: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 5.5240e-04 - accuracy: 1.0000 - val_loss: 4.9743 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 678/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 8.5745e-04 - accuracy: 1.0000\n",
            "Epoch 678: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 8.5745e-04 - accuracy: 1.0000 - val_loss: 4.9469 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 679/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.9299e-04 - accuracy: 1.0000\n",
            "Epoch 679: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.6943e-04 - accuracy: 1.0000 - val_loss: 4.9979 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 680/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 680: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.8484 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 681/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.8322e-04 - accuracy: 1.0000\n",
            "Epoch 681: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 5.0276e-04 - accuracy: 1.0000 - val_loss: 4.8183 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 682/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.3456e-04 - accuracy: 1.0000\n",
            "Epoch 682: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 5.3456e-04 - accuracy: 1.0000 - val_loss: 4.9295 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 683/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 683: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.9345 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 684/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.6536e-04 - accuracy: 1.0000\n",
            "Epoch 684: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 7.6536e-04 - accuracy: 1.0000 - val_loss: 5.1015 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 685/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.1670e-04 - accuracy: 1.0000\n",
            "Epoch 685: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 7.3562e-04 - accuracy: 1.0000 - val_loss: 5.0237 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 686/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.5947e-04 - accuracy: 1.0000\n",
            "Epoch 686: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 7.0076e-04 - accuracy: 1.0000 - val_loss: 5.1811 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 687/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.7807e-04 - accuracy: 1.0000\n",
            "Epoch 687: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 5.7807e-04 - accuracy: 1.0000 - val_loss: 5.1567 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 688/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 688: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 4.9778 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 689/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.4046e-04 - accuracy: 1.0000\n",
            "Epoch 689: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 5.9342e-04 - accuracy: 1.0000 - val_loss: 5.0247 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 690/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 690: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.8485 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 691/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.5638e-04 - accuracy: 1.0000\n",
            "Epoch 691: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 5.2822e-04 - accuracy: 1.0000 - val_loss: 4.7525 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 692/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 692: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.7745 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 693/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 693: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.8728 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 694/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 694: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.8557 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 695/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.6169e-04 - accuracy: 1.0000\n",
            "Epoch 695: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.7168 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 696/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.3691e-04 - accuracy: 1.0000\n",
            "Epoch 696: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 7.3691e-04 - accuracy: 1.0000 - val_loss: 4.7220 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 697/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.9344e-04 - accuracy: 1.0000\n",
            "Epoch 697: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.2897e-04 - accuracy: 1.0000 - val_loss: 4.8500 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 698/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5357e-04 - accuracy: 1.0000\n",
            "Epoch 698: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.7764e-04 - accuracy: 1.0000 - val_loss: 4.6688 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 699/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.0421e-04 - accuracy: 1.0000\n",
            "Epoch 699: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.7959e-04 - accuracy: 1.0000 - val_loss: 4.5842 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 700/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.6652e-04 - accuracy: 1.0000\n",
            "Epoch 700: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 7.1976e-04 - accuracy: 1.0000 - val_loss: 4.8378 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 701/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.4815e-04 - accuracy: 1.0000\n",
            "Epoch 701: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 5.4815e-04 - accuracy: 1.0000 - val_loss: 4.7155 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 702/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.1323e-04 - accuracy: 1.0000\n",
            "Epoch 702: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 7.4820e-04 - accuracy: 1.0000 - val_loss: 4.6029 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 703/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 9.2032e-04 - accuracy: 1.0000\n",
            "Epoch 703: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 9.2032e-04 - accuracy: 1.0000 - val_loss: 4.7125 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 704/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 704: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 9.1705e-04 - accuracy: 1.0000 - val_loss: 4.8400 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 705/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3181e-04 - accuracy: 1.0000\n",
            "Epoch 705: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.5909e-04 - accuracy: 1.0000 - val_loss: 4.9412 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 706/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.3782e-04 - accuracy: 1.0000\n",
            "Epoch 706: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 6.3782e-04 - accuracy: 1.0000 - val_loss: 4.5823 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 707/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.2609e-04 - accuracy: 1.0000\n",
            "Epoch 707: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 8.3452e-04 - accuracy: 1.0000 - val_loss: 4.7212 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 708/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1094e-04 - accuracy: 1.0000\n",
            "Epoch 708: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 2.8895e-04 - accuracy: 1.0000 - val_loss: 4.9254 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 709/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 709: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.9137 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 710/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.8968e-04 - accuracy: 1.0000\n",
            "Epoch 710: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 9.1111e-04 - accuracy: 1.0000 - val_loss: 4.9040 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 711/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 711: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.7586 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 712/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.5549e-04 - accuracy: 1.0000\n",
            "Epoch 712: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 2.5549e-04 - accuracy: 1.0000 - val_loss: 4.5954 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 713/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.5069e-04 - accuracy: 1.0000\n",
            "Epoch 713: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 7.8492e-04 - accuracy: 1.0000 - val_loss: 4.8445 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 714/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5814e-04 - accuracy: 1.0000\n",
            "Epoch 714: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.3658e-04 - accuracy: 1.0000 - val_loss: 4.7791 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 715/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.9818e-04 - accuracy: 1.0000\n",
            "Epoch 715: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.8985e-04 - accuracy: 1.0000 - val_loss: 4.8985 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 716/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 716: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 9.7714e-04 - accuracy: 1.0000 - val_loss: 4.6943 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 717/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.7992e-04 - accuracy: 1.0000\n",
            "Epoch 717: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.7086e-04 - accuracy: 1.0000 - val_loss: 4.5958 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 718/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.1586e-04 - accuracy: 1.0000\n",
            "Epoch 718: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 6.1586e-04 - accuracy: 1.0000 - val_loss: 4.5734 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 719/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 719: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.5683 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 720/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.7728e-04 - accuracy: 1.0000\n",
            "Epoch 720: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 8.8913e-04 - accuracy: 1.0000 - val_loss: 4.5829 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 721/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.4427e-04 - accuracy: 1.0000\n",
            "Epoch 721: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 4.9386e-04 - accuracy: 1.0000 - val_loss: 4.8905 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 722/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.0427e-04 - accuracy: 1.0000\n",
            "Epoch 722: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.0496e-04 - accuracy: 1.0000 - val_loss: 4.5756 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 723/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.4184e-04 - accuracy: 1.0000\n",
            "Epoch 723: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 6.7352e-04 - accuracy: 1.0000 - val_loss: 4.8341 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 724/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.9815e-04 - accuracy: 1.0000\n",
            "Epoch 724: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 5.4229e-04 - accuracy: 1.0000 - val_loss: 4.8532 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 725/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.3770e-04 - accuracy: 1.0000\n",
            "Epoch 725: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.3524e-04 - accuracy: 1.0000 - val_loss: 4.9878 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 726/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5836e-04 - accuracy: 1.0000\n",
            "Epoch 726: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.4786e-04 - accuracy: 1.0000 - val_loss: 4.9218 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 727/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.9814e-04 - accuracy: 1.0000\n",
            "Epoch 727: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.8951e-04 - accuracy: 1.0000 - val_loss: 4.9885 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 728/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.7304e-04 - accuracy: 1.0000\n",
            "Epoch 728: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 6.7304e-04 - accuracy: 1.0000 - val_loss: 5.1328 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 729/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.7824e-04 - accuracy: 1.0000\n",
            "Epoch 729: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 6.9657e-04 - accuracy: 1.0000 - val_loss: 5.0794 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 730/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.1568e-04 - accuracy: 1.0000\n",
            "Epoch 730: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 4.1568e-04 - accuracy: 1.0000 - val_loss: 4.9887 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 731/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2004e-04 - accuracy: 1.0000\n",
            "Epoch 731: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.5180e-04 - accuracy: 1.0000 - val_loss: 5.0317 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 732/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.1836e-04 - accuracy: 1.0000\n",
            "Epoch 732: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 4.1836e-04 - accuracy: 1.0000 - val_loss: 4.8150 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 733/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.5491e-04 - accuracy: 1.0000\n",
            "Epoch 733: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 4.5491e-04 - accuracy: 1.0000 - val_loss: 4.7923 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 734/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4828e-04 - accuracy: 1.0000\n",
            "Epoch 734: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 3.6206e-04 - accuracy: 1.0000 - val_loss: 4.8732 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 735/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.3178e-04 - accuracy: 1.0000\n",
            "Epoch 735: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 7.1418e-04 - accuracy: 1.0000 - val_loss: 5.0737 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 736/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.3516e-04 - accuracy: 1.0000\n",
            "Epoch 736: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 3.3516e-04 - accuracy: 1.0000 - val_loss: 4.9732 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 737/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.8234e-04 - accuracy: 1.0000\n",
            "Epoch 737: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 5.8234e-04 - accuracy: 1.0000 - val_loss: 4.9858 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 738/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.8888e-04 - accuracy: 1.0000\n",
            "Epoch 738: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 4.8888e-04 - accuracy: 1.0000 - val_loss: 4.9750 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 739/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.0232e-04 - accuracy: 1.0000\n",
            "Epoch 739: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 4.0232e-04 - accuracy: 1.0000 - val_loss: 5.0480 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 740/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 740: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 5.1004 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 741/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.1786e-04 - accuracy: 1.0000\n",
            "Epoch 741: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 93ms/step - loss: 5.1786e-04 - accuracy: 1.0000 - val_loss: 5.0784 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 742/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.2254e-04 - accuracy: 1.0000\n",
            "Epoch 742: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 8.3613e-04 - accuracy: 1.0000 - val_loss: 5.2495 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 743/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 743: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 79ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 5.0868 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 744/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.6293e-04 - accuracy: 1.0000\n",
            "Epoch 744: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 5.6293e-04 - accuracy: 1.0000 - val_loss: 5.1090 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 745/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.5515e-04 - accuracy: 1.0000\n",
            "Epoch 745: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 4.3741e-04 - accuracy: 1.0000 - val_loss: 5.0679 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 746/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.2012e-04 - accuracy: 1.0000\n",
            "Epoch 746: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 4.2012e-04 - accuracy: 1.0000 - val_loss: 5.0712 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 747/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.8865e-04 - accuracy: 1.0000\n",
            "Epoch 747: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.9600e-04 - accuracy: 1.0000 - val_loss: 5.1238 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 748/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6291e-04 - accuracy: 1.0000\n",
            "Epoch 748: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.8148e-04 - accuracy: 1.0000 - val_loss: 4.7908 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 749/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.8322e-04 - accuracy: 1.0000\n",
            "Epoch 749: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 4.3342e-04 - accuracy: 1.0000 - val_loss: 4.7822 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 750/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7604e-04 - accuracy: 1.0000\n",
            "Epoch 750: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.3839e-04 - accuracy: 1.0000 - val_loss: 4.9017 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 751/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8003e-04 - accuracy: 1.0000\n",
            "Epoch 751: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 2.6146e-04 - accuracy: 1.0000 - val_loss: 5.0658 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 752/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.8527e-04 - accuracy: 1.0000\n",
            "Epoch 752: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 7.9794e-04 - accuracy: 1.0000 - val_loss: 5.1147 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 753/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6804e-04 - accuracy: 1.0000\n",
            "Epoch 753: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.7530e-04 - accuracy: 1.0000 - val_loss: 5.0580 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 754/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.0260e-04 - accuracy: 1.0000\n",
            "Epoch 754: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 5.0260e-04 - accuracy: 1.0000 - val_loss: 5.0585 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 755/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.4304e-04 - accuracy: 1.0000\n",
            "Epoch 755: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 4.4304e-04 - accuracy: 1.0000 - val_loss: 4.9740 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 756/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8051e-04 - accuracy: 1.0000\n",
            "Epoch 756: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.4335e-04 - accuracy: 1.0000 - val_loss: 4.8980 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 757/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.6115e-04 - accuracy: 1.0000\n",
            "Epoch 757: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 3.6115e-04 - accuracy: 1.0000 - val_loss: 4.9117 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 758/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.8479e-04 - accuracy: 1.0000\n",
            "Epoch 758: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 6.8479e-04 - accuracy: 1.0000 - val_loss: 4.9608 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 759/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.3274e-04 - accuracy: 1.0000\n",
            "Epoch 759: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 5.3274e-04 - accuracy: 1.0000 - val_loss: 4.6661 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 760/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.3641e-04 - accuracy: 1.0000\n",
            "Epoch 760: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 4.9522e-04 - accuracy: 1.0000 - val_loss: 4.4671 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 761/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4080e-04 - accuracy: 1.0000\n",
            "Epoch 761: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 4.2642e-04 - accuracy: 1.0000 - val_loss: 4.6450 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 762/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 762: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.6592 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 763/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1723e-04 - accuracy: 1.0000\n",
            "Epoch 763: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 4.9258e-04 - accuracy: 1.0000 - val_loss: 4.6343 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 764/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.1277e-04 - accuracy: 1.0000\n",
            "Epoch 764: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 3.1277e-04 - accuracy: 1.0000 - val_loss: 4.7770 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 765/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8267e-04 - accuracy: 1.0000\n",
            "Epoch 765: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.8573e-04 - accuracy: 1.0000 - val_loss: 4.5672 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 766/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.7598e-04 - accuracy: 1.0000\n",
            "Epoch 766: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 2.5762e-04 - accuracy: 1.0000 - val_loss: 4.5648 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 767/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.9847e-04 - accuracy: 1.0000\n",
            "Epoch 767: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 5.9847e-04 - accuracy: 1.0000 - val_loss: 4.5597 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 768/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.9122e-04 - accuracy: 1.0000\n",
            "Epoch 768: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 6.1894e-04 - accuracy: 1.0000 - val_loss: 4.5715 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 769/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.7839e-04 - accuracy: 1.0000\n",
            "Epoch 769: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 7.8520e-04 - accuracy: 1.0000 - val_loss: 4.6487 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 770/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.1761e-04 - accuracy: 1.0000\n",
            "Epoch 770: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 6.1761e-04 - accuracy: 1.0000 - val_loss: 4.5707 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 771/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.0631e-04 - accuracy: 1.0000\n",
            "Epoch 771: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.7473e-04 - accuracy: 1.0000 - val_loss: 4.4253 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 772/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 772: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.6600 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 773/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1467e-04 - accuracy: 1.0000\n",
            "Epoch 773: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.1169e-04 - accuracy: 1.0000 - val_loss: 4.7138 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 774/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1184e-04 - accuracy: 1.0000\n",
            "Epoch 774: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.7428e-04 - accuracy: 1.0000 - val_loss: 4.8087 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 775/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.9444e-04 - accuracy: 1.0000\n",
            "Epoch 775: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 3.9444e-04 - accuracy: 1.0000 - val_loss: 4.8535 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 776/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.2872e-04 - accuracy: 1.0000\n",
            "Epoch 776: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 4.0163e-04 - accuracy: 1.0000 - val_loss: 4.7771 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 777/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 777: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6855 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 778/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 778: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.8438 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 779/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6398e-04 - accuracy: 1.0000\n",
            "Epoch 779: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 4.6920e-04 - accuracy: 1.0000 - val_loss: 4.6245 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 780/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3760e-04 - accuracy: 1.0000\n",
            "Epoch 780: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 4.3296e-04 - accuracy: 1.0000 - val_loss: 4.4570 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 781/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.2257e-04 - accuracy: 1.0000\n",
            "Epoch 781: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.0742e-04 - accuracy: 1.0000 - val_loss: 4.4570 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 782/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.3418e-04 - accuracy: 1.0000\n",
            "Epoch 782: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 7.5259e-04 - accuracy: 1.0000 - val_loss: 4.6298 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 783/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.5119e-04 - accuracy: 1.0000\n",
            "Epoch 783: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 4.5119e-04 - accuracy: 1.0000 - val_loss: 4.4957 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 784/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5783e-04 - accuracy: 1.0000\n",
            "Epoch 784: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.8691e-04 - accuracy: 1.0000 - val_loss: 4.4032 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 785/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9781e-04 - accuracy: 1.0000\n",
            "Epoch 785: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 2.0110e-04 - accuracy: 1.0000 - val_loss: 4.4162 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 786/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.9023e-04 - accuracy: 1.0000\n",
            "Epoch 786: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 2.9023e-04 - accuracy: 1.0000 - val_loss: 4.3578 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 787/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8249e-04 - accuracy: 1.0000\n",
            "Epoch 787: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 2.5908e-04 - accuracy: 1.0000 - val_loss: 4.3125 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 788/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.9722e-04 - accuracy: 1.0000\n",
            "Epoch 788: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 6.2678e-04 - accuracy: 1.0000 - val_loss: 4.4677 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 789/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.4831e-04 - accuracy: 1.0000\n",
            "Epoch 789: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 4.4831e-04 - accuracy: 1.0000 - val_loss: 4.4615 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 790/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3400e-04 - accuracy: 1.0000\n",
            "Epoch 790: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.0323e-04 - accuracy: 1.0000 - val_loss: 4.5299 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 791/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.2186e-04 - accuracy: 1.0000\n",
            "Epoch 791: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 4.2186e-04 - accuracy: 1.0000 - val_loss: 4.4469 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 792/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
            "Epoch 792: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.3753 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 793/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.6263e-04 - accuracy: 1.0000\n",
            "Epoch 793: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 5.9422e-04 - accuracy: 1.0000 - val_loss: 4.6537 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 794/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.9720e-04 - accuracy: 1.0000\n",
            "Epoch 794: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.6179e-04 - accuracy: 1.0000 - val_loss: 4.7561 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 795/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.2467e-04 - accuracy: 1.0000\n",
            "Epoch 795: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 2.2467e-04 - accuracy: 1.0000 - val_loss: 4.8114 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 796/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.3556e-04 - accuracy: 1.0000\n",
            "Epoch 796: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 7.5560e-04 - accuracy: 1.0000 - val_loss: 4.9295 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 797/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.4754e-04 - accuracy: 1.0000\n",
            "Epoch 797: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 8.5862e-04 - accuracy: 1.0000 - val_loss: 4.7394 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 798/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 798: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 9.7971e-04 - accuracy: 1.0000 - val_loss: 4.9405 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 799/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 799: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 4.8369 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 800/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.0480e-04 - accuracy: 1.0000\n",
            "Epoch 800: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 5.0480e-04 - accuracy: 1.0000 - val_loss: 4.9219 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 801/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9576e-04 - accuracy: 1.0000\n",
            "Epoch 801: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.7805e-04 - accuracy: 1.0000 - val_loss: 4.8679 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 802/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.7819e-04 - accuracy: 1.0000\n",
            "Epoch 802: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 1.7788e-04 - accuracy: 1.0000 - val_loss: 4.6499 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 803/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6925e-04 - accuracy: 1.0000\n",
            "Epoch 803: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 2.4438e-04 - accuracy: 1.0000 - val_loss: 4.8431 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 804/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.6506e-04 - accuracy: 1.0000\n",
            "Epoch 804: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 8.0573e-04 - accuracy: 1.0000 - val_loss: 4.7219 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 805/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.4219e-04 - accuracy: 1.0000\n",
            "Epoch 805: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.1813e-04 - accuracy: 1.0000 - val_loss: 4.7363 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 806/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.4079e-04 - accuracy: 1.0000\n",
            "Epoch 806: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.1654e-04 - accuracy: 1.0000 - val_loss: 4.7484 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 807/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.0421e-04 - accuracy: 1.0000\n",
            "Epoch 807: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 8.6735e-04 - accuracy: 1.0000 - val_loss: 4.5680 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 808/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6118e-04 - accuracy: 1.0000\n",
            "Epoch 808: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.1795e-04 - accuracy: 1.0000 - val_loss: 4.7807 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 809/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 809: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 9.3463e-04 - accuracy: 1.0000 - val_loss: 4.8059 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 810/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.2367e-04 - accuracy: 1.0000\n",
            "Epoch 810: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 2.2367e-04 - accuracy: 1.0000 - val_loss: 4.7851 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 811/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4867e-04 - accuracy: 1.0000\n",
            "Epoch 811: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.2438e-04 - accuracy: 1.0000 - val_loss: 4.6993 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 812/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.0091e-04 - accuracy: 1.0000\n",
            "Epoch 812: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.9161e-04 - accuracy: 1.0000 - val_loss: 4.8482 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 813/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.6836e-04 - accuracy: 1.0000\n",
            "Epoch 813: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 9.6234e-04 - accuracy: 1.0000 - val_loss: 4.8477 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 814/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.2267e-04 - accuracy: 1.0000\n",
            "Epoch 814: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 3.2267e-04 - accuracy: 1.0000 - val_loss: 4.9079 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 815/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 815: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.9048 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 816/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.8280e-04 - accuracy: 1.0000\n",
            "Epoch 816: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 4.8280e-04 - accuracy: 1.0000 - val_loss: 4.9297 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 817/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0307e-04 - accuracy: 1.0000\n",
            "Epoch 817: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.0517e-04 - accuracy: 1.0000 - val_loss: 4.9983 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 818/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.4364e-04 - accuracy: 1.0000\n",
            "Epoch 818: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 2.4364e-04 - accuracy: 1.0000 - val_loss: 4.9735 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 819/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.7190e-04 - accuracy: 1.0000\n",
            "Epoch 819: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 7.7190e-04 - accuracy: 1.0000 - val_loss: 4.9639 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 820/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.0863e-04 - accuracy: 1.0000\n",
            "Epoch 820: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 2.0863e-04 - accuracy: 1.0000 - val_loss: 4.9289 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 821/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 821: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.9577 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 822/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.6969e-04 - accuracy: 1.0000\n",
            "Epoch 822: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 4.6969e-04 - accuracy: 1.0000 - val_loss: 4.9169 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 823/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7513e-04 - accuracy: 1.0000\n",
            "Epoch 823: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.5901e-04 - accuracy: 1.0000 - val_loss: 4.8757 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 824/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.2910e-04 - accuracy: 1.0000\n",
            "Epoch 824: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 5.6902e-04 - accuracy: 1.0000 - val_loss: 4.8750 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 825/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.0567e-04 - accuracy: 1.0000\n",
            "Epoch 825: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.7889e-04 - accuracy: 1.0000 - val_loss: 4.9542 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 826/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000    \n",
            "Epoch 826: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 9.2779e-04 - accuracy: 1.0000 - val_loss: 4.8528 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 827/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5633e-04 - accuracy: 1.0000\n",
            "Epoch 827: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 3.3169e-04 - accuracy: 1.0000 - val_loss: 4.9203 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 828/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.7343e-04 - accuracy: 1.0000\n",
            "Epoch 828: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 3.7343e-04 - accuracy: 1.0000 - val_loss: 4.9239 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 829/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6250e-04 - accuracy: 1.0000\n",
            "Epoch 829: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 4.2390e-04 - accuracy: 1.0000 - val_loss: 4.8636 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 830/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.5418e-04 - accuracy: 1.0000\n",
            "Epoch 830: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 5.3555e-04 - accuracy: 1.0000 - val_loss: 4.8438 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 831/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.3959e-04 - accuracy: 1.0000\n",
            "Epoch 831: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 3.3959e-04 - accuracy: 1.0000 - val_loss: 4.7218 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 832/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0609e-04 - accuracy: 1.0000\n",
            "Epoch 832: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 2.0321e-04 - accuracy: 1.0000 - val_loss: 4.6634 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 833/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.0575e-04 - accuracy: 1.0000\n",
            "Epoch 833: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 2.8565e-04 - accuracy: 1.0000 - val_loss: 4.6378 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 834/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.8106e-04 - accuracy: 1.0000\n",
            "Epoch 834: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.7159e-04 - accuracy: 1.0000 - val_loss: 4.6014 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 835/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3099e-04 - accuracy: 1.0000\n",
            "Epoch 835: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.3851e-04 - accuracy: 1.0000 - val_loss: 4.8836 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 836/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.7116e-04 - accuracy: 1.0000\n",
            "Epoch 836: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.7116e-04 - accuracy: 1.0000 - val_loss: 4.7445 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 837/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.2460e-04 - accuracy: 1.0000\n",
            "Epoch 837: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 2.2460e-04 - accuracy: 1.0000 - val_loss: 4.7295 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 838/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.0700e-04 - accuracy: 1.0000\n",
            "Epoch 838: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 3.0700e-04 - accuracy: 1.0000 - val_loss: 4.5376 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 839/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.7043e-04 - accuracy: 1.0000\n",
            "Epoch 839: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.2663e-04 - accuracy: 1.0000 - val_loss: 4.5098 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 840/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.6033e-04 - accuracy: 1.0000\n",
            "Epoch 840: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 4.6033e-04 - accuracy: 1.0000 - val_loss: 4.5503 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 841/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9351e-04 - accuracy: 1.0000\n",
            "Epoch 841: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 1.8582e-04 - accuracy: 1.0000 - val_loss: 4.6323 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 842/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 842: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.4913 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 843/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.7010e-04 - accuracy: 1.0000\n",
            "Epoch 843: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 5.3413e-04 - accuracy: 1.0000 - val_loss: 4.4829 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 844/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.3530e-04 - accuracy: 1.0000\n",
            "Epoch 844: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 3.3530e-04 - accuracy: 1.0000 - val_loss: 4.5911 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 845/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.5501e-04 - accuracy: 1.0000\n",
            "Epoch 845: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 2.2979e-04 - accuracy: 1.0000 - val_loss: 4.7449 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 846/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3569e-04 - accuracy: 1.0000\n",
            "Epoch 846: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.5268 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 847/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.9366e-04 - accuracy: 1.0000\n",
            "Epoch 847: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 4.9366e-04 - accuracy: 1.0000 - val_loss: 4.5330 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 848/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2234e-04 - accuracy: 1.0000\n",
            "Epoch 848: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.8975e-04 - accuracy: 1.0000 - val_loss: 4.4946 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 849/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.8228e-04 - accuracy: 1.0000\n",
            "Epoch 849: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 1.7343e-04 - accuracy: 1.0000 - val_loss: 4.5280 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 850/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.5164e-04 - accuracy: 1.0000\n",
            "Epoch 850: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 3.5164e-04 - accuracy: 1.0000 - val_loss: 4.4299 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 851/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.5763e-04 - accuracy: 1.0000\n",
            "Epoch 851: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 91ms/step - loss: 1.5763e-04 - accuracy: 1.0000 - val_loss: 4.4149 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 852/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.3710e-04 - accuracy: 1.0000\n",
            "Epoch 852: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 7.3710e-04 - accuracy: 1.0000 - val_loss: 4.6052 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 853/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.7746e-04 - accuracy: 1.0000\n",
            "Epoch 853: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 1.7746e-04 - accuracy: 1.0000 - val_loss: 4.6253 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 854/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000    \n",
            "Epoch 854: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.6056 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 855/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.4026e-04 - accuracy: 1.0000\n",
            "Epoch 855: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 1.4026e-04 - accuracy: 1.0000 - val_loss: 4.5615 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 856/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.8611e-04 - accuracy: 1.0000\n",
            "Epoch 856: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 4.8611e-04 - accuracy: 1.0000 - val_loss: 4.5730 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 857/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.8404e-04 - accuracy: 1.0000\n",
            "Epoch 857: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 3.8404e-04 - accuracy: 1.0000 - val_loss: 4.4081 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 858/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.3683e-04 - accuracy: 1.0000\n",
            "Epoch 858: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 4.3683e-04 - accuracy: 1.0000 - val_loss: 4.4106 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 859/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.2559e-04 - accuracy: 1.0000\n",
            "Epoch 859: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 6.2559e-04 - accuracy: 1.0000 - val_loss: 4.6113 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 860/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.9608e-04 - accuracy: 1.0000\n",
            "Epoch 860: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 6.9608e-04 - accuracy: 1.0000 - val_loss: 4.7542 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 861/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.0670e-04 - accuracy: 1.0000\n",
            "Epoch 861: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 5.0670e-04 - accuracy: 1.0000 - val_loss: 4.7967 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 862/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6629e-04 - accuracy: 1.0000\n",
            "Epoch 862: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 2.5587e-04 - accuracy: 1.0000 - val_loss: 4.7811 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 863/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.4104e-04 - accuracy: 1.0000\n",
            "Epoch 863: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 3.0610e-04 - accuracy: 1.0000 - val_loss: 4.4878 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 864/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.9755e-04 - accuracy: 1.0000\n",
            "Epoch 864: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 2.9755e-04 - accuracy: 1.0000 - val_loss: 4.6826 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 865/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.7665e-05 - accuracy: 1.0000\n",
            "Epoch 865: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.0481e-04 - accuracy: 1.0000 - val_loss: 4.5221 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 866/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2589e-04 - accuracy: 1.0000\n",
            "Epoch 866: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 3.2215e-04 - accuracy: 1.0000 - val_loss: 4.4789 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 867/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000    \n",
            "Epoch 867: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.4351 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 868/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.7380e-04 - accuracy: 1.0000\n",
            "Epoch 868: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 1.7380e-04 - accuracy: 1.0000 - val_loss: 4.4468 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 869/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.7096e-04 - accuracy: 1.0000\n",
            "Epoch 869: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 2.7096e-04 - accuracy: 1.0000 - val_loss: 4.4378 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 870/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.5484e-04 - accuracy: 1.0000\n",
            "Epoch 870: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 4.0634e-04 - accuracy: 1.0000 - val_loss: 4.4759 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 871/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.2836e-04 - accuracy: 1.0000\n",
            "Epoch 871: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.9557e-04 - accuracy: 1.0000 - val_loss: 4.3640 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 872/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.5935e-04 - accuracy: 1.0000\n",
            "Epoch 872: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 2.5935e-04 - accuracy: 1.0000 - val_loss: 4.3909 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 873/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.5385e-04 - accuracy: 1.0000\n",
            "Epoch 873: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 6.5385e-04 - accuracy: 1.0000 - val_loss: 4.4099 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 874/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.8622e-04 - accuracy: 1.0000\n",
            "Epoch 874: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.5245e-04 - accuracy: 1.0000 - val_loss: 4.3477 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 875/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6681e-04 - accuracy: 1.0000\n",
            "Epoch 875: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.4359e-04 - accuracy: 1.0000 - val_loss: 4.3692 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 876/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.6700e-04 - accuracy: 1.0000\n",
            "Epoch 876: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.2788e-04 - accuracy: 1.0000 - val_loss: 4.3243 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 877/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.3304e-04 - accuracy: 1.0000\n",
            "Epoch 877: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 2.1624e-04 - accuracy: 1.0000 - val_loss: 4.3474 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 878/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7481e-04 - accuracy: 1.0000\n",
            "Epoch 878: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.5686e-04 - accuracy: 1.0000 - val_loss: 4.3621 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 879/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.2439e-04 - accuracy: 1.0000\n",
            "Epoch 879: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 7.3715e-04 - accuracy: 1.0000 - val_loss: 4.5602 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 880/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.4029e-04 - accuracy: 1.0000\n",
            "Epoch 880: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 4.4029e-04 - accuracy: 1.0000 - val_loss: 4.5608 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 881/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.4532e-04 - accuracy: 1.0000\n",
            "Epoch 881: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 1.4532e-04 - accuracy: 1.0000 - val_loss: 4.5834 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 882/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.0519e-04 - accuracy: 1.0000\n",
            "Epoch 882: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 1.1896e-04 - accuracy: 1.0000 - val_loss: 4.6989 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 883/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.9328e-04 - accuracy: 1.0000\n",
            "Epoch 883: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 2.9328e-04 - accuracy: 1.0000 - val_loss: 4.7366 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 884/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.9957e-04 - accuracy: 1.0000\n",
            "Epoch 884: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 1.9957e-04 - accuracy: 1.0000 - val_loss: 4.7279 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 885/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.7185e-04 - accuracy: 1.0000\n",
            "Epoch 885: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.5116e-04 - accuracy: 1.0000 - val_loss: 4.6886 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 886/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8749e-04 - accuracy: 1.0000\n",
            "Epoch 886: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.3335e-04 - accuracy: 1.0000 - val_loss: 4.4489 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 887/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.6095e-04 - accuracy: 1.0000\n",
            "Epoch 887: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 1.6095e-04 - accuracy: 1.0000 - val_loss: 4.4443 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 888/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2276e-04 - accuracy: 1.0000\n",
            "Epoch 888: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.5551e-04 - accuracy: 1.0000 - val_loss: 4.2815 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 889/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.2060e-04 - accuracy: 1.0000\n",
            "Epoch 889: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 5.5662e-04 - accuracy: 1.0000 - val_loss: 4.2471 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 890/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.0017e-04 - accuracy: 1.0000\n",
            "Epoch 890: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 7.5777e-04 - accuracy: 1.0000 - val_loss: 4.2737 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 891/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.6783e-04 - accuracy: 1.0000\n",
            "Epoch 891: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 5.5934e-04 - accuracy: 1.0000 - val_loss: 4.2296 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 892/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.6528e-04 - accuracy: 1.0000\n",
            "Epoch 892: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 1.5555e-04 - accuracy: 1.0000 - val_loss: 4.0786 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 893/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3727e-04 - accuracy: 1.0000\n",
            "Epoch 893: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.0800e-04 - accuracy: 1.0000 - val_loss: 4.2386 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 894/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.7168e-04 - accuracy: 1.0000\n",
            "Epoch 894: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 4.2937e-04 - accuracy: 1.0000 - val_loss: 4.2598 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 895/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8071e-04 - accuracy: 1.0000\n",
            "Epoch 895: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.5544e-04 - accuracy: 1.0000 - val_loss: 4.2962 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 896/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 6.1586e-04 - accuracy: 1.0000\n",
            "Epoch 896: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 6.1586e-04 - accuracy: 1.0000 - val_loss: 4.1579 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 897/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.8417e-04 - accuracy: 1.0000\n",
            "Epoch 897: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 2.8417e-04 - accuracy: 1.0000 - val_loss: 4.1529 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 898/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3321e-04 - accuracy: 1.0000\n",
            "Epoch 898: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.4854e-04 - accuracy: 1.0000 - val_loss: 4.1637 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 899/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.9984e-04 - accuracy: 1.0000\n",
            "Epoch 899: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.9073e-04 - accuracy: 1.0000 - val_loss: 4.1452 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 900/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.3309e-04 - accuracy: 1.0000\n",
            "Epoch 900: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.2924e-04 - accuracy: 1.0000 - val_loss: 4.0839 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 901/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.9400e-04 - accuracy: 1.0000\n",
            "Epoch 901: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 2.9400e-04 - accuracy: 1.0000 - val_loss: 4.1291 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 902/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8802e-04 - accuracy: 1.0000\n",
            "Epoch 902: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.5902e-04 - accuracy: 1.0000 - val_loss: 4.1183 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 903/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.5377e-04 - accuracy: 1.0000\n",
            "Epoch 903: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 5.5377e-04 - accuracy: 1.0000 - val_loss: 4.2939 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 904/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.7556e-04 - accuracy: 1.0000\n",
            "Epoch 904: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 5.7556e-04 - accuracy: 1.0000 - val_loss: 4.3190 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 905/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.6016e-04 - accuracy: 1.0000\n",
            "Epoch 905: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 4.1518e-04 - accuracy: 1.0000 - val_loss: 4.3154 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 906/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.6034e-04 - accuracy: 1.0000\n",
            "Epoch 906: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 5.6034e-04 - accuracy: 1.0000 - val_loss: 4.3404 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 907/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.7083e-04 - accuracy: 1.0000\n",
            "Epoch 907: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.4729e-04 - accuracy: 1.0000 - val_loss: 4.4701 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 908/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.6311e-04 - accuracy: 1.0000\n",
            "Epoch 908: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.4153e-04 - accuracy: 1.0000 - val_loss: 4.4656 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 909/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2677e-04 - accuracy: 1.0000\n",
            "Epoch 909: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.0531e-04 - accuracy: 1.0000 - val_loss: 4.3098 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 910/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.5535e-04 - accuracy: 1.0000\n",
            "Epoch 910: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 3.2384e-04 - accuracy: 1.0000 - val_loss: 4.5103 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 911/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.7143e-04 - accuracy: 1.0000\n",
            "Epoch 911: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 1.7143e-04 - accuracy: 1.0000 - val_loss: 4.4541 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 912/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.7865e-04 - accuracy: 1.0000\n",
            "Epoch 912: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.5192e-04 - accuracy: 1.0000 - val_loss: 4.5672 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 913/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.9624e-04 - accuracy: 1.0000\n",
            "Epoch 913: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.5056e-04 - accuracy: 1.0000 - val_loss: 4.3385 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 914/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.7055e-04 - accuracy: 1.0000\n",
            "Epoch 914: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 4.4949e-04 - accuracy: 1.0000 - val_loss: 4.4286 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 915/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 5.5998e-04 - accuracy: 1.0000\n",
            "Epoch 915: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 5.5998e-04 - accuracy: 1.0000 - val_loss: 4.5748 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 916/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.3872e-04 - accuracy: 1.0000\n",
            "Epoch 916: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 1.4147e-04 - accuracy: 1.0000 - val_loss: 4.6595 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 917/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9853e-04 - accuracy: 1.0000\n",
            "Epoch 917: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.8378e-04 - accuracy: 1.0000 - val_loss: 4.5676 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 918/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.3271e-04 - accuracy: 1.0000\n",
            "Epoch 918: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.2198e-04 - accuracy: 1.0000 - val_loss: 4.4918 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 919/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.8098e-04 - accuracy: 1.0000\n",
            "Epoch 919: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 2.5078e-04 - accuracy: 1.0000 - val_loss: 4.4874 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 920/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 920: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.3270 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 921/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.8627e-04 - accuracy: 1.0000\n",
            "Epoch 921: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.0446e-04 - accuracy: 1.0000 - val_loss: 4.1984 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 922/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.1868e-04 - accuracy: 1.0000\n",
            "Epoch 922: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 5.5316e-04 - accuracy: 1.0000 - val_loss: 4.1849 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 923/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.3897e-04 - accuracy: 1.0000\n",
            "Epoch 923: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 4.3897e-04 - accuracy: 1.0000 - val_loss: 4.0834 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 924/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.1031e-04 - accuracy: 1.0000\n",
            "Epoch 924: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 3.1031e-04 - accuracy: 1.0000 - val_loss: 4.0510 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 925/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 9.3461e-04 - accuracy: 1.0000\n",
            "Epoch 925: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 8.6005e-04 - accuracy: 1.0000 - val_loss: 4.4079 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 926/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.9635e-04 - accuracy: 1.0000\n",
            "Epoch 926: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 2.9635e-04 - accuracy: 1.0000 - val_loss: 4.5027 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 927/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.8275e-04 - accuracy: 1.0000\n",
            "Epoch 927: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 2.8275e-04 - accuracy: 1.0000 - val_loss: 4.5598 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 928/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.2443e-04 - accuracy: 1.0000\n",
            "Epoch 928: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 1.3449e-04 - accuracy: 1.0000 - val_loss: 4.2808 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 929/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.1268e-04 - accuracy: 1.0000\n",
            "Epoch 929: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 1.9260e-04 - accuracy: 1.0000 - val_loss: 4.3525 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 930/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.2914e-04 - accuracy: 1.0000\n",
            "Epoch 930: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 1.2914e-04 - accuracy: 1.0000 - val_loss: 4.5294 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 931/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.8596e-04 - accuracy: 1.0000\n",
            "Epoch 931: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.3589e-04 - accuracy: 1.0000 - val_loss: 4.5806 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 932/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.4162e-04 - accuracy: 1.0000\n",
            "Epoch 932: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 1.3687e-04 - accuracy: 1.0000 - val_loss: 4.6625 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 933/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 4.2103e-04 - accuracy: 1.0000\n",
            "Epoch 933: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 4.2103e-04 - accuracy: 1.0000 - val_loss: 4.7386 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 934/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.9934e-04 - accuracy: 1.0000\n",
            "Epoch 934: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.7297e-04 - accuracy: 1.0000 - val_loss: 4.7753 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 935/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.9381e-04 - accuracy: 1.0000\n",
            "Epoch 935: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 3.6716e-04 - accuracy: 1.0000 - val_loss: 4.7232 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 936/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2953e-04 - accuracy: 1.0000\n",
            "Epoch 936: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.9533e-04 - accuracy: 1.0000 - val_loss: 4.8393 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 937/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0826e-04 - accuracy: 1.0000\n",
            "Epoch 937: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.1052e-04 - accuracy: 1.0000 - val_loss: 4.9219 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 938/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9383e-04 - accuracy: 1.0000\n",
            "Epoch 938: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.7706e-04 - accuracy: 1.0000 - val_loss: 4.7470 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 939/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.9502e-04 - accuracy: 1.0000\n",
            "Epoch 939: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 77ms/step - loss: 2.9502e-04 - accuracy: 1.0000 - val_loss: 4.9021 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 940/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.6739e-04 - accuracy: 1.0000\n",
            "Epoch 940: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 3.6739e-04 - accuracy: 1.0000 - val_loss: 4.5548 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 941/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 6.7751e-05 - accuracy: 1.0000\n",
            "Epoch 941: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 8.0904e-05 - accuracy: 1.0000 - val_loss: 4.5501 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 942/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0630e-04 - accuracy: 1.0000\n",
            "Epoch 942: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 1.9006e-04 - accuracy: 1.0000 - val_loss: 4.6298 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 943/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.3570e-04 - accuracy: 1.0000\n",
            "Epoch 943: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 6.6160e-04 - accuracy: 1.0000 - val_loss: 4.6588 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 944/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.3520e-04 - accuracy: 1.0000\n",
            "Epoch 944: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 1.2593e-04 - accuracy: 1.0000 - val_loss: 4.6690 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 945/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.9265e-04 - accuracy: 1.0000\n",
            "Epoch 945: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 1.9265e-04 - accuracy: 1.0000 - val_loss: 4.7656 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 946/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0224e-04 - accuracy: 1.0000\n",
            "Epoch 946: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 1.9530e-04 - accuracy: 1.0000 - val_loss: 4.6450 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 947/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.4639e-04 - accuracy: 1.0000\n",
            "Epoch 947: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 1.3477e-04 - accuracy: 1.0000 - val_loss: 4.5770 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 948/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.2981e-04 - accuracy: 1.0000\n",
            "Epoch 948: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.1880e-04 - accuracy: 1.0000 - val_loss: 4.6656 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 949/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.5011e-04 - accuracy: 1.0000\n",
            "Epoch 949: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 1.3900e-04 - accuracy: 1.0000 - val_loss: 4.8110 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 950/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2600e-04 - accuracy: 1.0000\n",
            "Epoch 950: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 2.9175e-04 - accuracy: 1.0000 - val_loss: 4.7983 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 951/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.9693e-04 - accuracy: 1.0000\n",
            "Epoch 951: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.6175e-04 - accuracy: 1.0000 - val_loss: 4.7374 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 952/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.1409e-04 - accuracy: 1.0000\n",
            "Epoch 952: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 2.1409e-04 - accuracy: 1.0000 - val_loss: 4.4798 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 953/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.3503e-04 - accuracy: 1.0000\n",
            "Epoch 953: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 1.2860e-04 - accuracy: 1.0000 - val_loss: 4.8498 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 954/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.1876e-04 - accuracy: 1.0000\n",
            "Epoch 954: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 2.1876e-04 - accuracy: 1.0000 - val_loss: 4.8232 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 955/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.2347e-04 - accuracy: 1.0000\n",
            "Epoch 955: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.9298e-04 - accuracy: 1.0000 - val_loss: 4.7030 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 956/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1553e-04 - accuracy: 1.0000\n",
            "Epoch 956: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.8577e-04 - accuracy: 1.0000 - val_loss: 4.8721 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 957/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.3130e-04 - accuracy: 1.0000\n",
            "Epoch 957: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.3579e-04 - accuracy: 1.0000 - val_loss: 4.6013 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 958/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000    \n",
            "Epoch 958: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.5515 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 959/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.5425e-04 - accuracy: 1.0000\n",
            "Epoch 959: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 2.4460e-04 - accuracy: 1.0000 - val_loss: 4.7208 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 960/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.3256e-04 - accuracy: 1.0000\n",
            "Epoch 960: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.8866e-04 - accuracy: 1.0000 - val_loss: 4.6828 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 961/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 961: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 74ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.8051 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 962/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.1905e-04 - accuracy: 1.0000\n",
            "Epoch 962: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.0690e-04 - accuracy: 1.0000 - val_loss: 5.1027 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 963/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.2590e-04 - accuracy: 1.0000\n",
            "Epoch 963: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.8379e-04 - accuracy: 1.0000 - val_loss: 5.0352 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 964/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1420e-04 - accuracy: 1.0000\n",
            "Epoch 964: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 4.6138e-04 - accuracy: 1.0000 - val_loss: 4.8855 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 965/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.9920e-04 - accuracy: 1.0000\n",
            "Epoch 965: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 2.6987e-04 - accuracy: 1.0000 - val_loss: 5.0917 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 966/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.1655e-04 - accuracy: 1.0000\n",
            "Epoch 966: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 1.0764e-04 - accuracy: 1.0000 - val_loss: 5.0871 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 967/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.2266e-04 - accuracy: 1.0000\n",
            "Epoch 967: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 72ms/step - loss: 4.6961e-04 - accuracy: 1.0000 - val_loss: 5.1464 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 968/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.9984e-05 - accuracy: 1.0000\n",
            "Epoch 968: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 80ms/step - loss: 9.1449e-05 - accuracy: 1.0000 - val_loss: 5.1778 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 969/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.9119e-04 - accuracy: 1.0000\n",
            "Epoch 969: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 66ms/step - loss: 8.0029e-04 - accuracy: 1.0000 - val_loss: 5.0963 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 970/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.4472e-04 - accuracy: 1.0000\n",
            "Epoch 970: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 1.4472e-04 - accuracy: 1.0000 - val_loss: 5.1326 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 971/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.9656e-04 - accuracy: 1.0000\n",
            "Epoch 971: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 75ms/step - loss: 4.6017e-04 - accuracy: 1.0000 - val_loss: 5.0631 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 972/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.4618e-04 - accuracy: 1.0000\n",
            "Epoch 972: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 1.4618e-04 - accuracy: 1.0000 - val_loss: 4.9748 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 973/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.0084e-04 - accuracy: 1.0000\n",
            "Epoch 973: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 1.0084e-04 - accuracy: 1.0000 - val_loss: 4.7762 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 974/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.4434e-04 - accuracy: 1.0000\n",
            "Epoch 974: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 84ms/step - loss: 1.4434e-04 - accuracy: 1.0000 - val_loss: 4.9147 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 975/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.4236e-04 - accuracy: 1.0000\n",
            "Epoch 975: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 78ms/step - loss: 1.4236e-04 - accuracy: 1.0000 - val_loss: 4.8953 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 976/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 3.6459e-04 - accuracy: 1.0000\n",
            "Epoch 976: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 3.6459e-04 - accuracy: 1.0000 - val_loss: 4.8432 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 977/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 1.2355e-04 - accuracy: 1.0000\n",
            "Epoch 977: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 82ms/step - loss: 1.2355e-04 - accuracy: 1.0000 - val_loss: 4.8900 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 978/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 7.9033e-04 - accuracy: 1.0000\n",
            "Epoch 978: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 7.9033e-04 - accuracy: 1.0000 - val_loss: 4.7544 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 979/1000\n",
            "5/5 [==============================] - ETA: 0s - loss: 2.5668e-04 - accuracy: 1.0000\n",
            "Epoch 979: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 76ms/step - loss: 2.5668e-04 - accuracy: 1.0000 - val_loss: 4.5874 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 980/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0863e-04 - accuracy: 1.0000\n",
            "Epoch 980: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.0453e-04 - accuracy: 1.0000 - val_loss: 4.5998 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 981/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.0687e-04 - accuracy: 1.0000\n",
            "Epoch 981: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 71ms/step - loss: 9.8072e-05 - accuracy: 1.0000 - val_loss: 4.7339 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 982/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.5458e-04 - accuracy: 1.0000\n",
            "Epoch 982: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 2.3598e-04 - accuracy: 1.0000 - val_loss: 4.6252 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 983/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.8949e-04 - accuracy: 1.0000\n",
            "Epoch 983: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 2.3423e-04 - accuracy: 1.0000 - val_loss: 4.5898 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 984/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 4.2993e-04 - accuracy: 1.0000\n",
            "Epoch 984: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 3.8421e-04 - accuracy: 1.0000 - val_loss: 4.6465 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 985/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7794e-04 - accuracy: 1.0000\n",
            "Epoch 985: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 69ms/step - loss: 3.5621e-04 - accuracy: 1.0000 - val_loss: 4.7506 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 986/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 7.9769e-04 - accuracy: 1.0000\n",
            "Epoch 986: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 7.5566e-04 - accuracy: 1.0000 - val_loss: 4.8397 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 987/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 8.9333e-04 - accuracy: 1.0000\n",
            "Epoch 987: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 7.9734e-04 - accuracy: 1.0000 - val_loss: 4.7547 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 988/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.3812e-04 - accuracy: 1.0000\n",
            "Epoch 988: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.3002e-04 - accuracy: 1.0000 - val_loss: 4.8120 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 989/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.4733e-04 - accuracy: 1.0000\n",
            "Epoch 989: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.3228e-04 - accuracy: 1.0000 - val_loss: 4.6398 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 990/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.3627e-04 - accuracy: 1.0000\n",
            "Epoch 990: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 70ms/step - loss: 1.3109e-04 - accuracy: 1.0000 - val_loss: 4.6010 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 991/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.4877e-04 - accuracy: 1.0000\n",
            "Epoch 991: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.4340e-04 - accuracy: 1.0000 - val_loss: 4.5604 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 992/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.1388e-04 - accuracy: 1.0000\n",
            "Epoch 992: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 1.9754e-04 - accuracy: 1.0000 - val_loss: 4.4723 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 993/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7979e-04 - accuracy: 1.0000\n",
            "Epoch 993: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 3.5129e-04 - accuracy: 1.0000 - val_loss: 4.5235 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 994/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.7146e-04 - accuracy: 1.0000\n",
            "Epoch 994: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 3.8093e-04 - accuracy: 1.0000 - val_loss: 4.5562 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 995/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 5.1122e-04 - accuracy: 1.0000\n",
            "Epoch 995: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 4.5627e-04 - accuracy: 1.0000 - val_loss: 4.6096 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 996/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.6857e-04 - accuracy: 1.0000\n",
            "Epoch 996: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 68ms/step - loss: 1.5267e-04 - accuracy: 1.0000 - val_loss: 4.7041 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 997/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 3.3813e-04 - accuracy: 1.0000\n",
            "Epoch 997: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 65ms/step - loss: 3.0610e-04 - accuracy: 1.0000 - val_loss: 4.6547 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 998/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.3046e-04 - accuracy: 1.0000\n",
            "Epoch 998: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.4604e-04 - accuracy: 1.0000 - val_loss: 4.6565 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 999/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 1.9762e-04 - accuracy: 1.0000\n",
            "Epoch 999: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.8731e-04 - accuracy: 1.0000 - val_loss: 4.4280 - val_accuracy: 0.5000 - lr: 1.0000e-05\n",
            "Epoch 1000/1000\n",
            "4/5 [=======================>......] - ETA: 0s - loss: 2.0085e-04 - accuracy: 1.0000\n",
            "Epoch 1000: val_loss did not improve from 0.62514\n",
            "5/5 [==============================] - 0s 67ms/step - loss: 1.9195e-04 - accuracy: 1.0000 - val_loss: 4.4053 - val_accuracy: 0.5000 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x=XF,\n",
        "                    y=yF,\n",
        "                    batch_size=4,\n",
        "                    epochs=1000,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks,\n",
        "                    shuffle = True,  \n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fXsVS3brAkTW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXsVS3brAkTW",
        "outputId": "94e7b8b0-bc81-41c1-c9b8-043034d334f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "model.save(\"/content/drive/MyDrive/CSC 514/\" + model_name + \"_last_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289542a4",
      "metadata": {
        "id": "289542a4"
      },
      "source": [
        "## Plot loss and accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f30a2e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5f30a2e7",
        "outputId": "5ad0deda-0ab8-46a3-8a53-b8beb32c3041"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "save_fig_dir = \"/content/drive/MyDrive/CSC 514/plots/\"\n",
        "if not os.path.exists(save_fig_dir): os.makedirs(save_fig_dir)\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "history_dict = history.history\n",
        "\n",
        "# Plot loss curve\n",
        "loss_values = history_dict[\"loss\"]\n",
        "val_loss_values = history_dict[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "ax[0].plot(epochs, loss_values, \"r\", label=\"Training loss\")\n",
        "ax[0].plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "ax[0].set_title(\"Loss curve\")\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].legend()\n",
        "\n",
        "# Plot accuracy curve\n",
        "acc_train = history_dict[\"accuracy\"]\n",
        "acc_val = history_dict[\"val_accuracy\"]\n",
        "ax[1].plot(epochs, acc_train, \"r\", label=\"Training accuracy\")\n",
        "ax[1].plot(epochs, acc_val, \"b\", label=\"Validation accuracy\")\n",
        "ax[1].set_title(\"Accuracy curve\")\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"accuracy\")\n",
        "ax[1].legend()\n",
        "\n",
        "fig.savefig(os.path.join(save_fig_dir, model_name + '.png'))    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "989bf07f",
      "metadata": {
        "id": "989bf07f"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "068fbd17",
      "metadata": {
        "id": "068fbd17"
      },
      "outputs": [],
      "source": [
        "# Uncomment to load the model\n",
        "# model = resnet50(input_shape=XF.shape[1:], n_classes=n_classes)  \n",
        "# model.load_weights(checkpoint_loc + '//' + 'best_model.h5') \n",
        "# model.compile(loss=loss, optimizer=Adam(learning_rate = 1e-5), metrics=metrics)\n",
        "\n",
        "\n",
        "# model = tensorflow.keras.models.load_model(model_name + \"_last_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample test"
      ],
      "metadata": {
        "id": "4mTauRbWORRE"
      },
      "id": "4mTauRbWORRE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "#model = tf.keras.models.load_model(\"burn.model\")\n",
        "filepath = '/content/drive/MyDrive/CSC 514/Burns_BIP_US_database/Testing set/24.jpg'\n",
        "test_img = prepare(filepath, IMG_SIZE, DIM)\n",
        "test_img = test_img/255.0\n",
        "prediction = model.predict(test_img)\n",
        "pred_img_class = np.argmax(prediction[0]) + 1 # one added because python starts from class 0\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(np.squeeze(test_img))\n",
        "\n",
        "print(\"Class: \", pred_img_class, CATEGORIES[int(pred_img_class - 1)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "FoLXNQ4UNxGY",
        "outputId": "064332c0-cae2-4edf-c749-8cc47d7264c1"
      },
      "id": "FoLXNQ4UNxGY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Class:  2 deep\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create test dataset"
      ],
      "metadata": {
        "id": "cTVD1mLSOK0k"
      },
      "id": "cTVD1mLSOK0k"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create testing dataset\n",
        "testing_data = create_image_data(CATEGORIES, IMG_SIZE, DIM, TESTDIR)\n",
        "\n",
        "# Randomize the dataset\n",
        "random.shuffle(testing_data)\n",
        "\n",
        "Xt = []\n",
        "yt = []\n",
        "\n",
        "for features,label in testing_data:\n",
        "    Xt.append(features)\n",
        "    yt.append(label)\n",
        "\n",
        "Xt = np.array(Xt).reshape(-1, IMG_SIZE, IMG_SIZE, DIM)\n",
        "print(\"Shape Xt\", Xt.shape)\n",
        "\n",
        "yt = np.array(yt)\n",
        "print(yt)\n",
        "\n",
        "# Xt = Xt/255.0\n",
        "\n",
        "Xt = tf.keras.applications.resnet50.preprocess_input(Xt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbBsrIJFOI-s",
        "outputId": "22bd5066-6883-4423-a876-e1e0aa36ddca"
      },
      "id": "kbBsrIJFOI-s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "superficial\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33/33 [00:06<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deep\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:04<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:02<00:00,  5.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape Xt (74, 200, 200, 3)\n",
            "[0 1 1 0 0 0 0 0 0 0 2 0 0 1 1 0 0 2 2 1 1 2 2 0 0 1 1 1 2 1 0 0 1 0 0 1 1\n",
            " 0 0 1 0 2 1 1 0 0 0 1 1 0 2 0 0 1 1 0 1 2 2 0 0 1 1 2 0 0 1 2 1 2 2 1 1 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xt.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK2rOYM2WB9-",
        "outputId": "5755891e-a08c-4c7a-e4bc-a73f8e9a9e0c"
      },
      "id": "gK2rOYM2WB9-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(74, 200, 200, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "score = model.evaluate(Xt, yt, verbose=0)\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy: ', score[1]*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sds55hUoOV5c",
        "outputId": "7127a9ef-ae5f-4936-d3cb-8835b201c54f"
      },
      "id": "sds55hUoOV5c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.8794867992401123\n",
            "Test accuracy:  43.24324429035187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0c5ab2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c0c5ab2",
        "outputId": "66e46eb4-a0df-4331-a332-2a00a240b428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 81ms/step\n"
          ]
        }
      ],
      "source": [
        "# Prediction\n",
        "y_pred_raw = model.predict(Xt)\n",
        "# pred= [np.argmax(element) for element in y_pred_raw]\n",
        "pred = np.argmax(y_pred_raw, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE4i-boeVRqd",
        "outputId": "bd401aaf-eb8c-4676-9256-9634d65cbb9c"
      },
      "id": "uE4i-boeVRqd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 0, 1, 2, 1, 2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
              "       2, 0, 0, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 2, 1, 2, 1, 1, 0, 1,\n",
              "       1, 1, 2, 1, 2, 2, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT_yVmk9VUfu",
        "outputId": "2fa89902-a38c-4693-f4e8-89cce91bdb08"
      },
      "id": "vT_yVmk9VUfu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 2, 2, 1, 1, 2,\n",
              "       2, 0, 0, 1, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1,\n",
              "       0, 0, 0, 1, 1, 0, 2, 0, 0, 1, 1, 0, 1, 2, 2, 0, 0, 1, 1, 2, 0, 0,\n",
              "       1, 2, 1, 2, 2, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pleXzSCJh92C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pleXzSCJh92C",
        "outputId": "61c1cb55-94b1-4eea-dbbe-446ec7c49d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[13 13  7]\n",
            " [ 6 17  4]\n",
            " [ 3  9  2]]\n"
          ]
        }
      ],
      "source": [
        "# Create confusion matrix\n",
        "conf_mat = confusion_matrix(yt, pred)\n",
        "print(conf_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fqhGQ_5icLJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fqhGQ_5icLJ",
        "outputId": "d96b3e59-89e2-40de-ee96-b23a218cadc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             superficial  deep  full\n",
            "superficial           13    13     7\n",
            "deep                   6    17     4\n",
            "full                   3     9     2\n"
          ]
        }
      ],
      "source": [
        "# Pandas view of confusion matrix\n",
        "df = pd.DataFrame(conf_mat, index = CATEGORIES, columns = CATEGORIES)\n",
        "print(df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}